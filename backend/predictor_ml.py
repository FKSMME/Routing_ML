# backend/predictor_ml.py
from __future__ import annotations

# ‚îÄ‚îÄ ÌëúÏ§Ä ÎùºÏù¥Î∏åÎü¨Î¶¨
from pathlib import Path
from typing import List, Tuple, Dict, Optional, Any, Union
import logging
from collections import Counter, defaultdict
import threading
import numpy as np
import pandas as pd
import json
from datetime import datetime

# ‚îÄ‚îÄ ÏÇ¨ÎÇ¥ Î™®Îìà
from backend.constants import (
    NUMERIC_FEATURES,
    ROUTING_OUTPUT_COLS,
    get_routing_alias_map,
    get_routing_output_columns,
)
from common.config_store import PredictorRuntimeConfig, workflow_config_store
from backend.trainer_ml import load_optimized_model
from backend.feature_weights import FeatureWeightManager
from backend.database import (
    fetch_single_item, 
    fetch_routing_for_item, 
    fetch_routings_for_items,
    check_item_has_routing,
)
from common.logger import get_logger

# Í∞úÏÑ†Îêú save_load Î™®Îìà import
try:
    from models.save_load import load_model_with_metadata
except ImportError:
    load_model_with_metadata = None

logger = get_logger("predictor_ml_improved", level=logging.DEBUG)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Í∏∞Î≥∏ ÏÑ§Ï†ïÍ∞í
DEFAULT_TOP_K: int = 10
MISSING_RATIO_THRESHOLD: float = 0.50
MIN_SAMPLES_FOR_STATS: int = 1
CONFIDENCE_THRESHOLD: float = 0.0
SIMILARITY_HIGH_THRESHOLD: float = 0.8
MIN_SIMILARITY_THRESHOLD: float = SIMILARITY_HIGH_THRESHOLD
MAX_ROUTING_VARIANTS: int = 4

# Ï†úÏô∏Ìï† Ïª¨ÎüºÎì§ - ÏùºÍ¥ÄÏÑ± ÏûàÍ≤å Ï†ïÏùò
COLUMNS_TO_EXCLUDE = ['DRAW_USE', 'ITEM_NM_ENG', 'MID_SEALSIZE_UOM', 'ROTATE_CTRCLOCKWISE']

ROUTING_ALIAS_MAP = {
    'JOB_CD': 'dbo_BI_ROUTING_VIEW_JOB_CD',
    'CUST_NM': 'dbo_BI_ROUTING_VIEW_CUST_NM',
    'VIEW_REMARK': 'dbo_BI_ROUTING_VIEW_REMARK',
}


def _active_alias_map() -> Dict[str, str]:
    """ÌòÑÏû¨ ÏÑ§Ï†ï Í∏∞Î∞ò Ïª¨Îüº Î≥ÑÏπ≠ÏùÑ Î∞òÌôòÌïúÎã§."""

    try:
        return get_routing_alias_map()
    except Exception:  # pragma: no cover - ÏÑ§Ï†ï ÌååÏùº ÏÜêÏÉÅ Ïãú Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
        return dict(ROUTING_ALIAS_MAP)


SUMMARY_META_COLUMNS = {
    'ITEM_CD', 'CANDIDATE_ID', 'ROUTING_SIGNATURE', 'PRIORITY',
    'SIMILARITY_TIER', 'SIMILARITY_SCORE', 'REFERENCE_ITEM_CD'
}


def build_routing_signature(routing_df: pd.DataFrame) -> str:
    """Í≥µÏ†ï Î™©Î°ùÏùÑ Í∏∞Î∞òÏúºÎ°ú ÎùºÏö∞ÌåÖ ÏãúÍ∑∏ÎãàÏ≤ò Î¨∏ÏûêÏó¥ ÏÉùÏÑ±."""
    if routing_df is None or routing_df.empty:
        return ""

    job_names = routing_df.get('JOB_NM')
    tokens: List[str] = []
    if job_names is not None:
        tokens = [str(val).strip() for val in job_names if isinstance(val, str) and val.strip()]

    if not tokens:
        res_names = routing_df.get('RES_DIS')
        if res_names is not None:
            tokens = [str(val).strip() for val in res_names if isinstance(val, str) and val.strip()]

    if not tokens:
        return "ROUTING"

    return '+'.join(tokens[:4])


def normalize_routing_frame(
    target_item: str,
    candidate_id: str,
    base_df: pd.DataFrame,
    *,
    similarity: float,
    reference_item: str,
    priority: str,
    signature: str,
) -> pd.DataFrame:
    """ÎùºÏö∞ÌåÖ DataFrameÏùÑ API/SQL Ï∂úÎ†• Í∑úÍ≤©Ïóê ÎßûÍ≤å Ï†ïÍ∑úÌôî."""
    if base_df is None or base_df.empty:
        return pd.DataFrame()

    frame = base_df.copy()
    alias_map = _active_alias_map()
    frame = frame.rename(columns=alias_map)

    alias_map = _active_alias_map()
    frame = frame.rename(columns=alias_map)
    frame = frame.rename(columns=ROUTING_ALIAS_MAP)


    frame['ITEM_CD'] = target_item
    frame['CANDIDATE_ID'] = candidate_id
    frame['ROUTING_SIGNATURE'] = signature
    frame['PRIORITY'] = priority
    frame['SIMILARITY_TIER'] = 'HIGH' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'LOW'
    frame['SIMILARITY_SCORE'] = float(similarity)
    frame['REFERENCE_ITEM_CD'] = reference_item

    if 'MACH_WORKED_HOURS' not in frame.columns:
        if 'ACT_RUN_TIME' in frame.columns:
            frame['MACH_WORKED_HOURS'] = pd.to_numeric(frame['ACT_RUN_TIME'], errors='coerce').fillna(0.0)
        elif 'RUN_TIME' in frame.columns:
            frame['MACH_WORKED_HOURS'] = pd.to_numeric(frame['RUN_TIME'], errors='coerce').fillna(0.0)
        else:
            frame['MACH_WORKED_HOURS'] = 0.0

    numeric_columns = [
        'MFG_LT', 'QUEUE_TIME', 'SETUP_TIME', 'MACH_WORKED_HOURS',
        'ACT_SETUP_TIME', 'ACT_RUN_TIME', 'WAIT_TIME', 'MOVE_TIME',
        'RUN_TIME_QTY', 'SUBCONTRACT_PRC'
    ]
    for col in numeric_columns:
        if col in frame.columns:
            frame[col] = pd.to_numeric(frame[col], errors='coerce').fillna(0.0)
        else:
            frame[col] = 0.0

    for col in SUMMARY_META_COLUMNS:
        if col not in frame.columns:
            if col == 'SIMILARITY_SCORE':
                frame[col] = float(similarity)
            elif col == 'REFERENCE_ITEM_CD':
                frame[col] = reference_item
            elif col == 'PRIORITY':
                frame[col] = priority
            elif col == 'SIMILARITY_TIER':
                frame[col] = 'HIGH' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'LOW'
            elif col == 'ROUTING_SIGNATURE':
                frame[col] = signature
            else:
                frame[col] = None

    output_columns = get_routing_output_columns()
    frame = frame.reindex(columns=output_columns, fill_value=None)

    output_columns = get_routing_output_columns()
    frame = frame.reindex(columns=output_columns, fill_value=None)
    frame = frame.reindex(columns=ROUTING_OUTPUT_COLS, fill_value=None)

    return frame

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîß ÏïàÏ†ÑÌïú ÌÉÄÏûÖ Î≥ÄÌôò Ìï®ÏàòÎì§
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def safe_int_conversion(value, default=0):
    """ÏïàÏ†ÑÌïú Ï†ïÏàò Î≥ÄÌôò"""
    if value is None or value == '':
        return default
    try:
        return int(value)
    except (ValueError, TypeError):
        logger.warning(f"Ï†ïÏàò Î≥ÄÌôò Ïã§Ìå®: {value}, Í∏∞Î≥∏Í∞í {default} ÏÇ¨Ïö©")
        return default

def safe_float_conversion(value, default=0.0):
    """ÏïàÏ†ÑÌïú Ïã§Ïàò Î≥ÄÌôò"""
    if value is None or value == '':
        return default
    try:
        return float(value)
    except (ValueError, TypeError):
        logger.warning(f"Ïã§Ïàò Î≥ÄÌôò Ïã§Ìå®: {value}, Í∏∞Î≥∏Í∞í {default} ÏÇ¨Ïö©")
        return default

def _safe_numeric(col: pd.Series) -> pd.Series:
    """ÏïàÏ†ÑÌïú ÏàòÏπòÌòï Î≥ÄÌôò"""
    col = col.replace(["", " ", "-", "--", "nan", "NaN", "null", "NULL", "None"], np.nan)
    num = pd.to_numeric(col, errors="coerce")
    return num.fillna(0).replace([np.inf, -np.inf], 0).infer_objects(copy=False).astype(np.float32)

def _safe_string(col: pd.Series) -> pd.Series:
    """ÏïàÏ†ÑÌïú Î¨∏ÏûêÏó¥ Î≥ÄÌôò"""
    return (
        col.astype(str)
           .str.strip()
           .str.strip("'\"")
           .replace(
               {r"^\s*$": "missing", r"^(nan|NaN|null|NULL|None|-{1,2})$": "missing"},
               regex=True,
           )
    )

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üéØ ÏãúÍ∞Ñ ÏãúÎÇòÎ¶¨Ïò§ ÏÑ§Ï†ï ÌÅ¥ÎûòÏä§
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class TimeScenarioConfig:
    """ÏãúÍ∞Ñ ÏãúÎÇòÎ¶¨Ïò§ ÏÑ§Ï†ïÍ∞íÎì§ - GUIÏóêÏÑú Ï°∞Ï†ï Í∞ÄÎä•"""
    
    def __init__(self):
        self.OPTIMAL_SIGMA = 0.67
        self.SAFE_SIGMA = 1.28
        self.CORRELATION_INDEPENDENT = 0.75
        self.CORRELATION_DEPENDENT = 0.25
        self.CV_STABLE = 0.15
        self.CV_MODERATE = 0.30
        self.CV_VARIABLE = 0.50
        self.SAMPLE_BONUS_RATE = 0.05
        self.MAX_SAMPLE_BONUS = 0.2
        self.PROCESS_OVERLAP_THRESHOLD = 0.3
        self.OUTLIER_DETECTION_ENABLED = True
        self.OUTLIER_Z_SCORE_THRESHOLD = 2.5
        self.SIMILARITY_WEIGHT_POWER = 2.0
        self.TRIM_STD_ENABLED = True
        self.TRIM_LOWER_PERCENT = 0.05
        self.TRIM_UPPER_PERCENT = 0.95
    
    def get_scenario_description(self, cv: float) -> str:
        if cv < self.CV_STABLE:
            return "ÏïàÏ†ïÏ†Å (ÎÇÆÏùÄ Î≥ÄÎèôÏÑ±)"
        elif cv < self.CV_MODERATE:
            return "Î≥¥ÌÜµ (Ï§ëÍ∞Ñ Î≥ÄÎèôÏÑ±)"
        elif cv < self.CV_VARIABLE:
            return "Í∞ÄÎ≥ÄÏ†Å (ÎÜíÏùÄ Î≥ÄÎèôÏÑ±)"
        else:
            return "Î∂àÏïàÏ†ï (Îß§Ïö∞ ÎÜíÏùÄ Î≥ÄÎèôÏÑ±)"
    
    def get_scenario_emoji(self, cv: float) -> str:
        if cv < self.CV_STABLE:
            return "üü¢"
        elif cv < self.CV_MODERATE:
            return "üü°"
        elif cv < self.CV_VARIABLE:
            return "üü†"
        else:
            return "üî¥"
    
    def to_dict(self) -> Dict:
        return {
            'optimal_sigma': self.OPTIMAL_SIGMA,
            'safe_sigma': self.SAFE_SIGMA,
            'correlation_independent': self.CORRELATION_INDEPENDENT,
            'correlation_dependent': self.CORRELATION_DEPENDENT,
            'cv_stable': self.CV_STABLE,
            'cv_moderate': self.CV_MODERATE,
            'cv_variable': self.CV_VARIABLE,
            'sample_bonus_rate': self.SAMPLE_BONUS_RATE,
            'max_sample_bonus': self.MAX_SAMPLE_BONUS,
            'process_overlap_threshold': self.PROCESS_OVERLAP_THRESHOLD,
            'outlier_detection_enabled': self.OUTLIER_DETECTION_ENABLED,
            'outlier_z_score_threshold': self.OUTLIER_Z_SCORE_THRESHOLD,
            'similarity_weight_power': self.SIMILARITY_WEIGHT_POWER,
            'trim_std_enabled': self.TRIM_STD_ENABLED,
            'trim_lower_percent': self.TRIM_LOWER_PERCENT,
            'trim_upper_percent': self.TRIM_UPPER_PERCENT,
        }

    def from_dict(self, config_dict: Dict):
        self.OPTIMAL_SIGMA = config_dict.get('optimal_sigma', 0.67)
        self.SAFE_SIGMA = config_dict.get('safe_sigma', 1.28)
        self.CORRELATION_INDEPENDENT = config_dict.get('correlation_independent', 0.75)
        self.CORRELATION_DEPENDENT = config_dict.get('correlation_dependent', 0.25)
        self.CV_STABLE = config_dict.get('cv_stable', 0.15)
        self.CV_MODERATE = config_dict.get('cv_moderate', 0.30)
        self.CV_VARIABLE = config_dict.get('cv_variable', 0.50)
        self.SAMPLE_BONUS_RATE = config_dict.get('sample_bonus_rate', 0.05)
        self.MAX_SAMPLE_BONUS = config_dict.get('max_sample_bonus', 0.2)
        self.PROCESS_OVERLAP_THRESHOLD = config_dict.get('process_overlap_threshold', 0.3)
        self.OUTLIER_DETECTION_ENABLED = config_dict.get('outlier_detection_enabled', True)
        self.OUTLIER_Z_SCORE_THRESHOLD = config_dict.get('outlier_z_score_threshold', 2.5)
        self.SIMILARITY_WEIGHT_POWER = config_dict.get('similarity_weight_power', 2.0)
        self.TRIM_STD_ENABLED = config_dict.get('trim_std_enabled', True)
        self.TRIM_LOWER_PERCENT = config_dict.get('trim_lower_percent', 0.05)
        self.TRIM_UPPER_PERCENT = config_dict.get('trim_upper_percent', 0.95)

# Ï†ÑÏó≠ ÏÑ§Ï†ï Ïù∏Ïä§ÌÑ¥Ïä§
SCENARIO_CONFIG = TimeScenarioConfig()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîß Í∞úÏÑ†Îêú Î™®Îç∏ Î°úÎìú Î∞è Í¥ÄÎ¶¨
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class EnhancedModelManager:
    """Í∞úÏÑ†Îêú Î™®Îç∏ Í¥ÄÎ¶¨Ïûê - Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î∞è Ï∂îÍ∞Ä Ïª¥Ìè¨ÎÑåÌä∏ ÏßÄÏõê"""
    
    def __init__(self, model_dir: Union[str, Path]):
        self.model_dir = Path(model_dir)
        self.model_components = {}
        self.metadata = {}
        self.is_enhanced = False
        
    def __enter__(self):
        """Context manager ÏßÑÏûÖ"""
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager Ï¢ÖÎ£å Ïãú Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        self.cleanup()
        
    def cleanup(self):
        """Î©îÎ™®Î¶¨ Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        if hasattr(self, 'model_components') and self.model_components:
            # ÌÅ∞ Î™®Îç∏ Í∞ùÏ≤¥Îì§ Ï†ïÎ¶¨
            for key in ['searcher', 'encoder', 'scaler', 'pca']:
                if key in self.model_components:
                    del self.model_components[key]
            self.model_components.clear()
            
        if hasattr(self, 'metadata') and self.metadata:
            self.metadata.clear()
            
        logger.debug("Î™®Îç∏ Îß§ÎãàÏ†Ä Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨ ÏôÑÎ£å")
        
    def load(self):
        """Î™®Îç∏ Î°úÎìú - Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ Ïö∞ÏÑ†, Ìò∏ÌôòÏÑ± Ïú†ÏßÄ"""
        
        if load_model_with_metadata and self._has_enhanced_model():
            try:
                logger.info("Í∞úÏÑ†Îêú Î™®Îç∏ Î°úÎìú ÏãúÎèÑ...")
                components = load_model_with_metadata(self.model_dir, load_sample_data=False)
                
                # feature_weights Î°úÎìú - FeatureWeightManager Ïö∞ÏÑ†
                feature_weights = components.get('feature_weights')
                if feature_weights is None or isinstance(feature_weights, (dict, np.ndarray)):
                    # FeatureWeightManagerÎ°ú Î°úÎìú ÏãúÎèÑ
                    try:
                        from backend.feature_weights import FeatureWeightManager
                        fw_manager = FeatureWeightManager(self.model_dir)
                        fw_manager.load_weights()
                        feature_weights = fw_manager
                        logger.debug("FeatureWeightManagerÎ°ú Í∞ÄÏ§ëÏπò Î°úÎìú ÏÑ±Í≥µ")
                    except Exception as e:
                        logger.debug(f"FeatureWeightManager Î°úÎìú Ïã§Ìå®, Í∏∞Ï°¥ Î∞©Ïãù ÏÇ¨Ïö©: {e}")
                        if feature_weights is None:
                            feature_weights = self._load_legacy_weights(len(components['feature_columns']))
                
                # feature_columns Ï†ïÎ¶¨ - COLUMNS_TO_EXCLUDE Ï†úÏô∏
                clean_feature_columns = [
                    c for c in components['feature_columns']
                    if c not in COLUMNS_TO_EXCLUDE
                ]
                
                self.model_components = {
                    'searcher': components['searcher'],
                    'encoder': components['encoder'],
                    'scaler': components['scaler'],
                    'feature_columns': clean_feature_columns,
                    'pca': components.get('pca'),
                    'variance_selector': components.get('variance_selector'),
                    'feature_weights': feature_weights,
                    'active_features': components.get('active_features'),
                }
                
                self.metadata = {
                    'model_metadata': components.get('model_metadata', {}),
                    'feature_metadata': components.get('feature_metadata', {}),
                    'training_metadata': components.get('training_metadata', {}),
                    'vector_statistics': components.get('vector_statistics', {}),
                }
                
                self.is_enhanced = True
                logger.info("‚úÖ Í∞úÏÑ†Îêú Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ")
                
            except Exception as e:
                logger.warning(f"Í∞úÏÑ†Îêú Î™®Îç∏ Î°úÎìú Ïã§Ìå®, Í∏∞Î≥∏ Î°úÎìú ÏãúÎèÑ: {e}")
                self._load_basic()
        else:
            self._load_basic()
    
    def _has_enhanced_model(self) -> bool:
        enhanced_files = [
            'training_metadata.json',
            'feature_metadata.json',
            'model_metadata.json'
        ]
        return any((self.model_dir / f).exists() for f in enhanced_files)
    
    def _load_basic(self):
        searcher, encoder, scaler, feature_columns = load_optimized_model(self.model_dir)
        
        # feature_columns Ï†ïÎ¶¨ - COLUMNS_TO_EXCLUDE Ï†úÏô∏
        clean_feature_columns = [
            c for c in feature_columns
            if c not in COLUMNS_TO_EXCLUDE
        ]
        
        self.model_components = {
            'searcher': searcher,
            'encoder': encoder,
            'scaler': scaler,
            'feature_columns': clean_feature_columns,
            'pca': None,
            'variance_selector': None,
            'feature_weights': self._load_legacy_weights(len(clean_feature_columns)),
            'active_features': None,
        }
        
        self.is_enhanced = False
        logger.info("‚úÖ Í∏∞Î≥∏ Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ")
    
    def _load_legacy_weights(self, n_features: int) -> Optional[Dict[str, float]]:
        weights_path = self.model_dir / "feature_weights.joblib"
        if weights_path.exists():
            try:
                import joblib
                weights = joblib.load(weights_path)
                # COLUMNS_TO_EXCLUDEÏóê ÏûàÎäî Ïª¨Îüº Ï†úÍ±∞
                if isinstance(weights, dict):
                    return {k: v for k, v in weights.items() if k not in COLUMNS_TO_EXCLUDE}
                return weights
            except Exception as e:
                logger.warning(f"joblib Í∞ÄÏ§ëÏπò Î°úÎìú Ïã§Ìå®: {e}")
        
        npy_path = self.model_dir / "feature_weights.npy"
        if npy_path.exists():
            try:
                weights_array = np.load(npy_path)
                feature_cols = self.model_components.get('feature_columns', [])
                if feature_cols and len(weights_array) >= len(feature_cols):
                    return {col: float(weights_array[i]) 
                           for i, col in enumerate(feature_cols) 
                           if col not in COLUMNS_TO_EXCLUDE}
            except Exception as e:
                logger.warning(f"numpy Í∞ÄÏ§ëÏπò Î°úÎìú Ïã§Ìå®: {e}")
        
        return None
    
    def get_feature_weights_array(self) -> np.ndarray:
        feature_cols = self.model_components['feature_columns']
        weights_dict = self.model_components.get('feature_weights')
        
        if weights_dict and isinstance(weights_dict, dict):
            weights = []
            for col in feature_cols:
                weights.append(weights_dict.get(col, 1.0))
            weights = np.array(weights)
        else:
            weights = np.ones(len(feature_cols))
        
        return weights.astype(np.float32)
    
    def transform_features(self, encoded_features: np.ndarray) -> np.ndarray:
        result = encoded_features
        
        if self.model_components.get('variance_selector') is not None:
            try:
                result = self.model_components['variance_selector'].transform(result)
                logger.debug(f"Variance selector Ï†ÅÏö©: {encoded_features.shape} ‚Üí {result.shape}")
            except Exception as e:
                logger.warning(f"Variance selector Ï†ÅÏö© Ïã§Ìå®: {e}")
        
        if self.model_components.get('pca') is not None:
            try:
                result = self.model_components['pca'].transform(result)
                logger.debug(f"PCA Ï†ÅÏö©: {result.shape[0]} ‚Üí {self.model_components['pca'].n_components_} Ï∞®Ïõê")
            except Exception as e:
                logger.warning(f"PCA Ï†ÅÏö© Ïã§Ìå®: {e}")
        
        if self.model_components.get('active_features') is not None:
            try:
                active_mask = self.model_components['active_features']
                if len(active_mask) == result.shape[1]:
                    result = result[:, active_mask]
                    logger.debug(f"Active features Ï†ÅÏö©: {np.sum(active_mask)}Í∞ú Ï∞®Ïõê Ïú†ÏßÄ")
            except Exception as e:
                logger.warning(f"Active features Ï†ÅÏö© Ïã§Ìå®: {e}")
        
        return result
    
    def get_model_info(self) -> Dict[str, Any]:
        info = {
            'is_enhanced': self.is_enhanced,
            'has_pca': self.model_components.get('pca') is not None,
            'has_variance_selector': self.model_components.get('variance_selector') is not None,
            'has_feature_weights': self.model_components.get('feature_weights') is not None,
            'has_active_features': self.model_components.get('active_features') is not None,
        }
        
        if self.is_enhanced and self.metadata.get('model_metadata'):
            info.update({
                'creation_time': self.metadata['model_metadata'].get('creation_time'),
                'vector_dimension': self.metadata['model_metadata'].get('searcher_info', {}).get('vector_dimension'),
                'total_items': self.metadata['model_metadata'].get('item_info', {}).get('total_items'),
            })
        
        return info

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîß Í∞úÏÑ†Îêú Ïù∏ÏΩîÎî© Ìï®Ïàò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def _clean_and_encode_enhanced(
    df: pd.DataFrame, 
    feature_cols: List[str], 
    model_manager: EnhancedModelManager
) -> Tuple[np.ndarray, float]:
    """
    ÏïÑÏù¥ÌÖú Ï†ïÎ≥¥Î•º Ï†ïÎ¶¨ÌïòÍ≥† Ïù∏ÏΩîÎî© (Í∞úÏÑ† Î≤ÑÏ†Ñ)
    OrdinalEncoderÏôÄ StandardScalerÏùò ÏöîÍµ¨ÏÇ¨Ìï≠ÏùÑ Î™®Îëê Ï∂©Ï°±
    """
    df = df.copy()
    
    # Ï†úÏô∏Ìï† Ïª¨Îüº Ïã§Ï†ú Ï†úÍ±∞
    for col in COLUMNS_TO_EXCLUDE:
        if col in df.columns:
            df = df.drop(columns=[col])
            logger.debug(f"ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ï†úÏô∏Îêú Ïª¨Îüº: {col}")
    
    # Í≤∞Ï∏°Î•† Í≥ÑÏÇ∞ÏùÄ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Í∏∞Ï§ÄÏúºÎ°ú
    available_cols = [col for col in feature_cols if col in df.columns]
    miss_ratio = df[available_cols].isna().mean(axis=1).iloc[0] if available_cols else 0.0
    
    # 1) OrdinalEncoderÍ∞Ä ÌïÑÏöîÎ°ú ÌïòÎäî Î≤îÏ£ºÌòï Ïª¨Îüº Ï≤òÎ¶¨
    encoder = model_manager.model_components['encoder']
    encoder_cols = encoder.feature_names_in_  # encoderÍ∞Ä ÌïôÏäµ Ïãú Î≥∏ Ïª¨ÎüºÎì§
    
    # encoderÍ∞Ä ÌïÑÏöîÎ°ú ÌïòÎäî Ïª¨ÎüºÎßå ÏÑ†ÌÉùÌïòÍ≥†, ÏóÜÎäî Ïª¨ÎüºÏùÄ 'missing'ÏúºÎ°ú Ï±ÑÏõÄ
    df_for_encoder = df.reindex(columns=encoder_cols, fill_value='missing')
    df_cat = df_for_encoder.apply(_safe_string)
    
    # Ïù∏ÏΩîÎî© ÏàòÌñâ
    enc_cat = encoder.transform(df_cat)
    enc_cat_df = pd.DataFrame(enc_cat, columns=encoder_cols, index=df.index)
    
    # 2) ÏàòÏπòÌòï Ïª¨Îüº Ï≤òÎ¶¨
    num_cols = [c for c in feature_cols if c in NUMERIC_FEATURES and c not in COLUMNS_TO_EXCLUDE]
    if num_cols:
        # ÏàòÏπòÌòï Ïª¨Îüº Ï§ë Ïã§Ï†úÎ°ú ÏûàÎäî Í≤ÉÎßå Ï≤òÎ¶¨
        available_num_cols = [c for c in num_cols if c in df.columns]
        if available_num_cols:
            df_num = df[available_num_cols].apply(_safe_numeric)
        else:
            df_num = pd.DataFrame(index=df.index)
        
        # ÏóÜÎäî ÏàòÏπòÌòï Ïª¨ÎüºÏùÄ 0ÏúºÎ°ú Ï±ÑÏõÄ
        for col in num_cols:
            if col not in df_num.columns:
                df_num[col] = 0.0
    else:
        df_num = pd.DataFrame(index=df.index)
    
    # 3) Î≤îÏ£ºÌòïÍ≥º ÏàòÏπòÌòï Í≤∞Ìï©
    fin = pd.concat([enc_cat_df, df_num], axis=1)
    
    # 4) StandardScalerÍ∞Ä ÌïÑÏöîÎ°ú ÌïòÎäî Ïª¨ÎüºÎßå ÏÑ†ÌÉùÌïòÍ≥† ÏàúÏÑú ÎßûÏ∂§
    scaler = model_manager.model_components['scaler']
    scaler_cols = scaler.feature_names_in_  # scalerÍ∞Ä ÌïôÏäµ Ïãú Î≥∏ Ïª¨ÎüºÎì§
    
    # scalerÍ∞Ä ÌïÑÏöîÎ°ú ÌïòÎäî Ïª¨ÎüºÎßå ÏÑ†ÌÉù (reindexÎ°ú ÏàúÏÑúÎèÑ ÎßûÏ∂§)
    fin = fin.reindex(columns=scaler_cols, fill_value=0.0).astype(np.float32)
    
    # 5) Í∞ÄÏ§ëÏπò Ï†ÅÏö©
    weights_obj = model_manager.model_components.get('feature_weights')
    
    # FeatureWeightManager Í∞ùÏ≤¥Ïù∏ÏßÄ ÌôïÏù∏
    if hasattr(weights_obj, 'get_weights_as_array'):
        # FeatureWeightManagerÎ•º ÏÇ¨Ïö©ÌïòÎäî Í≤ΩÏö∞ - ÌôúÏÑ±Ìôî ÏÉÅÌÉú Î∞òÏòÅ
        weights = weights_obj.get_weights_as_array(scaler_cols, apply_active_mask=True)
        logger.debug(f"FeatureWeightManager ÏÇ¨Ïö© - ÌôúÏÑ±ÌôîÎêú ÌîºÏ≤òÎßå Í∞ÄÏ§ëÏπò Ï†ÅÏö©")
    elif isinstance(weights_obj, dict):
        # Í∏∞Ï°¥ ÎîïÏÖîÎÑàÎ¶¨ Î∞©Ïãù (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
        weights = []
        for col in scaler_cols:
            weights.append(weights_obj.get(col, 1.0))
        weights = np.array(weights)
    else:
        # Í∞ÄÏ§ëÏπò Ï†ïÎ≥¥Í∞Ä ÏóÜÎäî Í≤ΩÏö∞
        weights = np.ones(len(scaler_cols))
        logger.debug("Í∞ÄÏ§ëÏπò Ï†ïÎ≥¥ ÏóÜÏùå - Í∏∞Î≥∏Í∞í 1.0 ÏÇ¨Ïö©")
    
    # Í∞ÄÏ§ëÏπò Ï†ÅÏö©
    if len(weights) == fin.shape[1]:
        fin = fin * weights
        # 0Ïù∏ Í∞ÄÏ§ëÏπòÎ•º Í∞ÄÏßÑ ÌîºÏ≤ò Ïàò Î°úÍπÖ
        zero_weight_count = np.sum(weights == 0)
        if zero_weight_count > 0:
            logger.info(f"ÎπÑÌôúÏÑ±ÌôîÎêú ÌîºÏ≤ò {zero_weight_count}Í∞úÍ∞Ä ÏòàÏ∏°ÏóêÏÑú Ï†úÏô∏Îê®")
    else:
        logger.warning(f"Í∞ÄÏ§ëÏπò Ï∞®Ïõê Î∂àÏùºÏπò: weights={len(weights)}, features={fin.shape[1]}")
    
    # 6) Ïä§ÏºÄÏùºÎßÅ
    scaled = scaler.transform(fin)
    
    # 7) Ï∂îÍ∞Ä Î≥ÄÌôò (PCA Îì±)
    result = model_manager.transform_features(scaled)
    
    return result, miss_ratio

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üöÄ Í∞úÏÑ†Îêú Î©îÏù∏ ÏòàÏ∏° Ìï®Ïàò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def predict_single_item_with_ml_enhanced(
    item_cd: str,
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
    mode: str = "summary",
    routing_selection: str = "latest"
) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:
    """Enhanced ML Îã®Ïùº ÌíàÎ™© ÏòàÏ∏° - Í∏∞Ï°¥ ÎùºÏö∞ÌåÖ Ïö∞ÏÑ†"""
    if config is None:
        config = SCENARIO_CONFIG
    
    logger.info(f"üöÄ Enhanced ML ÏòàÏ∏° ÏãúÏûë - ÌíàÎ™©: {item_cd}, Î™®Îìú: {mode}, ÎùºÏö∞ÌåÖ: {routing_selection}")
    
    # 1. Î®ºÏ†Ä Í∏∞Ï°¥ ÎùºÏö∞ÌåÖ ÌôïÏù∏
    existing_routing = fetch_routing_for_item(item_cd, latest_only=True, selection_mode=routing_selection)
    if not existing_routing.empty:
        # AÏïà: Î™®ÎìúÏóê Îî∞Îùº Î∞òÌôò ÌòïÌÉúÎ•º Îã¨Î¶¨ÌïúÎã§
        if mode == "summary":
            # ‚ë† ÏßëÍ≥Ñ ‚Üí 1 ÌñâÏúºÎ°ú Î≥ÄÌôò
            summary_row = {
                "ITEM_CD": item_cd,
                "SETUP_TIME": existing_routing["SETUP_TIME"].sum(),
                "RUN_TIME": existing_routing["RUN_TIME"].sum(),
                "WAIT_TIME": existing_routing["WAIT_TIME"].sum() if "WAIT_TIME" in existing_routing else 0,
                "MOVE_TIME": existing_routing["MOVE_TIME"].sum() if "MOVE_TIME" in existing_routing else 0,
                "OPTIMAL_TIME": None,
                "STANDARD_TIME": None,
                "SAFE_TIME": None,
                "PREDICTION_TYPE": "EXISTING",
                "CONFIDENCE": 1.0,
                "MESSAGE": "Í∏∞Ï°¥ ÎùºÏö∞ÌåÖ(ÏöîÏïΩ)"
            }
            summary_df = pd.DataFrame([summary_row])
            empty_cand_df = pd.DataFrame()
            model_info = {"is_enhanced": False, "source": "existing_routing"}
            return summary_df, empty_cand_df, model_info

        else:  # detailed Î™®Îìú
            existing_routing = existing_routing.copy()
            existing_routing["PREDICTION_TYPE"] = "EXISTING"
            existing_routing["INPUT_ITEM_CD"] = item_cd
            empty_cand_df = pd.DataFrame()
            model_info = {"is_enhanced": False, "source": "existing_routing"}
            return existing_routing, empty_cand_df, model_info
    
    # 2. Í∏∞Ï°¥ ÎùºÏö∞ÌåÖÏù¥ ÏóÜÎäî Í≤ΩÏö∞ÏóêÎßå ML ÏòàÏ∏° ÏßÑÌñâ
    logger.info(f"üîç {item_cd}Ïùò Í∏∞Ï°¥ ÎùºÏö∞ÌåÖ ÏóÜÏùå - ML ÏòàÏ∏° ÏßÑÌñâ")
    
    is_valid, msg, item_info = validate_input_item(item_cd)
    if not is_valid:
        error_df = pd.DataFrame({
            "ITEM_CD": [item_cd],
            "MESSAGE": [msg]
        })
        return error_df, pd.DataFrame(), {}
    
    # Context managerÎ•º ÏÇ¨Ïö©Ìïú Î™®Îç∏ Í¥ÄÎ¶¨
    with EnhancedModelManager(model_dir) as model_manager:
        model_manager.load()
        model_info = model_manager.get_model_info()
        
        logger.info(f"üìä Î™®Îç∏ Ï†ïÎ≥¥: Enhanced={model_info['is_enhanced']}, "
                   f"PCA={model_info['has_pca']}, "
                   f"Feature Weights={model_info['has_feature_weights']}")
        
        # Î™®Îç∏ÏóêÏÑú feature_cols Í∞ÄÏ†∏Ïò§Í∏∞
        model_feature_cols = model_manager.model_components['feature_columns']
        
        # Ïù∏ÏΩîÎî© ÏàòÌñâ
        encoded_vec, miss_ratio = _clean_and_encode_enhanced(item_info, model_feature_cols, model_manager)
        
        if miss_ratio > miss_thr:
            error_df = pd.DataFrame({
                "ITEM_CD": [item_cd],
                "MESSAGE": [f"Í∏∞Ï§ÄÏ†ïÎ≥¥ Í≤∞Ï∏°Î•† {miss_ratio:.0%} (ÏûÑÍ≥ÑÍ∞í: {miss_thr:.0%})"]
            })
            return error_df, pd.DataFrame(), model_info
        
        vec = encoded_vec.mean(axis=0).reshape(1, -1)
        
        searcher = model_manager.model_components['searcher']
        codes, scores = searcher.find_similar(vec, top_k)
        
        if not isinstance(codes, list):
            codes = [codes]
        if not isinstance(scores, list):
            scores = [scores]
        
        logger.info(f"üîç Ïú†ÏÇ¨Ìíà Í≤ÄÏÉâ ÏôÑÎ£å: {len(codes)}Í∞ú (Ïú†ÏÇ¨ÎèÑ: {min(scores):.3f}~{max(scores):.3f})")
        
        # ML ÏòàÏ∏° ÏàòÌñâ
        routing_df = predict_routing_from_similar_items(
            item_cd, codes, scores, config, mode
        )
        
        # ML ÏòàÏ∏° Í≤∞Í≥ºÏóê PREDICTION_TYPE Ï∂îÍ∞Ä
        if not routing_df.empty:
            routing_df['PREDICTION_TYPE'] = 'ML_BASED'
            routing_df['INPUT_ITEM_CD'] = item_cd
            
            if model_info.get('vector_dimension'):
                routing_df['MODEL_INFO'] = f"Dim={model_info['vector_dimension']}, Enhanced={model_info['is_enhanced']}"
        
        cand_df = pd.DataFrame({
            "ITEM_CD": item_cd,
            "CANDIDATE_RANK": list(range(1, len(codes)+1)),
            "CANDIDATE_ITEM_CD": codes,
            "SIMILARITY_SCORE": scores,
        })
        
        return routing_df, cand_df, model_info

def validate_input_item(item_cd: str) -> Tuple[bool, str, Optional[pd.DataFrame]]:
    """ÏûÖÎ†• ÌíàÎ™© Í≤ÄÏ¶ù - Í∏∞Ï°¥ ÎùºÏö∞ÌåÖ Í≤ΩÍ≥† Ï†úÍ±∞"""
    item_info = fetch_single_item(item_cd)
    if item_info.empty:
        return False, f"ÌíàÎ™© Ï†ïÎ≥¥Í∞Ä ÏóÜÏäµÎãàÎã§: {item_cd}", None
    
    # Í∏∞Ï°¥ ÎùºÏö∞ÌåÖ Ï°¥Ïû¨ Ïó¨Î∂ÄÎäî Îã®Ïàú Î°úÍπÖÎßå
    has_routing = check_item_has_routing(item_cd)
    if has_routing:
        logger.debug(f"ÌíàÎ™© {item_cd}Ïóê Í∏∞Ï°¥ ÎùºÏö∞ÌåÖ Ï°¥Ïû¨")
    
    return True, "ÌíàÎ™© Ï†ïÎ≥¥ ÌôïÏù∏ ÏôÑÎ£å", item_info

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîß Ïú†Ìã∏Î¶¨Ìã∞ Ìï®ÏàòÎì§
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def apply_similarity_weights(values: List[float], similarities: List[float], 
                           power: float = None) -> Tuple[List[float], List[float]]:
    if power is None:
        power = SCENARIO_CONFIG.SIMILARITY_WEIGHT_POWER
    
    weights = np.power(similarities, power)
    total_weight = np.sum(weights)
    if total_weight > 0:
        weights = weights / total_weight
    else:
        weights = np.ones_like(weights) / len(weights)
    
    return values, weights.tolist()

def filter_by_similarity_threshold(items: List[str], similarities: List[float], 
                                 threshold: float = MIN_SIMILARITY_THRESHOLD) -> Tuple[List[str], List[float]]:
    filtered_items = []
    filtered_sims = []
    
    for item, sim in zip(items, similarities):
        if sim >= threshold:
            filtered_items.append(item)
            filtered_sims.append(sim)
    
    if not filtered_items and items:
        filtered_items = [items[0]]
        filtered_sims = [similarities[0]]
        logger.warning(f"Î™®Îì† ÌíàÎ™©Ïù¥ ÏûÑÍ≥ÑÍ∞í({threshold:.2f}) ÎØ∏Îßå - ÏµúÍ≥† Ïú†ÏÇ¨ÎèÑ ÌíàÎ™©Îßå Ïú†ÏßÄ: {similarities[0]:.3f}")
    
    logger.info(f"Ïú†ÏÇ¨ÎèÑ ÌïÑÌÑ∞ÎßÅ: {len(items)}Í∞ú ‚Üí {len(filtered_items)}Í∞ú (ÏûÑÍ≥ÑÍ∞í: {threshold:.2f})")
    return filtered_items, filtered_sims

def remove_outliers_zscore(values: List[float], weights: List[float],
                          z_threshold: float = 2.5) -> Tuple[List[float], List[float], List[bool]]:
    if len(values) < 3:
        normalized = _normalize_weights_array(weights) if weights else np.ones(len(values)) / max(len(values), 1)
        return values, normalized.tolist(), [True] * len(values)

    values_array = np.array(values)
    weights_array = _normalize_weights_array(weights if weights else np.ones(len(values_array)))

    mean = np.average(values_array, weights=weights_array)
    variance = np.average((values_array - mean)**2, weights=weights_array)
    std = np.sqrt(variance)

    if std == 0:
        return values, weights_array.tolist(), [True] * len(values)

    z_scores = np.abs((values_array - mean) / std)
    valid_mask = z_scores <= z_threshold

    filtered_values = values_array[valid_mask].tolist()
    filtered_weights = weights_array[valid_mask].tolist()

    if filtered_weights:
        total_weight = sum(filtered_weights)
        filtered_weights = [w/total_weight for w in filtered_weights]

    removed_count = len(values) - len(filtered_values)
    if removed_count > 0:
        logger.debug(f"Ïù¥ÏÉÅÏπò Ï†úÍ±∞: {removed_count}Í∞ú Ï†úÍ±∞Îê® (Z-score > {z_threshold})")

    return filtered_values, filtered_weights, valid_mask.tolist()


def _normalize_weights_array(weights: Union[List[float], np.ndarray]) -> np.ndarray:
    arr = np.asarray(weights, dtype=np.float64)
    if arr.size == 0:
        return arr
    total = arr.sum()
    if total <= 0:
        return np.ones_like(arr) / len(arr)
    return arr / total


def _apply_weighted_trimmed_range(
    values: np.ndarray,
    weights: np.ndarray,
    lower_pct: float,
    upper_pct: float
) -> Tuple[np.ndarray, np.ndarray]:
    if values.size < 3:
        return values, weights

    lower_pct = float(np.clip(lower_pct, 0.0, 0.5))
    upper_pct = float(np.clip(upper_pct, 0.5, 1.0))
    if upper_pct <= lower_pct:
        return values, weights

    order = np.argsort(values)
    sorted_values = values[order]
    sorted_weights = weights[order]

    cumulative = np.cumsum(sorted_weights)
    lower_bounds = cumulative - sorted_weights

    keep_mask = (cumulative > lower_pct) & (lower_bounds < upper_pct)
    trimmed_values = sorted_values[keep_mask]
    trimmed_weights = sorted_weights[keep_mask]

    if trimmed_values.size == 0:
        return values, weights

    trimmed_weights = _normalize_weights_array(trimmed_weights)
    return trimmed_values, trimmed_weights

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üìä Ï†úÏ°∞ ÏãúÍ∞Ñ ÌÜµÍ≥Ñ Í≥ÑÏÇ∞
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def calculate_manufacturing_time_stats(
    times_list: List[float],
    weights_list: List[float],
    config: TimeScenarioConfig = None,
    *,
    apply_trimmed: bool = False,
    trim_lower: Optional[float] = None,
    trim_upper: Optional[float] = None
) -> Dict[str, float]:
    """Ï†úÏ°∞ ÏãúÍ∞Ñ ÌÜµÍ≥Ñ Í≥ÑÏÇ∞"""
    if config is None:
        config = SCENARIO_CONFIG

    if not times_list:
        return {
            'mean': 0.0,
            'std': 0.0,
            'cv': 0.0,
            'optimal': 0.0,
            'standard': 0.0,
            'safe': 0.0,
            'samples': 0,
            'raw_samples': 0,
        }

    times = np.array(times_list, dtype=np.float64)
    weights = _normalize_weights_array(weights_list if weights_list else np.ones(len(times)))

    trimmed_times = times
    trimmed_weights = weights

    if (
        apply_trimmed
        and len(times) >= 3
        and getattr(config, 'TRIM_STD_ENABLED', True)
    ):
        lower = trim_lower if trim_lower is not None else getattr(config, 'TRIM_LOWER_PERCENT', 0.05)
        upper = trim_upper if trim_upper is not None else getattr(config, 'TRIM_UPPER_PERCENT', 0.95)
        trimmed_times, trimmed_weights = _apply_weighted_trimmed_range(times, weights, lower, upper)
        removed = len(times) - len(trimmed_times)
        if removed > 0:
            logger.debug(
                "Í∞ÄÏ§ëÏπò Ìä∏Î¶º Ï†ÅÏö©: Ï¥ù %dÍ∞ú Ï§ë %dÍ∞ú Ï†úÍ±∞ (ÌïòÏúÑ %.0f%%, ÏÉÅÏúÑ %.0f%%)",
                len(times),
                removed,
                lower * 100,
                (1 - upper) * 100,
            )

    mean = np.average(trimmed_times, weights=trimmed_weights)
    variance = np.average((trimmed_times - mean)**2, weights=trimmed_weights)
    std = np.sqrt(variance)
    cv = std / mean if mean > 0 else 0

    optimal_time = mean + config.OPTIMAL_SIGMA * std
    standard_time = mean
    safe_time = mean + config.SAFE_SIGMA * std

    return {
        'mean': mean,
        'std': std,
        'cv': cv,
        'optimal': optimal_time,
        'standard': standard_time,
        'safe': safe_time,
        'samples': len(trimmed_times),
        'raw_samples': len(times)
    }

def calculate_confidence_score(
    sample_count: int,
    avg_similarity: float,
    cv: float,
    config: TimeScenarioConfig
) -> float:
    """ÏòàÏ∏° Ïã†Î¢∞ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞"""
    sample_score = min(0.3, sample_count * 0.05)
    similarity_score = avg_similarity * 0.4
    if cv < config.CV_STABLE:
        cv_score = 0.3
    elif cv < config.CV_MODERATE:
        cv_score = 0.2
    elif cv < config.CV_VARIABLE:
        cv_score = 0.1
    else:
        cv_score = 0.0
    
    confidence = sample_score + similarity_score + cv_score
    if sample_count >= 5 and avg_similarity >= 0.8:
        confidence = min(1.0, confidence * 1.1)
    
    return min(1.0, confidence)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üöÄ ÎùºÏö∞ÌåÖ ÏòàÏ∏° Ìï®Ïàò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def predict_routing_from_similar_items(
    input_item_cd: str,
    similar_items: List[str],
    similarity_scores: List[float],
    config: TimeScenarioConfig = None,
    mode: str = "summary",
    routing_selection: str = "latest"
) -> pd.DataFrame:
    """
    Ïú†ÏÇ¨ ÌíàÎ™©Îì§Ïùò ÎùºÏö∞ÌåÖ Ï†ïÎ≥¥Î•º Í∏∞Î∞òÏúºÎ°ú ÏÉàÎ°úÏö¥ ÌíàÎ™©Ïùò ÎùºÏö∞ÌåÖ ÏòàÏ∏°
    """
    if config is None:
        config = SCENARIO_CONFIG
    
    logger.info(f"ÎùºÏö∞ÌåÖ ÏòàÏ∏° ÏãúÏûë - ÌíàÎ™©: {input_item_cd}, Ïú†ÏÇ¨Ìíà: {len(similar_items)}Í∞ú, Î™®Îìú: {mode}, ÎùºÏö∞ÌåÖ: {routing_selection}")
    
    # ÎîîÎ≤ÑÍπÖ: Ïú†ÏÇ¨ÌíàÍ≥º Ïú†ÏÇ¨ÎèÑ Ï∂úÎ†•
    for item, score in zip(similar_items[:5], similarity_scores[:5]):
        logger.debug(f"  Ïú†ÏÇ¨Ìíà: {item}, Ïú†ÏÇ¨ÎèÑ: {score:.3f}")
    
    # 1. Ïú†ÏÇ¨ÎèÑ ÌïÑÌÑ∞ÎßÅ - Îçî Ïú†Ïó∞ÌïòÍ≤å Ï≤òÎ¶¨
    filtered_items, filtered_scores = filter_by_similarity_threshold(
        similar_items, similarity_scores, MIN_SIMILARITY_THRESHOLD
    )
    
    # 2. Ïú†ÏÇ¨ÌíàÏùò ÎùºÏö∞ÌåÖ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå - ÎùºÏö∞ÌåÖÏù¥ ÏûàÎäî ÌíàÎ™©ÏùÑ Ï∞æÏùÑ ÎïåÍπåÏßÄ ÌôïÏû• ÌÉêÏÉâ
    all_routings = []
    items_with_routing = []
    items_without_routing = []
    checked_items = set()  # Ï§ëÎ≥µ Ï≤¥ÌÅ¨ Î∞©ÏßÄ
    
    # ÌïÑÌÑ∞ÎßÅÎêú ÌíàÎ™©Îì§Î∂ÄÌÑ∞ ÌôïÏù∏
    for i, item_cd in enumerate(filtered_items):
        if item_cd in checked_items:
            continue
        checked_items.add(item_cd)
        
        routing = fetch_routing_for_item(item_cd, latest_only=True, selection_mode=routing_selection)
        if not routing.empty:
            all_routings.append((item_cd, routing))
            items_with_routing.append(item_cd)
            
            # ROUT_NO Ï†ïÎ≥¥ Î°úÍπÖ
            rout_no = routing['ROUT_NO'].iloc[0] if 'ROUT_NO' in routing.columns else 'UNKNOWN'
            logger.info(f"  ÎùºÏö∞ÌåÖ Î∞úÍ≤¨: {item_cd} (Ïú†ÏÇ¨ÎèÑ: {filtered_scores[i]:.3f}, ROUT_NO: {rout_no})")
            
            # ‚≠ê Ï§ëÏöî: Îã®Ïùº ROUT_NO Í∑∏Î£πÎßå ÏÇ¨Ïö©ÌïòÎØÄÎ°ú Ï≤´ Î≤àÏß∏ ÎùºÏö∞ÌåÖÏùÑ Ï∞æÏúºÎ©¥ Ï§ëÎã®
            logger.info(f"Îã®Ïùº ROUT_NO Í∑∏Î£π ÏÇ¨Ïö©: {item_cd}Ïùò {rout_no} (Í≥µÏ†ï Ïàò: {len(routing)})")
            break  # Ï≤´ Î≤àÏß∏ ÎùºÏö∞ÌåÖÎßå ÏÇ¨Ïö©ÌïòÍ≥† Ï§ëÎã®
        else:
            items_without_routing.append(item_cd)
    
    # 3. ÎùºÏö∞ÌåÖÏù¥ ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥ Ïú†ÏÇ¨Ìíà Î™©Î°ùÏóêÏÑú Ï∂îÍ∞Ä ÌÉêÏÉâ
    if not all_routings and len(similar_items) > len(filtered_items):
        logger.info("ÌïÑÌÑ∞ÎßÅÎêú ÌíàÎ™©ÏóêÏÑú ÎùºÏö∞ÌåÖÏùÑ Ï∞æÏßÄ Î™ªÌï®. Ï†ÑÏ≤¥ Ïú†ÏÇ¨Ìíà Î™©Î°ùÏóêÏÑú Ï∂îÍ∞Ä ÌÉêÏÉâ...")
        
        # ÏõêÎûò Ïú†ÏÇ¨Ìíà Î™©Î°ùÏóêÏÑú ÏïÑÏßÅ ÌôïÏù∏ÌïòÏßÄ ÏïäÏùÄ ÌíàÎ™©Îì§ ÌôïÏù∏
        for i, item_cd in enumerate(similar_items):
            if item_cd in checked_items:
                continue
            checked_items.add(item_cd)
            
            routing = fetch_routing_for_item(item_cd, latest_only=True)
            if not routing.empty:
                all_routings.append((item_cd, routing))
                items_with_routing.append(item_cd)
                # ÏõêÎûò Ïú†ÏÇ¨ÎèÑ ÏÇ¨Ïö©ÌïòÎêò, filtered_scoresÏóêÎèÑ Ï∂îÍ∞Ä
                if item_cd not in filtered_items:
                    filtered_items.append(item_cd)
                    filtered_scores.append(similarity_scores[i])
                
                # ROUT_NO Ï†ïÎ≥¥ Î°úÍπÖ
                rout_no = routing['ROUT_NO'].iloc[0] if 'ROUT_NO' in routing.columns else 'UNKNOWN'
                logger.info(f"  ÎùºÏö∞ÌåÖ Î∞úÍ≤¨: {item_cd} (Ïú†ÏÇ¨ÎèÑ: {similarity_scores[i]:.3f}, ROUT_NO: {rout_no})")
                
                # ‚≠ê Ï§ëÏöî: Îã®Ïùº ROUT_NO Í∑∏Î£πÎßå ÏÇ¨Ïö©ÌïòÎØÄÎ°ú Ï≤´ Î≤àÏß∏ ÎùºÏö∞ÌåÖÏùÑ Ï∞æÏúºÎ©¥ Ï§ëÎã®
                logger.info(f"Îã®Ïùº ROUT_NO Í∑∏Î£π ÏÇ¨Ïö©: {item_cd}Ïùò {rout_no} (Í≥µÏ†ï Ïàò: {len(routing)})")
                break  # Ï≤´ Î≤àÏß∏ ÎùºÏö∞ÌåÖÎßå ÏÇ¨Ïö©ÌïòÍ≥† Ï§ëÎã®
            else:
                items_without_routing.append(item_cd)
    
    logger.info(f"ÎùºÏö∞ÌåÖ ÏûàÎäî ÌíàÎ™©: {len(items_with_routing)}Í∞ú - {items_with_routing[:5]}")
    logger.info(f"ÎùºÏö∞ÌåÖ ÏóÜÎäî ÌíàÎ™©: {len(items_without_routing)}Í∞ú - {items_without_routing[:5]}")
    logger.info(f"Ï†ÑÏ≤¥ ÌôïÏù∏Ìïú ÌíàÎ™©: {len(checked_items)}Í∞ú")
    
    # Îã®Ïùº ROUT_NO ÏÇ¨Ïö© Ï†ïÎ≥¥ Ï∂îÍ∞Ä Î°úÍπÖ
    if all_routings:
        used_item = all_routings[0][0]
        used_routing = all_routings[0][1]
        used_rout_no = used_routing['ROUT_NO'].iloc[0] if 'ROUT_NO' in used_routing.columns else 'UNKNOWN'
        logger.info(f"[ÏòàÏ∏°] Îã®Ïùº ROUT_NO Í∏∞Ï§Ä ÏòàÏ∏°: {used_item}Ïùò {used_rout_no} ÏÇ¨Ïö©")
    
    # 4. Ïó¨Ï†ÑÌûà ÎùºÏö∞ÌåÖÏùÑ Ï∞æÏßÄ Î™ªÌïú Í≤ΩÏö∞
    if not all_routings:
        logger.warning(f"Î™®Îì† Ïú†ÏÇ¨Ìíà ({len(checked_items)}Í∞ú) Ï§ë ÎùºÏö∞ÌåÖ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§: {input_item_cd}")
        
        # Ïú†ÏÇ¨ÎèÑÍ∞Ä ÎÜíÏùÄ ÏÉÅÏúÑ 5Í∞ú ÌíàÎ™© Ï†ïÎ≥¥ Ìè¨Ìï®
        top_items_info = []
        for i, (item, score) in enumerate(zip(similar_items[:5], similarity_scores[:5])):
            top_items_info.append(f"{i+1}. {item} (Ïú†ÏÇ¨ÎèÑ: {score:.3f})")
        
        return pd.DataFrame({
            'ITEM_CD': [input_item_cd],
            'MESSAGE': [f'ÏÉÅÏúÑ {len(checked_items)}Í∞ú Ïú†ÏÇ¨Ìíà Ï§ë ÎùºÏö∞ÌåÖ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏùå'],
            'SIMILAR_ITEMS_CHECKED': [', '.join(similar_items[:10])],
            'SIMILARITY_SCORES': [', '.join([f'{s:.3f}' for s in similarity_scores[:10]])],
            'TOP_SIMILAR_ITEMS': ['\n'.join(top_items_info)]
        })
    
    logger.info(f"ÎùºÏö∞ÌåÖ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏôÑÎ£å: {len(all_routings)}Í∞ú ÌíàÎ™©")
    
    if mode == "detailed":
        # ÏÉÅÏÑ∏ Î™®Îìú: Í≥µÏ†ïÎ≥Ñ ÎùºÏö∞ÌåÖ ÏòàÏ∏°
        process_predictions = defaultdict(list)
        
        for item_cd, routing_df in all_routings:
            try:
                item_index = filtered_items.index(item_cd)
                item_similarity = filtered_scores[item_index]
            except (ValueError, IndexError):
                logger.warning(f"Ïú†ÏÇ¨ÎèÑÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏùå: {item_cd}, Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©")
                item_similarity = 0.5  # Í∏∞Î≥∏Í∞í
            
            for _, row in routing_df.iterrows():
                proc_seq = safe_int_conversion(row.get('PROC_SEQ'), 0)
                proc_info = {
                    'ROUT_NO': str(row.get('ROUT_NO', '')),
                    'PROC_SEQ': proc_seq,
                    'JOB_CD': str(row.get('JOB_CD', '')),
                    'JOB_NM': str(row.get('JOB_NM', '')),
                    'RES_CD': str(row.get('RES_CD', '')),
                    'RES_DIS': str(row.get('RES_DIS', '')),
                    'INSIDE_FLAG': str(row.get('INSIDE_FLAG', 'ÏÇ¨ÎÇ¥')),
                    'TIME_UNIT': str(row.get('TIME_UNIT', 'Î∂Ñ(Min.)')),
                    'SETUP_TIME': safe_float_conversion(row.get('SETUP_TIME'), 0.0),
                    'RUN_TIME': safe_float_conversion(row.get('RUN_TIME'), 0.0),
                    'MACH_WORKED_HOURS': safe_float_conversion(
                        row.get('MACH_WORKED_HOURS')
                        or row.get('ACT_RUN_TIME')
                        or row.get('RUN_TIME'),
                        0.0,
                    ),
                    'ACT_SETUP_TIME': safe_float_conversion(row.get('ACT_SETUP_TIME') or row.get('SETUP_TIME'), 0.0),
                    'ACT_RUN_TIME': safe_float_conversion(row.get('ACT_RUN_TIME') or row.get('RUN_TIME'), 0.0),
                    'WAIT_TIME': safe_float_conversion(row.get('WAIT_TIME'), 0.0),
                    'MOVE_TIME': safe_float_conversion(row.get('MOVE_TIME'), 0.0),
                    'SOURCE_ITEM': item_cd,
                    'SIMILARITY': item_similarity
                }
                process_predictions[proc_seq].append(proc_info)
        
        predicted_routing = []
        
        # PROC_SEQ ÏàúÏÑúÎåÄÎ°ú Ï≤òÎ¶¨
        for proc_seq in sorted(process_predictions.keys()):
            proc_list = process_predictions[proc_seq]
            if not proc_list:
                continue
            
            # Í∞ÄÏû• ÎπàÎ≤àÌïú Í∞í ÏÑ†ÌÉù
            job_cd = Counter(p['JOB_CD'] for p in proc_list).most_common(1)[0][0]
            job_nm = Counter(p['JOB_NM'] for p in proc_list).most_common(1)[0][0]
            res_cd = Counter(p['RES_CD'] for p in proc_list).most_common(1)[0][0]
            res_dis = Counter(p['RES_DIS'] for p in proc_list).most_common(1)[0][0]
            inside_flag = Counter(p['INSIDE_FLAG'] for p in proc_list).most_common(1)[0][0]
            time_unit = 'Î∂Ñ(Min.)'  # Í≥†Ï†ïÍ∞í
            
            # ÏãúÍ∞Ñ Îç∞Ïù¥ÌÑ∞ Í≥ÑÏÇ∞
            setup_times = [p['SETUP_TIME'] for p in proc_list]
            run_times = [p['RUN_TIME'] for p in proc_list]
            mach_times = [p['MACH_WORKED_HOURS'] for p in proc_list]
            wait_times = [p['WAIT_TIME'] for p in proc_list]
            move_times = [p['MOVE_TIME'] for p in proc_list]
            similarities = [p['SIMILARITY'] for p in proc_list]

            _, weights = apply_similarity_weights(run_times, similarities, config.SIMILARITY_WEIGHT_POWER)

            if config.OUTLIER_DETECTION_ENABLED and len(run_times) >= 3:
                run_times, weights, valid_mask = remove_outliers_zscore(
                    run_times, weights, config.OUTLIER_Z_SCORE_THRESHOLD
                )
                setup_times = [val for val, keep in zip(setup_times, valid_mask) if keep]
                mach_times = [val for val, keep in zip(mach_times, valid_mask) if keep]
                wait_times = [val for val, keep in zip(wait_times, valid_mask) if keep]
                move_times = [val for val, keep in zip(move_times, valid_mask) if keep]
                similarities = [val for val, keep in zip(similarities, valid_mask) if keep]

            setup_stats = calculate_manufacturing_time_stats(setup_times, weights, config, apply_trimmed=True)
            run_stats = calculate_manufacturing_time_stats(run_times, weights, config)
            mach_stats = calculate_manufacturing_time_stats(mach_times, weights, config, apply_trimmed=True)
            wait_stats = calculate_manufacturing_time_stats(wait_times, weights, config)
            move_stats = calculate_manufacturing_time_stats(move_times, weights, config)

            similarity_mean = np.mean(similarities) if similarities else 0.0

            confidence = calculate_confidence_score(
                len(proc_list), similarity_mean, mach_stats['cv'], config
            )

            prediction = {
                'ROUT_NO': 'PREDICTED',
                'ITEM_CD': input_item_cd,
                'PROC_SEQ': proc_seq,
                'INSIDE_FLAG': inside_flag,
                'JOB_CD': job_cd,
                'JOB_NM': job_nm,
                'RES_CD': res_cd,
                'RES_DIS': res_dis,
                'TIME_UNIT': time_unit,
                'SETUP_TIME': round(setup_stats['mean'], 3),
                'RUN_TIME': round(run_stats['mean'], 3),
                'MACH_WORKED_HOURS': round(mach_stats['mean'], 3),
                'ACT_SETUP_TIME': round(setup_stats['mean'], 3),
                'ACT_RUN_TIME': round(mach_stats['mean'], 3),
                'WAIT_TIME': round(wait_stats['mean'], 3),
                'MOVE_TIME': round(move_stats['mean'], 3),
                'OPTIMAL_TIME': round(mach_stats['optimal'], 3),
                'STANDARD_TIME': round(mach_stats['standard'], 3),
                'SAFE_TIME': round(mach_stats['safe'], 3),
                'TIME_STD': round(mach_stats['std'], 3),
                'TIME_CV': round(mach_stats['cv'], 3),
                'SIMILAR_ITEMS_USED': len(proc_list),
                'AVG_SIMILARITY': round(similarity_mean, 3),
                'CONFIDENCE': round(confidence, 3),
                'SCENARIO': config.get_scenario_description(mach_stats['cv']),
                'SCENARIO_EMOJI': config.get_scenario_emoji(mach_stats['cv']),
                'SOURCE_ITEMS': ','.join([p['SOURCE_ITEM'] for p in proc_list]),
                'SIMILARITY_SCORES': ','.join([f"{p['SIMILARITY']:.3f}" for p in proc_list]),
                'WEIGHTS': ','.join([f"{w:.3f}" for w in weights]),
                'VALID_FROM_DT': '01-Jan-01',
                'VALID_TO_DT': '31-Dec-99',
                'INSRT_DT': datetime.now().strftime('%Y-%m-%d')
            }
            
            predicted_routing.append(prediction)
        
        if predicted_routing:
            routing_df = pd.DataFrame(predicted_routing)
            routing_df = routing_df.sort_values('PROC_SEQ').reset_index(drop=True)
            
            # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïª¨Îüº ÏàúÏÑúÏóê ÎßûÍ≤å Ïû¨Ï†ïÎ†¨
            base_cols = [
                'ROUT_NO', 'ITEM_CD', 'PROC_SEQ', 'INSIDE_FLAG', 'JOB_CD', 'JOB_NM',
                'RES_CD', 'RES_DIS', 'TIME_UNIT', 'SETUP_TIME', 'RUN_TIME',
                'MACH_WORKED_HOURS', 'ACT_SETUP_TIME', 'ACT_RUN_TIME', 'WAIT_TIME', 'MOVE_TIME',
                'OPTIMAL_TIME', 'STANDARD_TIME', 'SAFE_TIME', 'VALID_FROM_DT', 'VALID_TO_DT', 'INSRT_DT'
            ]
            extra_cols = [col for col in routing_df.columns if col not in base_cols]
            routing_df = routing_df[base_cols + extra_cols]
            
            logger.info(f"ÏÉÅÏÑ∏ ÎùºÏö∞ÌåÖ ÏòàÏ∏° ÏôÑÎ£å: {len(routing_df)}Í∞ú Í≥µÏ†ï")
            return routing_df
        else:
            logger.warning(f"ÏòàÏ∏°Ìï† Ïàò ÏûàÎäî Í≥µÏ†ïÏù¥ ÏóÜÏäµÎãàÎã§: {input_item_cd}")
            return pd.DataFrame({
                'ITEM_CD': [input_item_cd],
                'MESSAGE': ['ÏòàÏ∏°Îêú Í≥µÏ†ï Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå']
            })
    
    else:
        # ÏöîÏïΩ Î™®Îìú: Ï¥ùÌï© ÏãúÍ∞Ñ ÏãúÎÇòÎ¶¨Ïò§ ÏòàÏ∏°
        total_setup_times = []
        total_run_times = []
        total_wait_times = []
        total_move_times = []
        total_mach_hours = []
        similarities = []
        source_items = []

        for item_cd, routing_df in all_routings:
            try:
                item_index = filtered_items.index(item_cd)
                item_similarity = filtered_scores[item_index]
            except (ValueError, IndexError):
                logger.warning(f"Ïú†ÏÇ¨ÎèÑÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏùå: {item_cd}, Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©")
                item_similarity = 0.5  # Í∏∞Î≥∏Í∞í
                
            setup_sum = routing_df['SETUP_TIME'].sum()
            run_sum = routing_df['RUN_TIME'].sum()
            wait_sum = routing_df['WAIT_TIME'].sum()
            move_sum = routing_df['MOVE_TIME'].sum()
            if 'MACH_WORKED_HOURS' in routing_df.columns:
                mach_series = pd.to_numeric(routing_df['MACH_WORKED_HOURS'], errors='coerce').fillna(0.0)
            elif 'ACT_RUN_TIME' in routing_df.columns:
                mach_series = pd.to_numeric(routing_df['ACT_RUN_TIME'], errors='coerce').fillna(0.0)
            else:
                mach_series = pd.to_numeric(routing_df['RUN_TIME'], errors='coerce').fillna(0.0)
            mach_sum = float(mach_series.sum())

            total_setup_times.append(setup_sum)
            total_run_times.append(run_sum)
            total_wait_times.append(wait_sum)
            total_move_times.append(move_sum)
            total_mach_hours.append(mach_sum)
            similarities.append(item_similarity)
            source_items.append(item_cd)

        _, weights = apply_similarity_weights(total_run_times, similarities, config.SIMILARITY_WEIGHT_POWER)

        if config.OUTLIER_DETECTION_ENABLED and len(total_run_times) >= 3:
            total_run_times, weights, valid_mask = remove_outliers_zscore(
                total_run_times, weights, config.OUTLIER_Z_SCORE_THRESHOLD
            )
            total_setup_times = [val for val, keep in zip(total_setup_times, valid_mask) if keep]
            total_wait_times = [val for val, keep in zip(total_wait_times, valid_mask) if keep]
            total_move_times = [val for val, keep in zip(total_move_times, valid_mask) if keep]
            total_mach_hours = [val for val, keep in zip(total_mach_hours, valid_mask) if keep]
            similarities = [val for val, keep in zip(similarities, valid_mask) if keep]
            source_items = [val for val, keep in zip(source_items, valid_mask) if keep]

        setup_stats = calculate_manufacturing_time_stats(total_setup_times, weights, config, apply_trimmed=True)
        run_stats = calculate_manufacturing_time_stats(total_run_times, weights, config)
        mach_stats = calculate_manufacturing_time_stats(total_mach_hours, weights, config, apply_trimmed=True)
        wait_stats = calculate_manufacturing_time_stats(total_wait_times, weights, config)
        move_stats = calculate_manufacturing_time_stats(total_move_times, weights, config)

        similarity_mean = np.mean(similarities) if similarities else 0.0

        confidence = calculate_confidence_score(
            len(all_routings), similarity_mean, mach_stats['cv'], config
        )

        prediction = {
            'ITEM_CD': input_item_cd,
            'SETUP_TIME': round(setup_stats['mean'], 3),
            'RUN_TIME': round(run_stats['mean'], 3),
            'MACH_WORKED_HOURS': round(mach_stats['mean'], 3),
            'ACT_SETUP_TIME': round(setup_stats['mean'], 3),
            'ACT_RUN_TIME': round(mach_stats['mean'], 3),
            'WAIT_TIME': round(wait_stats['mean'], 3),
            'MOVE_TIME': round(move_stats['mean'], 3),
            'OPTIMAL_TIME': round(mach_stats['optimal'], 3),
            'STANDARD_TIME': round(mach_stats['standard'], 3),
            'SAFE_TIME': round(mach_stats['safe'], 3),
            'TIME_STD': round(mach_stats['std'], 3),
            'TIME_CV': round(mach_stats['cv'], 3),
            'SIMILAR_ITEMS_USED': len(all_routings),
            'AVG_SIMILARITY': round(similarity_mean, 3),
            'CONFIDENCE': round(confidence, 3),
            'SCENARIO': config.get_scenario_description(mach_stats['cv']),
            'SCENARIO_EMOJI': config.get_scenario_emoji(mach_stats['cv']),
            'SOURCE_ITEMS': ','.join(source_items),
            'SIMILARITY_SCORES': ','.join([f"{s:.3f}" for s in similarities]),
            'WEIGHTS': ','.join([f"{w:.3f}" for w in weights]),
            'VALID_FROM_DT': '01-Jan-01',
            'VALID_TO_DT': '31-Dec-99',
            'INSRT_DT': datetime.now().strftime('%Y-%m-%d')
        }
        
        routing_df = pd.DataFrame([prediction])
        logger.info(f"ÏöîÏïΩ ÎùºÏö∞ÌåÖ ÏòàÏ∏° ÏôÑÎ£å: 1Í∞ú Î†àÏΩîÎìú")
        return routing_df

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üìä ÌíàÏßà Í≤ÄÏ¶ù Í∞úÏÑ†
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def validate_prediction_quality_enhanced(
    routing_df: pd.DataFrame, 
    model_info: Dict[str, Any] = None
) -> Dict[str, Any]:
    if routing_df.empty:
        return {'error': 'No predictions to validate'}
    
    metrics = {}
    
    if 'CONFIDENCE' in routing_df.columns:
        metrics['avg_confidence'] = routing_df['CONFIDENCE'].mean()
        metrics['min_confidence'] = routing_df['CONFIDENCE'].min()
        metrics['max_confidence'] = routing_df['CONFIDENCE'].max()
        metrics['high_confidence_ratio'] = (routing_df['CONFIDENCE'] >= 0.7).sum() / len(routing_df)
    
    if 'AVG_SIMILARITY' in routing_df.columns:
        metrics['avg_similarity'] = routing_df['AVG_SIMILARITY'].mean()
    
    if 'SIMILAR_ITEMS_USED' in routing_df.columns:
        metrics['avg_similar_items'] = routing_df['SIMILAR_ITEMS_USED'].mean()
    
    time_cols = ['SETUP_TIME', 'RUN_TIME']
    for col in time_cols:
        if col in routing_df.columns:
            metrics[f'total_{col.lower()}'] = routing_df[col].sum()
    
    if model_info:
        quality_score = 0.5
        if model_info.get('is_enhanced'):
            quality_score += 0.1
        if model_info.get('has_pca'):
            quality_score += 0.05
        if model_info.get('has_feature_weights'):
            quality_score += 0.1
        
        avg_conf = metrics.get('avg_confidence', 0.5)
        quality_score = quality_score * (0.5 + avg_conf * 0.5)
        
        metrics['model_quality_score'] = min(1.0, quality_score)
    
    avg_conf = metrics.get('avg_confidence', 0)
    model_score = metrics.get('model_quality_score', 0.5)
    combined_score = (avg_conf + model_score) / 2
    
    if combined_score >= 0.8:
        metrics['quality_grade'] = 'A (Ïö∞Ïàò)'
    elif combined_score >= 0.7:
        metrics['quality_grade'] = 'B (ÏñëÌò∏)'
    elif combined_score >= 0.6:
        metrics['quality_grade'] = 'C (Î≥¥ÌÜµ)'
    else:
        metrics['quality_grade'] = 'D (Í∞úÏÑ†ÌïÑÏöî)'
    
    suggestions = []
    if avg_conf < 0.7:
        suggestions.append("Îçî ÎßéÏùÄ Ïú†ÏÇ¨ ÌíàÎ™© Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÌïÑÏöî")
    if not model_info or not model_info.get('is_enhanced'):
        suggestions.append("Í∞úÏÑ†Îêú Î™®Îç∏Î°ú Ïû¨ÌïôÏäµ Í∂åÏû•")
    if metrics.get('avg_similar_items', 0) < 5:
        suggestions.append("Ïú†ÏÇ¨Ìíà Í≤ÄÏÉâ Î≤îÏúÑ(top_k) ÌôïÎåÄ Í≥†Î†§")
    
    metrics['improvement_suggestions'] = suggestions
    
    return metrics

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîß Í∏∞Ï°¥ API Ìò∏ÌôòÏÑ± Ïú†ÏßÄ
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def predict_single_item_with_ml_optimized(
    item_cd: str,
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
    mode: str = "summary"
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    routing_df, cand_df, _ = predict_single_item_with_ml_enhanced(
        item_cd, model_dir, top_k=top_k, miss_thr=miss_thr, 
        config=config, mode=mode
    )
    
    if not cand_df.empty:
        has_routing = []
        routing_counts = []
        
        for candidate in cand_df['CANDIDATE_ITEM_CD']:
            routing = fetch_routing_for_item(candidate)
            if not routing.empty:
                has_routing.append("‚úì ÏûàÏùå")
                routing_counts.append(len(routing))
            else:
                has_routing.append("‚úó ÏóÜÏùå")
                routing_counts.append(0)
        
        cand_df['HAS_ROUTING'] = has_routing
        cand_df['PROCESS_COUNT'] = routing_counts
        
        cols = list(cand_df.columns)
        if 'SIMILARITY_SCORE' in cols and 'HAS_ROUTING' in cols:
            cols.remove('HAS_ROUTING')
            cols.remove('PROCESS_COUNT')
            idx = cols.index('SIMILARITY_SCORE') + 1
            cols.insert(idx, 'HAS_ROUTING')
            cols.insert(idx + 1, 'PROCESS_COUNT')
            cand_df = cand_df[cols]
    
    return routing_df, cand_df

def predict_items_with_ml_optimized(
    item_codes: List[str],
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
    mode: str = "summary"
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if config is None:
        config = SCENARIO_CONFIG

    logger.info(f"üöÄ Enhanced ML Î∞∞Ïπò ÏòàÏ∏° ÏãúÏûë: {len(item_codes)}Í∞ú ÌíàÎ™©")

    predicted_routings: Dict[str, pd.DataFrame] = {}
    raw_candidate_frames: List[pd.DataFrame] = []
    routing_cache: Dict[str, pd.DataFrame] = {}

    for item_cd in item_codes:
        routing_df, cand_df, _ = predict_single_item_with_ml_enhanced(
            item_cd, model_dir, top_k=top_k, miss_thr=miss_thr,
            config=config, mode=mode
        )

        predicted_routings[item_cd] = routing_df if routing_df is not None else pd.DataFrame()

        if not cand_df.empty:
            cand_df['ITEM_CD'] = item_cd
            raw_candidate_frames.append(cand_df)
            for cand_item in cand_df['CANDIDATE_ITEM_CD']:
                if cand_item not in routing_cache:
                    routing_cache[cand_item] = fetch_routing_for_item(cand_item)

    composed_frames: List[pd.DataFrame] = []
    enhanced_candidates: List[Dict[str, Any]] = []
    per_item_counts: Dict[str, int] = defaultdict(int)

    # ML ÏòàÏ∏° Ï°∞Ìï©(Ï°¥Ïû¨ Ïãú) Ïö∞ÏÑ† Ï∂îÍ∞Ä
    for item_cd, predicted_df in predicted_routings.items():
        if predicted_df is None or predicted_df.empty:
            continue
        signature = build_routing_signature(predicted_df)
        candidate_id = f"{item_cd}_MLP"
        normalized = normalize_routing_frame(
            item_cd,
            candidate_id,
            predicted_df,
            similarity=1.0,
            reference_item="ML_PREDICTED",
            priority="primary",
            signature=signature,
        )
        if normalized.empty:
            continue
        composed_frames.append(normalized)
        per_item_counts[item_cd] += 1
        enhanced_candidates.append({
            'ITEM_CD': item_cd,
            'CANDIDATE_ITEM_CD': 'ML_PREDICTED',
            'SIMILARITY_SCORE': 1.0,
            'ROUTING_SIGNATURE': signature,
            'ROUTING_SUMMARY': f"ML ÏòàÏ∏° Í≥µÏ†ï {len(normalized)}Í∞ú",
            'PRIORITY': 'primary',
            'SIMILARITY_TIER': 'HIGH',
            'HAS_ROUTING': '‚úì ÏûàÏùå',
            'PROCESS_COUNT': len(normalized),
        })


    raw_candidates_df = (
        pd.concat(raw_candidate_frames, ignore_index=True)
        if raw_candidate_frames
        else pd.DataFrame()
    )

    if not raw_candidates_df.empty:
        raw_candidates_df = raw_candidates_df.sort_values(
            ['ITEM_CD', 'SIMILARITY_SCORE'], ascending=[True, False]
        )

        for _, cand_row in raw_candidates_df.iterrows():
            item_cd = cand_row.get('ITEM_CD')
            candidate_item = cand_row.get('CANDIDATE_ITEM_CD')
            similarity = float(cand_row.get('SIMILARITY_SCORE', 0.0))

            if not item_cd or not candidate_item:
                continue

            if per_item_counts[item_cd] >= MAX_ROUTING_VARIANTS:
                continue

            routing = routing_cache.get(candidate_item)
            if routing is None or routing.empty:
                continue

            signature = build_routing_signature(routing)
            priority = 'primary' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'fallback'
            candidate_index = per_item_counts[item_cd] + 1
            candidate_id = f"{item_cd}_C{candidate_index:02d}"

            normalized = normalize_routing_frame(
                item_cd,
                candidate_id,
                routing,
                similarity=similarity,
                reference_item=candidate_item,
                priority=priority,
                signature=signature,
            )

            if normalized.empty:
                continue

            composed_frames.append(normalized)
            per_item_counts[item_cd] += 1

            process_count = len(normalized)
            summary_text = f"Í≥µÏ†ï {process_count}Í∞ú / Ïú†ÏÇ¨ÎèÑ {similarity:.2f}"
            enhanced_candidates.append({
                'ITEM_CD': item_cd,
                'CANDIDATE_ITEM_CD': candidate_item,
                'SIMILARITY_SCORE': similarity,
                'ROUTING_SIGNATURE': signature,
                'ROUTING_SUMMARY': summary_text,
                'PRIORITY': priority,
                'SIMILARITY_TIER': 'HIGH' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'LOW',
                'HAS_ROUTING': '‚úì ÏûàÏùå',
                'PROCESS_COUNT': process_count,
            })



    raw_candidates_df = (
        pd.concat(raw_candidate_frames, ignore_index=True)
        if raw_candidate_frames
        else pd.DataFrame()
    )


    raw_candidates_df = (
        pd.concat(raw_candidate_frames, ignore_index=True)
        if raw_candidate_frames
        else pd.DataFrame()
    )

    if not raw_candidates_df.empty:
        raw_candidates_df = raw_candidates_df.sort_values(
            ['ITEM_CD', 'SIMILARITY_SCORE'], ascending=[True, False]
        )

        for _, cand_row in raw_candidates_df.iterrows():
            item_cd = cand_row.get('ITEM_CD')
            candidate_item = cand_row.get('CANDIDATE_ITEM_CD')
            similarity = float(cand_row.get('SIMILARITY_SCORE', 0.0))

            if not item_cd or not candidate_item:
                continue

            if per_item_counts[item_cd] >= MAX_ROUTING_VARIANTS:
                continue

            routing = routing_cache.get(candidate_item)
            if routing is None or routing.empty:
                continue

            signature = build_routing_signature(routing)
            priority = 'primary' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'fallback'
            candidate_index = per_item_counts[item_cd] + 1
            candidate_id = f"{item_cd}_C{candidate_index:02d}"

            normalized = normalize_routing_frame(
                item_cd,
                candidate_id,
                routing,
                similarity=similarity,
                reference_item=candidate_item,
                priority=priority,
                signature=signature,
            )

            if normalized.empty:
                continue

            composed_frames.append(normalized)
            per_item_counts[item_cd] += 1

            process_count = len(normalized)
            summary_text = f"Í≥µÏ†ï {process_count}Í∞ú / Ïú†ÏÇ¨ÎèÑ {similarity:.2f}"
            enhanced_candidates.append({
                'ITEM_CD': item_cd,
                'CANDIDATE_ITEM_CD': candidate_item,
                'SIMILARITY_SCORE': similarity,
                'ROUTING_SIGNATURE': signature,
                'ROUTING_SUMMARY': summary_text,
                'PRIORITY': priority,
                'SIMILARITY_TIER': 'HIGH' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'LOW',
                'HAS_ROUTING': '‚úì ÏûàÏùå',
                'PROCESS_COUNT': process_count,
            })


    final_routing_df = (
        pd.concat(composed_frames, ignore_index=True)
        if composed_frames
        else pd.DataFrame()
    )

    final_cand_df = (
        pd.DataFrame(enhanced_candidates)
        if enhanced_candidates
        else pd.DataFrame()
    )

    logger.info(
        "Î∞∞Ïπò ÏòàÏ∏° ÏôÑÎ£å: %dÍ∞ú ÎùºÏö∞ÌåÖ Ï°∞Ìï©, %dÍ∞ú ÌõÑÎ≥¥",
        len(final_routing_df),
        len(final_cand_df),
    )

    return final_routing_df, final_cand_df

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ‚öôÔ∏è Îü∞ÌÉÄÏûÑ ÏÑ§Ï†ï Ï†ÅÏö© Ìó¨Ìçº
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


def apply_runtime_config(runtime: PredictorRuntimeConfig) -> None:
    """ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ï†ÄÏû•ÏÜåÏóêÏÑú Ï†ÑÎã¨Îêú ÏÑ§Ï†ïÏùÑ Ï¶âÏãú Î∞òÏòÅÌïúÎã§."""

    global SIMILARITY_HIGH_THRESHOLD, MIN_SIMILARITY_THRESHOLD, MAX_ROUTING_VARIANTS

    SIMILARITY_HIGH_THRESHOLD = runtime.similarity_high_threshold
    MIN_SIMILARITY_THRESHOLD = runtime.similarity_high_threshold
    MAX_ROUTING_VARIANTS = runtime.max_routing_variants

    SCENARIO_CONFIG.TRIM_STD_ENABLED = runtime.trim_std_enabled
    SCENARIO_CONFIG.TRIM_LOWER_PERCENT = runtime.trim_lower_percent
    SCENARIO_CONFIG.TRIM_UPPER_PERCENT = runtime.trim_upper_percent

    logger.info(
        "Îü∞ÌÉÄÏûÑ ÏÑ§Ï†ï Í∞±Ïã†: threshold=%.2f, variants=%d, trim_std=%s (%.2f~%.2f)",
        SIMILARITY_HIGH_THRESHOLD,
        MAX_ROUTING_VARIANTS,
        SCENARIO_CONFIG.TRIM_STD_ENABLED,
        SCENARIO_CONFIG.TRIM_LOWER_PERCENT,
        SCENARIO_CONFIG.TRIM_UPPER_PERCENT,
    )


try:  # Î™®Îìà Î°úÎìú Ïãú Ï¥àÍ∏∞ ÏÑ§Ï†ï Ï†ÅÏö©
    apply_runtime_config(workflow_config_store.get_predictor_runtime())
except Exception as exc:  # pragma: no cover - ÏÑ§Ï†ï ÌååÏùº ÎØ∏Ï°¥Ïû¨ Îì±
    logger.debug("Í∏∞Î≥∏ Îü∞ÌÉÄÏûÑ ÏÑ§Ï†ïÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§: %s", exc)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîß Î†àÍ±∞Ïãú Ìò∏ÌôòÏÑ± Ìï®Ïàò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def predict_single_item_with_statistics_improved(
    item_cd: str,
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    return predict_single_item_with_ml_optimized(
        item_cd, model_dir, top_k=top_k, miss_thr=miss_thr, 
        config=config, mode="summary"
    )

def predict_items_with_ml_improved(
    item_codes: List[str],
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    return predict_items_with_ml_optimized(
        item_codes, model_dir, top_k=top_k, miss_thr=miss_thr,
        config=config, mode="summary"
    )

def predict_single_item_with_ml(
    item_cd: str,
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    return predict_single_item_with_ml_optimized(
        item_cd, model_dir, top_k=top_k, miss_thr=miss_thr, mode="summary"
    )

def predict_items_with_ml(
    item_codes: List[str],
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    return predict_items_with_ml_optimized(
        item_codes, model_dir, top_k=top_k, miss_thr=miss_thr, mode="summary"
    )

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üì§ Î™®Îìà ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Î∞è ÏÑ§Ï†ï Ìï®Ïàò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_scenario_config() -> TimeScenarioConfig:
    return SCENARIO_CONFIG

def set_scenario_config(config: TimeScenarioConfig):
    global SCENARIO_CONFIG
    SCENARIO_CONFIG = config
    logger.info("ÏãúÎÇòÎ¶¨Ïò§ ÏÑ§Ï†ïÏù¥ ÏóÖÎç∞Ïù¥Ìä∏ÎêòÏóàÏäµÎãàÎã§")

def reset_scenario_config():
    global SCENARIO_CONFIG
    SCENARIO_CONFIG = TimeScenarioConfig()
    logger.info("ÏãúÎÇòÎ¶¨Ïò§ ÏÑ§Ï†ïÏù¥ Ï¥àÍ∏∞Í∞íÏúºÎ°ú Ïû¨ÏÑ§Ï†ïÎêòÏóàÏäµÎãàÎã§")

__all__ = [
    "TimeScenarioConfig",
    "get_scenario_config",
    "set_scenario_config",
    "reset_scenario_config",
    "predict_single_item_with_ml_enhanced",
    "EnhancedModelManager",
    "validate_prediction_quality_enhanced",
    "predict_single_item_with_ml_optimized",
    "predict_items_with_ml_optimized",
    "predict_routing_from_similar_items",
    "predict_single_item_with_statistics_improved",
    "predict_items_with_ml_improved",
    "predict_single_item_with_ml",
    "predict_items_with_ml",
    "calculate_manufacturing_time_stats",
    "filter_by_similarity_threshold",
    "apply_similarity_weights",
    "safe_int_conversion",
    "safe_float_conversion",
    "apply_runtime_config",
]