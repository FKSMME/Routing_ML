# backend/predictor_ml.py
from __future__ import annotations

# â”€â”€ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
from dataclasses import dataclass
from pathlib import Path
from typing import List, Tuple, Dict, Optional, Any, Union
import json
import logging
from collections import Counter, defaultdict
from threading import RLock
import time
import numpy as np
import pandas as pd
from datetime import datetime

# â”€â”€ ì„œë“œíŒŒí‹°
from cachetools import LRUCache

# â”€â”€ ì‚¬ë‚´ ëª¨ë“ˆ
from backend.constants import (
    NUMERIC_FEATURES,
    ROUTING_OUTPUT_COLS,
    get_routing_alias_map,
    get_routing_output_columns,
)
from common.config_store import PredictorRuntimeConfig, workflow_config_store
from backend.trainer_ml import load_optimized_model
from backend.database import (
    fetch_single_item, 
    fetch_routing_for_item, 
    check_item_has_routing,
)
from common.logger import get_logger

# ê°œì„ ëœ save_load ëª¨ë“ˆ import
try:
    from models.save_load import load_model_with_metadata
except ImportError:
    load_model_with_metadata = None

logger = get_logger("predictor_ml_improved", level=logging.DEBUG)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¤ì •ê°’
DEFAULT_TOP_K: int = 10
MISSING_RATIO_THRESHOLD: float = 0.50
MIN_SAMPLES_FOR_STATS: int = 1
CONFIDENCE_THRESHOLD: float = 0.0
SIMILARITY_HIGH_THRESHOLD: float = 0.8
MIN_SIMILARITY_THRESHOLD: float = SIMILARITY_HIGH_THRESHOLD
MAX_ROUTING_VARIANTS: int = 4

# ì œì™¸í•  ì»¬ëŸ¼ë“¤ - ì¼ê´€ì„± ìˆê²Œ ì •ì˜
COLUMNS_TO_EXCLUDE = ['DRAW_USE', 'ITEM_NM_ENG', 'MID_SEALSIZE_UOM', 'ROTATE_CTRCLOCKWISE']

ROUTING_ALIAS_MAP = {
    'JOB_CD': 'dbo_BI_ROUTING_VIEW_JOB_CD',
    'CUST_NM': 'dbo_BI_ROUTING_VIEW_CUST_NM',
    'VIEW_REMARK': 'dbo_BI_ROUTING_VIEW_REMARK',
}


@dataclass
class ModelCacheEntry:
    components: Dict[str, Any]
    metadata: Dict[str, Any]
    is_enhanced: bool


_MODEL_CACHE_LOCK = RLock()
_MODEL_CACHE: Dict[str, ModelCacheEntry] = {}

DEFAULT_ENCODING_CACHE_MAXSIZE: int = 20000
DEFAULT_ENCODING_CACHE_TTL_SECONDS: int = 900

ENCODING_CACHE_MAXSIZE: int = DEFAULT_ENCODING_CACHE_MAXSIZE
ENCODING_CACHE_TTL_SECONDS: int = DEFAULT_ENCODING_CACHE_TTL_SECONDS

_ENCODING_CACHE_LOCK = RLock()
_ENCODING_CACHE: Optional[LRUCache[str, Tuple[float, Tuple[np.ndarray, float]]]] = LRUCache(
    maxsize=DEFAULT_ENCODING_CACHE_MAXSIZE
)


def _active_alias_map() -> Dict[str, str]:
    """í˜„ì¬ ì„¤ì • ê¸°ë°˜ ì»¬ëŸ¼ ë³„ì¹­ì„ ë°˜í™˜í•œë‹¤."""

    try:
        return get_routing_alias_map()
    except Exception:  # pragma: no cover - ì„¤ì • íŒŒì¼ ì†ìƒ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
        return dict(ROUTING_ALIAS_MAP)


def _configure_encoding_cache(maxsize: int, ttl_seconds: int) -> None:
    """ì¬ì‚¬ìš© ì¸ì½”ë”© ìºì‹œë¥¼ ì¬êµ¬ì„±í•œë‹¤."""

    global ENCODING_CACHE_MAXSIZE, ENCODING_CACHE_TTL_SECONDS, _ENCODING_CACHE

    ENCODING_CACHE_MAXSIZE = max(0, int(maxsize))
    ENCODING_CACHE_TTL_SECONDS = max(0, int(ttl_seconds))

    with _ENCODING_CACHE_LOCK:
        if ENCODING_CACHE_MAXSIZE == 0:
            _ENCODING_CACHE = None
        else:
            _ENCODING_CACHE = LRUCache(maxsize=ENCODING_CACHE_MAXSIZE)


def _normalize_value_for_cache(value: Any) -> Any:
    """ìºì‹œ í‚¤ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ì…ë ¥ ê°’ì„ ì •ê·œí™”í•œë‹¤."""

    if isinstance(value, (pd.Timestamp, datetime)):
        return value.isoformat()

    if isinstance(value, np.generic):
        value = value.item()

    if isinstance(value, (list, tuple)):
        return [_normalize_value_for_cache(v) for v in value]

    if isinstance(value, dict):
        return {str(k): _normalize_value_for_cache(v) for k, v in sorted(value.items())}

    try:
        if pd.isna(value):
            return None
    except Exception:
        pass

    return value


def _build_encoding_cache_key(df: pd.DataFrame) -> str:
    """ì…ë ¥ ë ˆì½”ë“œë¥¼ ì§ë ¬í™”í•˜ì—¬ ìºì‹œ í‚¤ë¥¼ ìƒì„±í•œë‹¤."""

    if df.empty:
        return "{}"

    record = df.iloc[0]
    normalized: Dict[str, Any] = {}
    for column in sorted(df.columns):
        normalized[column] = _normalize_value_for_cache(record.get(column))

    return json.dumps(normalized, sort_keys=True, ensure_ascii=False, default=str)


def _get_cached_encoding(key: str) -> Optional[Tuple[np.ndarray, float]]:
    cache = _ENCODING_CACHE
    if cache is None:
        return None

    with _ENCODING_CACHE_LOCK:
        entry = cache.get(key)

    if entry is None:
        return None

    timestamp, payload = entry

    if ENCODING_CACHE_TTL_SECONDS > 0 and (time.time() - timestamp) > ENCODING_CACHE_TTL_SECONDS:
        with _ENCODING_CACHE_LOCK:
            try:
                cache.pop(key, None)
            except Exception:
                pass
        return None

    return payload


def _store_cached_encoding(key: str, value: Tuple[np.ndarray, float]) -> None:
    cache = _ENCODING_CACHE
    if cache is None:
        return

    with _ENCODING_CACHE_LOCK:
        cache[key] = (time.time(), value)


def _has_enhanced_model(model_dir: Path) -> bool:
    enhanced_files = [
        'training_metadata.json',
        'feature_metadata.json',
        'model_metadata.json'
    ]
    return any((model_dir / filename).exists() for filename in enhanced_files)


def _load_legacy_weights(model_dir: Path, feature_columns: List[str]) -> Optional[Dict[str, float]]:
    weights_path = model_dir / "feature_weights.joblib"
    if weights_path.exists():
        try:
            import joblib

            weights = joblib.load(weights_path)
            if isinstance(weights, dict):
                return {k: v for k, v in weights.items() if k not in COLUMNS_TO_EXCLUDE}
            return weights
        except Exception as exc:
            logger.warning(f"joblib ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {exc}")

    npy_path = model_dir / "feature_weights.npy"
    if npy_path.exists():
        try:
            weights_array = np.load(npy_path)
            if feature_columns and len(weights_array) >= len(feature_columns):
                return {
                    col: float(weights_array[i])
                    for i, col in enumerate(feature_columns)
                    if col not in COLUMNS_TO_EXCLUDE
                }
        except Exception as exc:
            logger.warning(f"numpy ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {exc}")

    return None


def _load_model_entry(model_dir: Path) -> ModelCacheEntry:
    metadata: Dict[str, Any] = {}
    components: Dict[str, Any] = {}
    is_enhanced = False

    if load_model_with_metadata and _has_enhanced_model(model_dir):
        try:
            logger.info("ê°œì„ ëœ ëª¨ë¸ ë¡œë“œ ì‹œë„...")
            enhanced_components = load_model_with_metadata(model_dir, load_sample_data=False)

            feature_weights = enhanced_components.get('feature_weights')
            if feature_weights is None or isinstance(feature_weights, (dict, np.ndarray)):
                try:
                    from backend.feature_weights import FeatureWeightManager

                    fw_manager = FeatureWeightManager(model_dir)
                    fw_manager.load_weights()
                    feature_weights = fw_manager
                    logger.debug("FeatureWeightManagerë¡œ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
                except Exception as exc:
                    logger.debug(f"FeatureWeightManager ë¡œë“œ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©: {exc}")
                    if feature_weights is None:
                        feature_columns = enhanced_components.get('feature_columns') or []
                        feature_weights = _load_legacy_weights(
                            model_dir, [c for c in feature_columns if c not in COLUMNS_TO_EXCLUDE]
                        )

            feature_columns = [
                c for c in enhanced_components.get('feature_columns', [])
                if c not in COLUMNS_TO_EXCLUDE
            ]

            components = {
                'searcher': enhanced_components['searcher'],
                'encoder': enhanced_components['encoder'],
                'scaler': enhanced_components['scaler'],
                'feature_columns': feature_columns,
                'pca': enhanced_components.get('pca'),
                'variance_selector': enhanced_components.get('variance_selector'),
                'feature_weights': feature_weights,
                'active_features': enhanced_components.get('active_features'),
            }

            metadata = {
                'model_metadata': enhanced_components.get('model_metadata', {}),
                'feature_metadata': enhanced_components.get('feature_metadata', {}),
                'training_metadata': enhanced_components.get('training_metadata', {}),
                'vector_statistics': enhanced_components.get('vector_statistics', {}),
            }

            is_enhanced = True
            logger.info("âœ… ê°œì„ ëœ ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ìºì‹œ ì €ì¥)")
            return ModelCacheEntry(components=components, metadata=metadata, is_enhanced=is_enhanced)
        except Exception as exc:
            logger.warning(f"ê°œì„ ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ë¡œë“œ ì‹œë„: {exc}")

    searcher, encoder, scaler, feature_columns = load_optimized_model(model_dir)

    clean_feature_columns = [
        c for c in feature_columns
        if c not in COLUMNS_TO_EXCLUDE
    ]

    components = {
        'searcher': searcher,
        'encoder': encoder,
        'scaler': scaler,
        'feature_columns': clean_feature_columns,
        'pca': None,
        'variance_selector': None,
        'feature_weights': _load_legacy_weights(model_dir, clean_feature_columns),
        'active_features': None,
    }

    logger.info("âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì„±ê³µ (ìºì‹œ ì €ì¥)")
    return ModelCacheEntry(components=components, metadata=metadata, is_enhanced=is_enhanced)


def get_loaded_model(model_dir: Union[str, Path]) -> ModelCacheEntry:
    """ëª¨ë¸ ë””ë ‰í„°ë¦¬ë³„ ë¡œë“œ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ê³  ì¬í™œìš©í•œë‹¤."""

    path = Path(model_dir).resolve()
    cache_key = str(path)

    with _MODEL_CACHE_LOCK:
        cached = _MODEL_CACHE.get(cache_key)
        if cached is not None:
            logger.debug("ëª¨ë¸ ìºì‹œ íˆíŠ¸: %s", cache_key)
            return cached

        logger.debug("ëª¨ë¸ ìºì‹œ ë¯¸ìŠ¤: %s", cache_key)
        entry = _load_model_entry(path)
        _MODEL_CACHE[cache_key] = entry
        return entry


SUMMARY_META_COLUMNS = {
    'ITEM_CD', 'CANDIDATE_ID', 'ROUTING_SIGNATURE', 'PRIORITY',
    'SIMILARITY_TIER', 'SIMILARITY_SCORE', 'REFERENCE_ITEM_CD'
}


def build_routing_signature(routing_df: pd.DataFrame) -> str:
    """ê³µì • ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë¼ìš°íŒ… ì‹œê·¸ë‹ˆì²˜ ë¬¸ìì—´ ìƒì„±."""
    if routing_df is None or routing_df.empty:
        return ""

    job_names = routing_df.get('JOB_NM')
    tokens: List[str] = []
    if job_names is not None:
        tokens = [str(val).strip() for val in job_names if isinstance(val, str) and val.strip()]

    if not tokens:
        res_names = routing_df.get('RES_DIS')
        if res_names is not None:
            tokens = [str(val).strip() for val in res_names if isinstance(val, str) and val.strip()]

    if not tokens:
        return "ROUTING"

    return '+'.join(tokens[:4])


def normalize_routing_frame(
    target_item: str,
    candidate_id: str,
    base_df: pd.DataFrame,
    *,
    similarity: float,
    reference_item: str,
    priority: str,
    signature: str,
) -> pd.DataFrame:
    """ë¼ìš°íŒ… DataFrameì„ API/SQL ì¶œë ¥ ê·œê²©ì— ë§ê²Œ ì •ê·œí™”."""
    if base_df is None or base_df.empty:
        return pd.DataFrame()

    frame = base_df.copy()
    alias_map = _active_alias_map()
    frame = frame.rename(columns=alias_map)

    alias_map = _active_alias_map()
    frame = frame.rename(columns=alias_map)
    frame = frame.rename(columns=ROUTING_ALIAS_MAP)


    frame['ITEM_CD'] = target_item
    frame['CANDIDATE_ID'] = candidate_id
    frame['ROUTING_SIGNATURE'] = signature
    frame['PRIORITY'] = priority
    frame['SIMILARITY_TIER'] = 'HIGH' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'LOW'
    frame['SIMILARITY_SCORE'] = float(similarity)
    frame['REFERENCE_ITEM_CD'] = reference_item

    if 'MACH_WORKED_HOURS' not in frame.columns:
        if 'ACT_RUN_TIME' in frame.columns:
            frame['MACH_WORKED_HOURS'] = pd.to_numeric(frame['ACT_RUN_TIME'], errors='coerce').fillna(0.0)
        elif 'RUN_TIME' in frame.columns:
            frame['MACH_WORKED_HOURS'] = pd.to_numeric(frame['RUN_TIME'], errors='coerce').fillna(0.0)
        else:
            frame['MACH_WORKED_HOURS'] = 0.0

    numeric_columns = [
        'MFG_LT', 'QUEUE_TIME', 'SETUP_TIME', 'MACH_WORKED_HOURS',
        'ACT_SETUP_TIME', 'ACT_RUN_TIME', 'WAIT_TIME', 'MOVE_TIME',
        'RUN_TIME_QTY', 'SUBCONTRACT_PRC'
    ]
    for col in numeric_columns:
        if col in frame.columns:
            frame[col] = pd.to_numeric(frame[col], errors='coerce').fillna(0.0)
        else:
            frame[col] = 0.0

    for col in SUMMARY_META_COLUMNS:
        if col not in frame.columns:
            if col == 'SIMILARITY_SCORE':
                frame[col] = float(similarity)
            elif col == 'REFERENCE_ITEM_CD':
                frame[col] = reference_item
            elif col == 'PRIORITY':
                frame[col] = priority
            elif col == 'SIMILARITY_TIER':
                frame[col] = 'HIGH' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'LOW'
            elif col == 'ROUTING_SIGNATURE':
                frame[col] = signature
            else:
                frame[col] = None

    output_columns = get_routing_output_columns()
    frame = frame.reindex(columns=output_columns, fill_value=None)

    output_columns = get_routing_output_columns()
    frame = frame.reindex(columns=output_columns, fill_value=None)
    frame = frame.reindex(columns=ROUTING_OUTPUT_COLS, fill_value=None)

    return frame

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ ì•ˆì „í•œ íƒ€ì… ë³€í™˜ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def safe_int_conversion(value, default=0):
    """ì•ˆì „í•œ ì •ìˆ˜ ë³€í™˜"""
    if value is None or value == '':
        return default
    try:
        return int(value)
    except (ValueError, TypeError):
        logger.warning(f"ì •ìˆ˜ ë³€í™˜ ì‹¤íŒ¨: {value}, ê¸°ë³¸ê°’ {default} ì‚¬ìš©")
        return default

def safe_float_conversion(value, default=0.0):
    """ì•ˆì „í•œ ì‹¤ìˆ˜ ë³€í™˜"""
    if value is None or value == '':
        return default
    try:
        return float(value)
    except (ValueError, TypeError):
        logger.warning(f"ì‹¤ìˆ˜ ë³€í™˜ ì‹¤íŒ¨: {value}, ê¸°ë³¸ê°’ {default} ì‚¬ìš©")
        return default

def _safe_numeric(col: pd.Series) -> pd.Series:
    """ì•ˆì „í•œ ìˆ˜ì¹˜í˜• ë³€í™˜"""
    col = col.replace(["", " ", "-", "--", "nan", "NaN", "null", "NULL", "None"], np.nan)
    num = pd.to_numeric(col, errors="coerce")
    return num.fillna(0).replace([np.inf, -np.inf], 0).infer_objects(copy=False).astype(np.float32)

def _safe_string(col: pd.Series) -> pd.Series:
    """ì•ˆì „í•œ ë¬¸ìì—´ ë³€í™˜"""
    return (
        col.astype(str)
           .str.strip()
           .str.strip("'\"")
           .replace(
               {r"^\s*$": "missing", r"^(nan|NaN|null|NULL|None|-{1,2})$": "missing"},
               regex=True,
           )
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¯ ì‹œê°„ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì • í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TimeScenarioConfig:
    """ì‹œê°„ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •ê°’ë“¤ - GUIì—ì„œ ì¡°ì • ê°€ëŠ¥"""
    
    def __init__(self):
        self.OPTIMAL_SIGMA = 0.67
        self.SAFE_SIGMA = 1.28
        self.CORRELATION_INDEPENDENT = 0.75
        self.CORRELATION_DEPENDENT = 0.25
        self.CV_STABLE = 0.15
        self.CV_MODERATE = 0.30
        self.CV_VARIABLE = 0.50
        self.SAMPLE_BONUS_RATE = 0.05
        self.MAX_SAMPLE_BONUS = 0.2
        self.PROCESS_OVERLAP_THRESHOLD = 0.3
        self.OUTLIER_DETECTION_ENABLED = True
        self.OUTLIER_Z_SCORE_THRESHOLD = 2.5
        self.SIMILARITY_WEIGHT_POWER = 2.0
        self.TRIM_STD_ENABLED = True
        self.TRIM_LOWER_PERCENT = 0.05
        self.TRIM_UPPER_PERCENT = 0.95
    
    def get_scenario_description(self, cv: float) -> str:
        if cv < self.CV_STABLE:
            return "ì•ˆì •ì  (ë‚®ì€ ë³€ë™ì„±)"
        elif cv < self.CV_MODERATE:
            return "ë³´í†µ (ì¤‘ê°„ ë³€ë™ì„±)"
        elif cv < self.CV_VARIABLE:
            return "ê°€ë³€ì  (ë†’ì€ ë³€ë™ì„±)"
        else:
            return "ë¶ˆì•ˆì • (ë§¤ìš° ë†’ì€ ë³€ë™ì„±)"
    
    def get_scenario_emoji(self, cv: float) -> str:
        if cv < self.CV_STABLE:
            return "ğŸŸ¢"
        elif cv < self.CV_MODERATE:
            return "ğŸŸ¡"
        elif cv < self.CV_VARIABLE:
            return "ğŸŸ "
        else:
            return "ğŸ”´"
    
    def to_dict(self) -> Dict:
        return {
            'optimal_sigma': self.OPTIMAL_SIGMA,
            'safe_sigma': self.SAFE_SIGMA,
            'correlation_independent': self.CORRELATION_INDEPENDENT,
            'correlation_dependent': self.CORRELATION_DEPENDENT,
            'cv_stable': self.CV_STABLE,
            'cv_moderate': self.CV_MODERATE,
            'cv_variable': self.CV_VARIABLE,
            'sample_bonus_rate': self.SAMPLE_BONUS_RATE,
            'max_sample_bonus': self.MAX_SAMPLE_BONUS,
            'process_overlap_threshold': self.PROCESS_OVERLAP_THRESHOLD,
            'outlier_detection_enabled': self.OUTLIER_DETECTION_ENABLED,
            'outlier_z_score_threshold': self.OUTLIER_Z_SCORE_THRESHOLD,
            'similarity_weight_power': self.SIMILARITY_WEIGHT_POWER,
            'trim_std_enabled': self.TRIM_STD_ENABLED,
            'trim_lower_percent': self.TRIM_LOWER_PERCENT,
            'trim_upper_percent': self.TRIM_UPPER_PERCENT,
        }

    def from_dict(self, config_dict: Dict):
        self.OPTIMAL_SIGMA = config_dict.get('optimal_sigma', 0.67)
        self.SAFE_SIGMA = config_dict.get('safe_sigma', 1.28)
        self.CORRELATION_INDEPENDENT = config_dict.get('correlation_independent', 0.75)
        self.CORRELATION_DEPENDENT = config_dict.get('correlation_dependent', 0.25)
        self.CV_STABLE = config_dict.get('cv_stable', 0.15)
        self.CV_MODERATE = config_dict.get('cv_moderate', 0.30)
        self.CV_VARIABLE = config_dict.get('cv_variable', 0.50)
        self.SAMPLE_BONUS_RATE = config_dict.get('sample_bonus_rate', 0.05)
        self.MAX_SAMPLE_BONUS = config_dict.get('max_sample_bonus', 0.2)
        self.PROCESS_OVERLAP_THRESHOLD = config_dict.get('process_overlap_threshold', 0.3)
        self.OUTLIER_DETECTION_ENABLED = config_dict.get('outlier_detection_enabled', True)
        self.OUTLIER_Z_SCORE_THRESHOLD = config_dict.get('outlier_z_score_threshold', 2.5)
        self.SIMILARITY_WEIGHT_POWER = config_dict.get('similarity_weight_power', 2.0)
        self.TRIM_STD_ENABLED = config_dict.get('trim_std_enabled', True)
        self.TRIM_LOWER_PERCENT = config_dict.get('trim_lower_percent', 0.05)
        self.TRIM_UPPER_PERCENT = config_dict.get('trim_upper_percent', 0.95)

# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
SCENARIO_CONFIG = TimeScenarioConfig()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ ê°œì„ ëœ ëª¨ë¸ ë¡œë“œ ë° ê´€ë¦¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class EnhancedModelManager:
    """ê°œì„ ëœ ëª¨ë¸ ê´€ë¦¬ì - ë©”íƒ€ë°ì´í„° ë° ì¶”ê°€ ì»´í¬ë„ŒíŠ¸ ì§€ì›"""

    def __init__(self, model_dir: Union[str, Path]):
        self.model_dir = Path(model_dir)
        self.model_components: Dict[str, Any] = {}
        self.metadata: Dict[str, Any] = {}
        self.is_enhanced = False
        self._cache_entry: Optional[ModelCacheEntry] = None

    def __enter__(self):
        """Context manager ì§„ì…"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager ì¢…ë£Œ ì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.cleanup()
        
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.model_components = {}
        self.metadata = {}
        self.is_enhanced = False
        self._cache_entry = None
        logger.debug("ëª¨ë¸ ë§¤ë‹ˆì € ë¦¬ì†ŒìŠ¤ ì°¸ì¡° í•´ì œ")

    def load(self):
        """ëª¨ë¸ ë¡œë“œ - ê°œì„ ëœ ë²„ì „ ìš°ì„ , í˜¸í™˜ì„± ìœ ì§€"""

        entry = get_loaded_model(self.model_dir)
        self._cache_entry = entry
        self.model_components = entry.components
        self.metadata = entry.metadata
        self.is_enhanced = entry.is_enhanced

    def get_feature_weights_array(self) -> np.ndarray:
        feature_cols = self.model_components['feature_columns']
        weights_dict = self.model_components.get('feature_weights')
        
        if weights_dict and isinstance(weights_dict, dict):
            weights = []
            for col in feature_cols:
                weights.append(weights_dict.get(col, 1.0))
            weights = np.array(weights)
        else:
            weights = np.ones(len(feature_cols))
        
        return weights.astype(np.float32)
    
    def transform_features(self, encoded_features: np.ndarray) -> np.ndarray:
        result = encoded_features
        
        if self.model_components.get('variance_selector') is not None:
            try:
                result = self.model_components['variance_selector'].transform(result)
                logger.debug(f"Variance selector ì ìš©: {encoded_features.shape} â†’ {result.shape}")
            except Exception as e:
                logger.warning(f"Variance selector ì ìš© ì‹¤íŒ¨: {e}")
        
        if self.model_components.get('pca') is not None:
            try:
                result = self.model_components['pca'].transform(result)
                logger.debug(f"PCA ì ìš©: {result.shape[0]} â†’ {self.model_components['pca'].n_components_} ì°¨ì›")
            except Exception as e:
                logger.warning(f"PCA ì ìš© ì‹¤íŒ¨: {e}")
        
        if self.model_components.get('active_features') is not None:
            try:
                active_mask = self.model_components['active_features']
                if len(active_mask) == result.shape[1]:
                    result = result[:, active_mask]
                    logger.debug(f"Active features ì ìš©: {np.sum(active_mask)}ê°œ ì°¨ì› ìœ ì§€")
            except Exception as e:
                logger.warning(f"Active features ì ìš© ì‹¤íŒ¨: {e}")
        
        return result
    
    def get_model_info(self) -> Dict[str, Any]:
        info = {
            'is_enhanced': self.is_enhanced,
            'has_pca': self.model_components.get('pca') is not None,
            'has_variance_selector': self.model_components.get('variance_selector') is not None,
            'has_feature_weights': self.model_components.get('feature_weights') is not None,
            'has_active_features': self.model_components.get('active_features') is not None,
        }
        
        if self.is_enhanced and self.metadata.get('model_metadata'):
            info.update({
                'creation_time': self.metadata['model_metadata'].get('creation_time'),
                'vector_dimension': self.metadata['model_metadata'].get('searcher_info', {}).get('vector_dimension'),
                'total_items': self.metadata['model_metadata'].get('item_info', {}).get('total_items'),
            })
        
        return info

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ ê°œì„ ëœ ì¸ì½”ë”© í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _clean_and_encode_enhanced(
    df: pd.DataFrame,
    feature_cols: List[str],
    model_manager: EnhancedModelManager
) -> Tuple[np.ndarray, float]:
    """
    ì•„ì´í…œ ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³  ì¸ì½”ë”© (ê°œì„  ë²„ì „)
    OrdinalEncoderì™€ StandardScalerì˜ ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ ì¶©ì¡±
    """
    cache_key = _build_encoding_cache_key(df)
    cached = _get_cached_encoding(cache_key)
    if cached is not None:
        cached_vector, cached_miss_ratio = cached
        logger.debug("ì¸ì½”ë”© ìºì‹œ íˆíŠ¸ - ì¬ì‚¬ìš©")
        return cached_vector.copy(), float(cached_miss_ratio)

    df = df.copy()

    # ì œì™¸í•  ì»¬ëŸ¼ ì‹¤ì œ ì œê±°
    for col in COLUMNS_TO_EXCLUDE:
        if col in df.columns:
            df = df.drop(columns=[col])
            logger.debug(f"ì…ë ¥ ë°ì´í„°ì—ì„œ ì œì™¸ëœ ì»¬ëŸ¼: {col}")
    
    # ê²°ì¸¡ë¥  ê³„ì‚°ì€ ì…ë ¥ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ
    available_cols = [col for col in feature_cols if col in df.columns]
    miss_ratio = df[available_cols].isna().mean(axis=1).iloc[0] if available_cols else 0.0
    
    # 1) OrdinalEncoderê°€ í•„ìš”ë¡œ í•˜ëŠ” ë²”ì£¼í˜• ì»¬ëŸ¼ ì²˜ë¦¬
    encoder = model_manager.model_components['encoder']
    encoder_cols = encoder.feature_names_in_  # encoderê°€ í•™ìŠµ ì‹œ ë³¸ ì»¬ëŸ¼ë“¤
    
    # encoderê°€ í•„ìš”ë¡œ í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³ , ì—†ëŠ” ì»¬ëŸ¼ì€ 'missing'ìœ¼ë¡œ ì±„ì›€
    df_for_encoder = df.reindex(columns=encoder_cols, fill_value='missing')
    df_cat = df_for_encoder.apply(_safe_string)
    
    # ì¸ì½”ë”© ìˆ˜í–‰
    enc_cat = encoder.transform(df_cat)
    enc_cat_df = pd.DataFrame(enc_cat, columns=encoder_cols, index=df.index)
    
    # 2) ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì²˜ë¦¬
    num_cols = [c for c in feature_cols if c in NUMERIC_FEATURES and c not in COLUMNS_TO_EXCLUDE]
    if num_cols:
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì¤‘ ì‹¤ì œë¡œ ìˆëŠ” ê²ƒë§Œ ì²˜ë¦¬
        available_num_cols = [c for c in num_cols if c in df.columns]
        if available_num_cols:
            df_num = df[available_num_cols].apply(_safe_numeric)
        else:
            df_num = pd.DataFrame(index=df.index)
        
        # ì—†ëŠ” ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì€ 0ìœ¼ë¡œ ì±„ì›€
        for col in num_cols:
            if col not in df_num.columns:
                df_num[col] = 0.0
    else:
        df_num = pd.DataFrame(index=df.index)
    
    # 3) ë²”ì£¼í˜•ê³¼ ìˆ˜ì¹˜í˜• ê²°í•©
    fin = pd.concat([enc_cat_df, df_num], axis=1)
    
    # 4) StandardScalerê°€ í•„ìš”ë¡œ í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ìˆœì„œ ë§ì¶¤
    scaler = model_manager.model_components['scaler']
    scaler_cols = scaler.feature_names_in_  # scalerê°€ í•™ìŠµ ì‹œ ë³¸ ì»¬ëŸ¼ë“¤
    
    # scalerê°€ í•„ìš”ë¡œ í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ (reindexë¡œ ìˆœì„œë„ ë§ì¶¤)
    fin = fin.reindex(columns=scaler_cols, fill_value=0.0).astype(np.float32)
    
    # 5) ê°€ì¤‘ì¹˜ ì ìš©
    weights_obj = model_manager.model_components.get('feature_weights')
    
    # FeatureWeightManager ê°ì²´ì¸ì§€ í™•ì¸
    if hasattr(weights_obj, 'get_weights_as_array'):
        # FeatureWeightManagerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° - í™œì„±í™” ìƒíƒœ ë°˜ì˜
        weights = weights_obj.get_weights_as_array(scaler_cols, apply_active_mask=True)
        logger.debug("FeatureWeightManager ì‚¬ìš© - í™œì„±í™”ëœ í”¼ì²˜ë§Œ ê°€ì¤‘ì¹˜ ì ìš©")
    elif isinstance(weights_obj, dict):
        # ê¸°ì¡´ ë”•ì…”ë„ˆë¦¬ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
        weights = []
        for col in scaler_cols:
            weights.append(weights_obj.get(col, 1.0))
        weights = np.array(weights)
    else:
        # ê°€ì¤‘ì¹˜ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
        weights = np.ones(len(scaler_cols))
        logger.debug("ê°€ì¤‘ì¹˜ ì •ë³´ ì—†ìŒ - ê¸°ë³¸ê°’ 1.0 ì‚¬ìš©")
    
    # ê°€ì¤‘ì¹˜ ì ìš©
    if len(weights) == fin.shape[1]:
        fin = fin * weights
        # 0ì¸ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ í”¼ì²˜ ìˆ˜ ë¡œê¹…
        zero_weight_count = np.sum(weights == 0)
        if zero_weight_count > 0:
            logger.info(f"ë¹„í™œì„±í™”ëœ í”¼ì²˜ {zero_weight_count}ê°œê°€ ì˜ˆì¸¡ì—ì„œ ì œì™¸ë¨")
    else:
        logger.warning(f"ê°€ì¤‘ì¹˜ ì°¨ì› ë¶ˆì¼ì¹˜: weights={len(weights)}, features={fin.shape[1]}")
    
    # 6) ìŠ¤ì¼€ì¼ë§
    scaled = scaler.transform(fin)
    
    # 7) ì¶”ê°€ ë³€í™˜ (PCA ë“±)
    result = model_manager.transform_features(scaled)

    _store_cached_encoding(cache_key, (result.copy(), float(miss_ratio)))

    return result, miss_ratio

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ ê°œì„ ëœ ë©”ì¸ ì˜ˆì¸¡ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def predict_single_item_with_ml_enhanced(
    item_cd: str,
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
    mode: str = "summary",
    routing_selection: str = "latest"
) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:
    """Enhanced ML ë‹¨ì¼ í’ˆëª© ì˜ˆì¸¡ - ê¸°ì¡´ ë¼ìš°íŒ… ìš°ì„ """
    if config is None:
        config = SCENARIO_CONFIG
    
    logger.info(f"ğŸš€ Enhanced ML ì˜ˆì¸¡ ì‹œì‘ - í’ˆëª©: {item_cd}, ëª¨ë“œ: {mode}, ë¼ìš°íŒ…: {routing_selection}")
    
    # 1. ë¨¼ì € ê¸°ì¡´ ë¼ìš°íŒ… í™•ì¸
    existing_routing = fetch_routing_for_item(item_cd, latest_only=True, selection_mode=routing_selection)
    if not existing_routing.empty:
        # Aì•ˆ: ëª¨ë“œì— ë”°ë¼ ë°˜í™˜ í˜•íƒœë¥¼ ë‹¬ë¦¬í•œë‹¤
        if mode == "summary":
            # â‘  ì§‘ê³„ â†’ 1 í–‰ìœ¼ë¡œ ë³€í™˜
            summary_row = {
                "ITEM_CD": item_cd,
                "SETUP_TIME": existing_routing["SETUP_TIME"].sum(),
                "RUN_TIME": existing_routing["RUN_TIME"].sum(),
                "WAIT_TIME": existing_routing["WAIT_TIME"].sum() if "WAIT_TIME" in existing_routing else 0,
                "MOVE_TIME": existing_routing["MOVE_TIME"].sum() if "MOVE_TIME" in existing_routing else 0,
                "OPTIMAL_TIME": None,
                "STANDARD_TIME": None,
                "SAFE_TIME": None,
                "PREDICTION_TYPE": "EXISTING",
                "CONFIDENCE": 1.0,
                "MESSAGE": "ê¸°ì¡´ ë¼ìš°íŒ…(ìš”ì•½)"
            }
            summary_df = pd.DataFrame([summary_row])
            empty_cand_df = pd.DataFrame()
            model_info = {"is_enhanced": False, "source": "existing_routing"}
            return summary_df, empty_cand_df, model_info

        else:  # detailed ëª¨ë“œ
            existing_routing = existing_routing.copy()
            existing_routing["PREDICTION_TYPE"] = "EXISTING"
            existing_routing["INPUT_ITEM_CD"] = item_cd
            empty_cand_df = pd.DataFrame()
            model_info = {"is_enhanced": False, "source": "existing_routing"}
            return existing_routing, empty_cand_df, model_info
    
    # 2. ê¸°ì¡´ ë¼ìš°íŒ…ì´ ì—†ëŠ” ê²½ìš°ì—ë§Œ ML ì˜ˆì¸¡ ì§„í–‰
    logger.info(f"ğŸ” {item_cd}ì˜ ê¸°ì¡´ ë¼ìš°íŒ… ì—†ìŒ - ML ì˜ˆì¸¡ ì§„í–‰")
    
    is_valid, msg, item_info = validate_input_item(item_cd)
    if not is_valid:
        error_df = pd.DataFrame({
            "ITEM_CD": [item_cd],
            "MESSAGE": [msg]
        })
        return error_df, pd.DataFrame(), {}
    
    # Context managerë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ê´€ë¦¬
    with EnhancedModelManager(model_dir) as model_manager:
        model_manager.load()
        model_info = model_manager.get_model_info()
        
        logger.info(f"ğŸ“Š ëª¨ë¸ ì •ë³´: Enhanced={model_info['is_enhanced']}, "
                   f"PCA={model_info['has_pca']}, "
                   f"Feature Weights={model_info['has_feature_weights']}")
        
        # ëª¨ë¸ì—ì„œ feature_cols ê°€ì ¸ì˜¤ê¸°
        model_feature_cols = model_manager.model_components['feature_columns']
        
        # ì¸ì½”ë”© ìˆ˜í–‰
        encoded_vec, miss_ratio = _clean_and_encode_enhanced(item_info, model_feature_cols, model_manager)
        
        if miss_ratio > miss_thr:
            error_df = pd.DataFrame({
                "ITEM_CD": [item_cd],
                "MESSAGE": [f"ê¸°ì¤€ì •ë³´ ê²°ì¸¡ë¥  {miss_ratio:.0%} (ì„ê³„ê°’: {miss_thr:.0%})"]
            })
            return error_df, pd.DataFrame(), model_info
        
        vec = encoded_vec.mean(axis=0).reshape(1, -1)
        
        searcher = model_manager.model_components['searcher']
        codes, scores = searcher.find_similar(vec, top_k)
        
        if not isinstance(codes, list):
            codes = [codes]
        if not isinstance(scores, list):
            scores = [scores]
        
        logger.info(f"ğŸ” ìœ ì‚¬í’ˆ ê²€ìƒ‰ ì™„ë£Œ: {len(codes)}ê°œ (ìœ ì‚¬ë„: {min(scores):.3f}~{max(scores):.3f})")
        
        # ML ì˜ˆì¸¡ ìˆ˜í–‰
        routing_df = predict_routing_from_similar_items(
            item_cd, codes, scores, config, mode
        )
        
        # ML ì˜ˆì¸¡ ê²°ê³¼ì— PREDICTION_TYPE ì¶”ê°€
        if not routing_df.empty:
            routing_df['PREDICTION_TYPE'] = 'ML_BASED'
            routing_df['INPUT_ITEM_CD'] = item_cd
            
            if model_info.get('vector_dimension'):
                routing_df['MODEL_INFO'] = f"Dim={model_info['vector_dimension']}, Enhanced={model_info['is_enhanced']}"
        
        cand_df = pd.DataFrame({
            "ITEM_CD": item_cd,
            "CANDIDATE_RANK": list(range(1, len(codes)+1)),
            "CANDIDATE_ITEM_CD": codes,
            "SIMILARITY_SCORE": scores,
        })
        
        return routing_df, cand_df, model_info

def validate_input_item(item_cd: str) -> Tuple[bool, str, Optional[pd.DataFrame]]:
    """ì…ë ¥ í’ˆëª© ê²€ì¦ - ê¸°ì¡´ ë¼ìš°íŒ… ê²½ê³  ì œê±°"""
    item_info = fetch_single_item(item_cd)
    if item_info.empty:
        return False, f"í’ˆëª© ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤: {item_cd}", None
    
    # ê¸°ì¡´ ë¼ìš°íŒ… ì¡´ì¬ ì—¬ë¶€ëŠ” ë‹¨ìˆœ ë¡œê¹…ë§Œ
    has_routing = check_item_has_routing(item_cd)
    if has_routing:
        logger.debug(f"í’ˆëª© {item_cd}ì— ê¸°ì¡´ ë¼ìš°íŒ… ì¡´ì¬")
    
    return True, "í’ˆëª© ì •ë³´ í™•ì¸ ì™„ë£Œ", item_info

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def apply_similarity_weights(values: List[float], similarities: List[float], 
                           power: float = None) -> Tuple[List[float], List[float]]:
    if power is None:
        power = SCENARIO_CONFIG.SIMILARITY_WEIGHT_POWER
    
    weights = np.power(similarities, power)
    total_weight = np.sum(weights)
    if total_weight > 0:
        weights = weights / total_weight
    else:
        weights = np.ones_like(weights) / len(weights)
    
    return values, weights.tolist()

def filter_by_similarity_threshold(items: List[str], similarities: List[float], 
                                 threshold: float = MIN_SIMILARITY_THRESHOLD) -> Tuple[List[str], List[float]]:
    filtered_items = []
    filtered_sims = []
    
    for item, sim in zip(items, similarities):
        if sim >= threshold:
            filtered_items.append(item)
            filtered_sims.append(sim)
    
    if not filtered_items and items:
        filtered_items = [items[0]]
        filtered_sims = [similarities[0]]
        logger.warning(f"ëª¨ë“  í’ˆëª©ì´ ì„ê³„ê°’({threshold:.2f}) ë¯¸ë§Œ - ìµœê³  ìœ ì‚¬ë„ í’ˆëª©ë§Œ ìœ ì§€: {similarities[0]:.3f}")
    
    logger.info(f"ìœ ì‚¬ë„ í•„í„°ë§: {len(items)}ê°œ â†’ {len(filtered_items)}ê°œ (ì„ê³„ê°’: {threshold:.2f})")
    return filtered_items, filtered_sims

def remove_outliers_zscore(values: List[float], weights: List[float],
                          z_threshold: float = 2.5) -> Tuple[List[float], List[float], List[bool]]:
    if len(values) < 3:
        normalized = _normalize_weights_array(weights) if weights else np.ones(len(values)) / max(len(values), 1)
        return values, normalized.tolist(), [True] * len(values)

    values_array = np.array(values)
    weights_array = _normalize_weights_array(weights if weights else np.ones(len(values_array)))

    mean = np.average(values_array, weights=weights_array)
    variance = np.average((values_array - mean)**2, weights=weights_array)
    std = np.sqrt(variance)

    if std == 0:
        return values, weights_array.tolist(), [True] * len(values)

    z_scores = np.abs((values_array - mean) / std)
    valid_mask = z_scores <= z_threshold

    filtered_values = values_array[valid_mask].tolist()
    filtered_weights = weights_array[valid_mask].tolist()

    if filtered_weights:
        total_weight = sum(filtered_weights)
        filtered_weights = [w/total_weight for w in filtered_weights]

    removed_count = len(values) - len(filtered_values)
    if removed_count > 0:
        logger.debug(f"ì´ìƒì¹˜ ì œê±°: {removed_count}ê°œ ì œê±°ë¨ (Z-score > {z_threshold})")

    return filtered_values, filtered_weights, valid_mask.tolist()


def _normalize_weights_array(weights: Union[List[float], np.ndarray]) -> np.ndarray:
    arr = np.asarray(weights, dtype=np.float64)
    if arr.size == 0:
        return arr
    total = arr.sum()
    if total <= 0:
        return np.ones_like(arr) / len(arr)
    return arr / total


def _apply_weighted_trimmed_range(
    values: np.ndarray,
    weights: np.ndarray,
    lower_pct: float,
    upper_pct: float
) -> Tuple[np.ndarray, np.ndarray]:
    if values.size < 3:
        return values, weights

    lower_pct = float(np.clip(lower_pct, 0.0, 0.5))
    upper_pct = float(np.clip(upper_pct, 0.5, 1.0))
    if upper_pct <= lower_pct:
        return values, weights

    order = np.argsort(values)
    sorted_values = values[order]
    sorted_weights = weights[order]

    cumulative = np.cumsum(sorted_weights)
    lower_bounds = cumulative - sorted_weights

    keep_mask = (cumulative > lower_pct) & (lower_bounds < upper_pct)
    trimmed_values = sorted_values[keep_mask]
    trimmed_weights = sorted_weights[keep_mask]

    if trimmed_values.size == 0:
        return values, weights

    trimmed_weights = _normalize_weights_array(trimmed_weights)
    return trimmed_values, trimmed_weights

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š ì œì¡° ì‹œê°„ í†µê³„ ê³„ì‚°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def calculate_manufacturing_time_stats(
    times_list: List[float],
    weights_list: List[float],
    config: TimeScenarioConfig = None,
    *,
    apply_trimmed: bool = False,
    trim_lower: Optional[float] = None,
    trim_upper: Optional[float] = None
) -> Dict[str, float]:
    """ì œì¡° ì‹œê°„ í†µê³„ ê³„ì‚°"""
    if config is None:
        config = SCENARIO_CONFIG

    if not times_list:
        return {
            'mean': 0.0,
            'std': 0.0,
            'cv': 0.0,
            'optimal': 0.0,
            'standard': 0.0,
            'safe': 0.0,
            'samples': 0,
            'raw_samples': 0,
        }

    times = np.array(times_list, dtype=np.float64)
    weights = _normalize_weights_array(weights_list if weights_list else np.ones(len(times)))

    trimmed_times = times
    trimmed_weights = weights

    if (
        apply_trimmed
        and len(times) >= 3
        and getattr(config, 'TRIM_STD_ENABLED', True)
    ):
        lower = trim_lower if trim_lower is not None else getattr(config, 'TRIM_LOWER_PERCENT', 0.05)
        upper = trim_upper if trim_upper is not None else getattr(config, 'TRIM_UPPER_PERCENT', 0.95)
        trimmed_times, trimmed_weights = _apply_weighted_trimmed_range(times, weights, lower, upper)
        removed = len(times) - len(trimmed_times)
        if removed > 0:
            logger.debug(
                "ê°€ì¤‘ì¹˜ íŠ¸ë¦¼ ì ìš©: ì´ %dê°œ ì¤‘ %dê°œ ì œê±° (í•˜ìœ„ %.0f%%, ìƒìœ„ %.0f%%)",
                len(times),
                removed,
                lower * 100,
                (1 - upper) * 100,
            )

    mean = np.average(trimmed_times, weights=trimmed_weights)
    variance = np.average((trimmed_times - mean)**2, weights=trimmed_weights)
    std = np.sqrt(variance)
    cv = std / mean if mean > 0 else 0

    optimal_time = mean + config.OPTIMAL_SIGMA * std
    standard_time = mean
    safe_time = mean + config.SAFE_SIGMA * std

    return {
        'mean': mean,
        'std': std,
        'cv': cv,
        'optimal': optimal_time,
        'standard': standard_time,
        'safe': safe_time,
        'samples': len(trimmed_times),
        'raw_samples': len(times)
    }

def calculate_confidence_score(
    sample_count: int,
    avg_similarity: float,
    cv: float,
    config: TimeScenarioConfig
) -> float:
    """ì˜ˆì¸¡ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
    sample_score = min(0.3, sample_count * 0.05)
    similarity_score = avg_similarity * 0.4
    if cv < config.CV_STABLE:
        cv_score = 0.3
    elif cv < config.CV_MODERATE:
        cv_score = 0.2
    elif cv < config.CV_VARIABLE:
        cv_score = 0.1
    else:
        cv_score = 0.0
    
    confidence = sample_score + similarity_score + cv_score
    if sample_count >= 5 and avg_similarity >= 0.8:
        confidence = min(1.0, confidence * 1.1)
    
    return min(1.0, confidence)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸš€ ë¼ìš°íŒ… ì˜ˆì¸¡ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def predict_routing_from_similar_items(
    input_item_cd: str,
    similar_items: List[str],
    similarity_scores: List[float],
    config: TimeScenarioConfig = None,
    mode: str = "summary",
    routing_selection: str = "latest"
) -> pd.DataFrame:
    """
    ìœ ì‚¬ í’ˆëª©ë“¤ì˜ ë¼ìš°íŒ… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ í’ˆëª©ì˜ ë¼ìš°íŒ… ì˜ˆì¸¡
    """
    if config is None:
        config = SCENARIO_CONFIG
    
    logger.info(f"ë¼ìš°íŒ… ì˜ˆì¸¡ ì‹œì‘ - í’ˆëª©: {input_item_cd}, ìœ ì‚¬í’ˆ: {len(similar_items)}ê°œ, ëª¨ë“œ: {mode}, ë¼ìš°íŒ…: {routing_selection}")
    
    # ë””ë²„ê¹…: ìœ ì‚¬í’ˆê³¼ ìœ ì‚¬ë„ ì¶œë ¥
    for item, score in zip(similar_items[:5], similarity_scores[:5]):
        logger.debug(f"  ìœ ì‚¬í’ˆ: {item}, ìœ ì‚¬ë„: {score:.3f}")
    
    # 1. ìœ ì‚¬ë„ í•„í„°ë§ - ë” ìœ ì—°í•˜ê²Œ ì²˜ë¦¬
    filtered_items, filtered_scores = filter_by_similarity_threshold(
        similar_items, similarity_scores, MIN_SIMILARITY_THRESHOLD
    )
    
    # 2. ìœ ì‚¬í’ˆì˜ ë¼ìš°íŒ… ë°ì´í„° ì¡°íšŒ - ë¼ìš°íŒ…ì´ ìˆëŠ” í’ˆëª©ì„ ì°¾ì„ ë•Œê¹Œì§€ í™•ì¥ íƒìƒ‰
    all_routings = []
    items_with_routing = []
    items_without_routing = []
    checked_items = set()  # ì¤‘ë³µ ì²´í¬ ë°©ì§€
    
    # í•„í„°ë§ëœ í’ˆëª©ë“¤ë¶€í„° í™•ì¸
    for i, item_cd in enumerate(filtered_items):
        if item_cd in checked_items:
            continue
        checked_items.add(item_cd)
        
        routing = fetch_routing_for_item(item_cd, latest_only=True, selection_mode=routing_selection)
        if not routing.empty:
            all_routings.append((item_cd, routing))
            items_with_routing.append(item_cd)
            
            # ROUT_NO ì •ë³´ ë¡œê¹…
            rout_no = routing['ROUT_NO'].iloc[0] if 'ROUT_NO' in routing.columns else 'UNKNOWN'
            logger.info(f"  ë¼ìš°íŒ… ë°œê²¬: {item_cd} (ìœ ì‚¬ë„: {filtered_scores[i]:.3f}, ROUT_NO: {rout_no})")
            
            # â­ ì¤‘ìš”: ë‹¨ì¼ ROUT_NO ê·¸ë£¹ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì²« ë²ˆì§¸ ë¼ìš°íŒ…ì„ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
            logger.info(f"ë‹¨ì¼ ROUT_NO ê·¸ë£¹ ì‚¬ìš©: {item_cd}ì˜ {rout_no} (ê³µì • ìˆ˜: {len(routing)})")
            break  # ì²« ë²ˆì§¸ ë¼ìš°íŒ…ë§Œ ì‚¬ìš©í•˜ê³  ì¤‘ë‹¨
        else:
            items_without_routing.append(item_cd)
    
    # 3. ë¼ìš°íŒ…ì´ ì—†ìœ¼ë©´ ì „ì²´ ìœ ì‚¬í’ˆ ëª©ë¡ì—ì„œ ì¶”ê°€ íƒìƒ‰
    if not all_routings and len(similar_items) > len(filtered_items):
        logger.info("í•„í„°ë§ëœ í’ˆëª©ì—ì„œ ë¼ìš°íŒ…ì„ ì°¾ì§€ ëª»í•¨. ì „ì²´ ìœ ì‚¬í’ˆ ëª©ë¡ì—ì„œ ì¶”ê°€ íƒìƒ‰...")
        
        # ì›ë˜ ìœ ì‚¬í’ˆ ëª©ë¡ì—ì„œ ì•„ì§ í™•ì¸í•˜ì§€ ì•Šì€ í’ˆëª©ë“¤ í™•ì¸
        for i, item_cd in enumerate(similar_items):
            if item_cd in checked_items:
                continue
            checked_items.add(item_cd)
            
            routing = fetch_routing_for_item(item_cd, latest_only=True)
            if not routing.empty:
                all_routings.append((item_cd, routing))
                items_with_routing.append(item_cd)
                # ì›ë˜ ìœ ì‚¬ë„ ì‚¬ìš©í•˜ë˜, filtered_scoresì—ë„ ì¶”ê°€
                if item_cd not in filtered_items:
                    filtered_items.append(item_cd)
                    filtered_scores.append(similarity_scores[i])
                
                # ROUT_NO ì •ë³´ ë¡œê¹…
                rout_no = routing['ROUT_NO'].iloc[0] if 'ROUT_NO' in routing.columns else 'UNKNOWN'
                logger.info(f"  ë¼ìš°íŒ… ë°œê²¬: {item_cd} (ìœ ì‚¬ë„: {similarity_scores[i]:.3f}, ROUT_NO: {rout_no})")
                
                # â­ ì¤‘ìš”: ë‹¨ì¼ ROUT_NO ê·¸ë£¹ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì²« ë²ˆì§¸ ë¼ìš°íŒ…ì„ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
                logger.info(f"ë‹¨ì¼ ROUT_NO ê·¸ë£¹ ì‚¬ìš©: {item_cd}ì˜ {rout_no} (ê³µì • ìˆ˜: {len(routing)})")
                break  # ì²« ë²ˆì§¸ ë¼ìš°íŒ…ë§Œ ì‚¬ìš©í•˜ê³  ì¤‘ë‹¨
            else:
                items_without_routing.append(item_cd)
    
    logger.info(f"ë¼ìš°íŒ… ìˆëŠ” í’ˆëª©: {len(items_with_routing)}ê°œ - {items_with_routing[:5]}")
    logger.info(f"ë¼ìš°íŒ… ì—†ëŠ” í’ˆëª©: {len(items_without_routing)}ê°œ - {items_without_routing[:5]}")
    logger.info(f"ì „ì²´ í™•ì¸í•œ í’ˆëª©: {len(checked_items)}ê°œ")
    
    # ë‹¨ì¼ ROUT_NO ì‚¬ìš© ì •ë³´ ì¶”ê°€ ë¡œê¹…
    if all_routings:
        used_item = all_routings[0][0]
        used_routing = all_routings[0][1]
        used_rout_no = used_routing['ROUT_NO'].iloc[0] if 'ROUT_NO' in used_routing.columns else 'UNKNOWN'
        logger.info(f"[ì˜ˆì¸¡] ë‹¨ì¼ ROUT_NO ê¸°ì¤€ ì˜ˆì¸¡: {used_item}ì˜ {used_rout_no} ì‚¬ìš©")
    
    # 4. ì—¬ì „íˆ ë¼ìš°íŒ…ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
    if not all_routings:
        logger.warning(f"ëª¨ë“  ìœ ì‚¬í’ˆ ({len(checked_items)}ê°œ) ì¤‘ ë¼ìš°íŒ… ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {input_item_cd}")
        
        # ìœ ì‚¬ë„ê°€ ë†’ì€ ìƒìœ„ 5ê°œ í’ˆëª© ì •ë³´ í¬í•¨
        top_items_info = []
        for i, (item, score) in enumerate(zip(similar_items[:5], similarity_scores[:5])):
            top_items_info.append(f"{i+1}. {item} (ìœ ì‚¬ë„: {score:.3f})")
        
        return pd.DataFrame({
            'ITEM_CD': [input_item_cd],
            'MESSAGE': [f'ìƒìœ„ {len(checked_items)}ê°œ ìœ ì‚¬í’ˆ ì¤‘ ë¼ìš°íŒ… ë°ì´í„°ê°€ ì—†ìŒ'],
            'SIMILAR_ITEMS_CHECKED': [', '.join(similar_items[:10])],
            'SIMILARITY_SCORES': [', '.join([f'{s:.3f}' for s in similarity_scores[:10]])],
            'TOP_SIMILAR_ITEMS': ['\n'.join(top_items_info)]
        })
    
    logger.info(f"ë¼ìš°íŒ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(all_routings)}ê°œ í’ˆëª©")
    
    if mode == "detailed":
        # ìƒì„¸ ëª¨ë“œ: ê³µì •ë³„ ë¼ìš°íŒ… ì˜ˆì¸¡
        process_predictions = defaultdict(list)
        
        for item_cd, routing_df in all_routings:
            try:
                item_index = filtered_items.index(item_cd)
                item_similarity = filtered_scores[item_index]
            except (ValueError, IndexError):
                logger.warning(f"ìœ ì‚¬ë„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {item_cd}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                item_similarity = 0.5  # ê¸°ë³¸ê°’
            
            for _, row in routing_df.iterrows():
                proc_seq = safe_int_conversion(row.get('PROC_SEQ'), 0)
                proc_info = {
                    'ROUT_NO': str(row.get('ROUT_NO', '')),
                    'PROC_SEQ': proc_seq,
                    'JOB_CD': str(row.get('JOB_CD', '')),
                    'JOB_NM': str(row.get('JOB_NM', '')),
                    'RES_CD': str(row.get('RES_CD', '')),
                    'RES_DIS': str(row.get('RES_DIS', '')),
                    'INSIDE_FLAG': str(row.get('INSIDE_FLAG', 'ì‚¬ë‚´')),
                    'TIME_UNIT': str(row.get('TIME_UNIT', 'ë¶„(Min.)')),
                    'SETUP_TIME': safe_float_conversion(row.get('SETUP_TIME'), 0.0),
                    'RUN_TIME': safe_float_conversion(row.get('RUN_TIME'), 0.0),
                    'MACH_WORKED_HOURS': safe_float_conversion(
                        row.get('MACH_WORKED_HOURS')
                        or row.get('ACT_RUN_TIME')
                        or row.get('RUN_TIME'),
                        0.0,
                    ),
                    'ACT_SETUP_TIME': safe_float_conversion(row.get('ACT_SETUP_TIME') or row.get('SETUP_TIME'), 0.0),
                    'ACT_RUN_TIME': safe_float_conversion(row.get('ACT_RUN_TIME') or row.get('RUN_TIME'), 0.0),
                    'WAIT_TIME': safe_float_conversion(row.get('WAIT_TIME'), 0.0),
                    'MOVE_TIME': safe_float_conversion(row.get('MOVE_TIME'), 0.0),
                    'SOURCE_ITEM': item_cd,
                    'SIMILARITY': item_similarity
                }
                process_predictions[proc_seq].append(proc_info)
        
        predicted_routing = []
        
        # PROC_SEQ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        for proc_seq in sorted(process_predictions.keys()):
            proc_list = process_predictions[proc_seq]
            if not proc_list:
                continue
            
            # ê°€ì¥ ë¹ˆë²ˆí•œ ê°’ ì„ íƒ
            job_cd = Counter(p['JOB_CD'] for p in proc_list).most_common(1)[0][0]
            job_nm = Counter(p['JOB_NM'] for p in proc_list).most_common(1)[0][0]
            res_cd = Counter(p['RES_CD'] for p in proc_list).most_common(1)[0][0]
            res_dis = Counter(p['RES_DIS'] for p in proc_list).most_common(1)[0][0]
            inside_flag = Counter(p['INSIDE_FLAG'] for p in proc_list).most_common(1)[0][0]
            time_unit = 'ë¶„(Min.)'  # ê³ ì •ê°’
            
            # ì‹œê°„ ë°ì´í„° ê³„ì‚°
            setup_times = [p['SETUP_TIME'] for p in proc_list]
            run_times = [p['RUN_TIME'] for p in proc_list]
            mach_times = [p['MACH_WORKED_HOURS'] for p in proc_list]
            wait_times = [p['WAIT_TIME'] for p in proc_list]
            move_times = [p['MOVE_TIME'] for p in proc_list]
            similarities = [p['SIMILARITY'] for p in proc_list]

            _, weights = apply_similarity_weights(run_times, similarities, config.SIMILARITY_WEIGHT_POWER)

            if config.OUTLIER_DETECTION_ENABLED and len(run_times) >= 3:
                run_times, weights, valid_mask = remove_outliers_zscore(
                    run_times, weights, config.OUTLIER_Z_SCORE_THRESHOLD
                )
                setup_times = [val for val, keep in zip(setup_times, valid_mask) if keep]
                mach_times = [val for val, keep in zip(mach_times, valid_mask) if keep]
                wait_times = [val for val, keep in zip(wait_times, valid_mask) if keep]
                move_times = [val for val, keep in zip(move_times, valid_mask) if keep]
                similarities = [val for val, keep in zip(similarities, valid_mask) if keep]

            setup_stats = calculate_manufacturing_time_stats(setup_times, weights, config, apply_trimmed=True)
            run_stats = calculate_manufacturing_time_stats(run_times, weights, config)
            mach_stats = calculate_manufacturing_time_stats(mach_times, weights, config, apply_trimmed=True)
            wait_stats = calculate_manufacturing_time_stats(wait_times, weights, config)
            move_stats = calculate_manufacturing_time_stats(move_times, weights, config)

            similarity_mean = np.mean(similarities) if similarities else 0.0

            confidence = calculate_confidence_score(
                len(proc_list), similarity_mean, mach_stats['cv'], config
            )

            prediction = {
                'ROUT_NO': 'PREDICTED',
                'ITEM_CD': input_item_cd,
                'PROC_SEQ': proc_seq,
                'INSIDE_FLAG': inside_flag,
                'JOB_CD': job_cd,
                'JOB_NM': job_nm,
                'RES_CD': res_cd,
                'RES_DIS': res_dis,
                'TIME_UNIT': time_unit,
                'SETUP_TIME': round(setup_stats['mean'], 3),
                'RUN_TIME': round(run_stats['mean'], 3),
                'MACH_WORKED_HOURS': round(mach_stats['mean'], 3),
                'ACT_SETUP_TIME': round(setup_stats['mean'], 3),
                'ACT_RUN_TIME': round(mach_stats['mean'], 3),
                'WAIT_TIME': round(wait_stats['mean'], 3),
                'MOVE_TIME': round(move_stats['mean'], 3),
                'OPTIMAL_TIME': round(mach_stats['optimal'], 3),
                'STANDARD_TIME': round(mach_stats['standard'], 3),
                'SAFE_TIME': round(mach_stats['safe'], 3),
                'TIME_STD': round(mach_stats['std'], 3),
                'TIME_CV': round(mach_stats['cv'], 3),
                'SIMILAR_ITEMS_USED': len(proc_list),
                'AVG_SIMILARITY': round(similarity_mean, 3),
                'CONFIDENCE': round(confidence, 3),
                'SCENARIO': config.get_scenario_description(mach_stats['cv']),
                'SCENARIO_EMOJI': config.get_scenario_emoji(mach_stats['cv']),
                'SOURCE_ITEMS': ','.join([p['SOURCE_ITEM'] for p in proc_list]),
                'SIMILARITY_SCORES': ','.join([f"{p['SIMILARITY']:.3f}" for p in proc_list]),
                'WEIGHTS': ','.join([f"{w:.3f}" for w in weights]),
                'VALID_FROM_DT': '01-Jan-01',
                'VALID_TO_DT': '31-Dec-99',
                'INSRT_DT': datetime.now().strftime('%Y-%m-%d')
            }
            
            predicted_routing.append(prediction)
        
        if predicted_routing:
            routing_df = pd.DataFrame(predicted_routing)
            routing_df = routing_df.sort_values('PROC_SEQ').reset_index(drop=True)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì»¬ëŸ¼ ìˆœì„œì— ë§ê²Œ ì¬ì •ë ¬
            base_cols = [
                'ROUT_NO', 'ITEM_CD', 'PROC_SEQ', 'INSIDE_FLAG', 'JOB_CD', 'JOB_NM',
                'RES_CD', 'RES_DIS', 'TIME_UNIT', 'SETUP_TIME', 'RUN_TIME',
                'MACH_WORKED_HOURS', 'ACT_SETUP_TIME', 'ACT_RUN_TIME', 'WAIT_TIME', 'MOVE_TIME',
                'OPTIMAL_TIME', 'STANDARD_TIME', 'SAFE_TIME', 'VALID_FROM_DT', 'VALID_TO_DT', 'INSRT_DT'
            ]
            extra_cols = [col for col in routing_df.columns if col not in base_cols]
            routing_df = routing_df[base_cols + extra_cols]
            
            logger.info(f"ìƒì„¸ ë¼ìš°íŒ… ì˜ˆì¸¡ ì™„ë£Œ: {len(routing_df)}ê°œ ê³µì •")
            return routing_df
        else:
            logger.warning(f"ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ê³µì •ì´ ì—†ìŠµë‹ˆë‹¤: {input_item_cd}")
            return pd.DataFrame({
                'ITEM_CD': [input_item_cd],
                'MESSAGE': ['ì˜ˆì¸¡ëœ ê³µì • ë°ì´í„° ì—†ìŒ']
            })
    
    else:
        # ìš”ì•½ ëª¨ë“œ: ì´í•© ì‹œê°„ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì¸¡
        total_setup_times = []
        total_run_times = []
        total_wait_times = []
        total_move_times = []
        total_mach_hours = []
        similarities = []
        source_items = []

        for item_cd, routing_df in all_routings:
            try:
                item_index = filtered_items.index(item_cd)
                item_similarity = filtered_scores[item_index]
            except (ValueError, IndexError):
                logger.warning(f"ìœ ì‚¬ë„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {item_cd}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                item_similarity = 0.5  # ê¸°ë³¸ê°’
                
            setup_sum = routing_df['SETUP_TIME'].sum()
            run_sum = routing_df['RUN_TIME'].sum()
            wait_sum = routing_df['WAIT_TIME'].sum()
            move_sum = routing_df['MOVE_TIME'].sum()
            if 'MACH_WORKED_HOURS' in routing_df.columns:
                mach_series = pd.to_numeric(routing_df['MACH_WORKED_HOURS'], errors='coerce').fillna(0.0)
            elif 'ACT_RUN_TIME' in routing_df.columns:
                mach_series = pd.to_numeric(routing_df['ACT_RUN_TIME'], errors='coerce').fillna(0.0)
            else:
                mach_series = pd.to_numeric(routing_df['RUN_TIME'], errors='coerce').fillna(0.0)
            mach_sum = float(mach_series.sum())

            total_setup_times.append(setup_sum)
            total_run_times.append(run_sum)
            total_wait_times.append(wait_sum)
            total_move_times.append(move_sum)
            total_mach_hours.append(mach_sum)
            similarities.append(item_similarity)
            source_items.append(item_cd)

        _, weights = apply_similarity_weights(total_run_times, similarities, config.SIMILARITY_WEIGHT_POWER)

        if config.OUTLIER_DETECTION_ENABLED and len(total_run_times) >= 3:
            total_run_times, weights, valid_mask = remove_outliers_zscore(
                total_run_times, weights, config.OUTLIER_Z_SCORE_THRESHOLD
            )
            total_setup_times = [val for val, keep in zip(total_setup_times, valid_mask) if keep]
            total_wait_times = [val for val, keep in zip(total_wait_times, valid_mask) if keep]
            total_move_times = [val for val, keep in zip(total_move_times, valid_mask) if keep]
            total_mach_hours = [val for val, keep in zip(total_mach_hours, valid_mask) if keep]
            similarities = [val for val, keep in zip(similarities, valid_mask) if keep]
            source_items = [val for val, keep in zip(source_items, valid_mask) if keep]

        setup_stats = calculate_manufacturing_time_stats(total_setup_times, weights, config, apply_trimmed=True)
        run_stats = calculate_manufacturing_time_stats(total_run_times, weights, config)
        mach_stats = calculate_manufacturing_time_stats(total_mach_hours, weights, config, apply_trimmed=True)
        wait_stats = calculate_manufacturing_time_stats(total_wait_times, weights, config)
        move_stats = calculate_manufacturing_time_stats(total_move_times, weights, config)

        similarity_mean = np.mean(similarities) if similarities else 0.0

        confidence = calculate_confidence_score(
            len(all_routings), similarity_mean, mach_stats['cv'], config
        )

        prediction = {
            'ITEM_CD': input_item_cd,
            'SETUP_TIME': round(setup_stats['mean'], 3),
            'RUN_TIME': round(run_stats['mean'], 3),
            'MACH_WORKED_HOURS': round(mach_stats['mean'], 3),
            'ACT_SETUP_TIME': round(setup_stats['mean'], 3),
            'ACT_RUN_TIME': round(mach_stats['mean'], 3),
            'WAIT_TIME': round(wait_stats['mean'], 3),
            'MOVE_TIME': round(move_stats['mean'], 3),
            'OPTIMAL_TIME': round(mach_stats['optimal'], 3),
            'STANDARD_TIME': round(mach_stats['standard'], 3),
            'SAFE_TIME': round(mach_stats['safe'], 3),
            'TIME_STD': round(mach_stats['std'], 3),
            'TIME_CV': round(mach_stats['cv'], 3),
            'SIMILAR_ITEMS_USED': len(all_routings),
            'AVG_SIMILARITY': round(similarity_mean, 3),
            'CONFIDENCE': round(confidence, 3),
            'SCENARIO': config.get_scenario_description(mach_stats['cv']),
            'SCENARIO_EMOJI': config.get_scenario_emoji(mach_stats['cv']),
            'SOURCE_ITEMS': ','.join(source_items),
            'SIMILARITY_SCORES': ','.join([f"{s:.3f}" for s in similarities]),
            'WEIGHTS': ','.join([f"{w:.3f}" for w in weights]),
            'VALID_FROM_DT': '01-Jan-01',
            'VALID_TO_DT': '31-Dec-99',
            'INSRT_DT': datetime.now().strftime('%Y-%m-%d')
        }
        
        routing_df = pd.DataFrame([prediction])
        logger.info("ìš”ì•½ ë¼ìš°íŒ… ì˜ˆì¸¡ ì™„ë£Œ: 1ê°œ ë ˆì½”ë“œ")
        return routing_df

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š í’ˆì§ˆ ê²€ì¦ ê°œì„ 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def validate_prediction_quality_enhanced(
    routing_df: pd.DataFrame, 
    model_info: Dict[str, Any] = None
) -> Dict[str, Any]:
    if routing_df.empty:
        return {'error': 'No predictions to validate'}
    
    metrics = {}
    
    if 'CONFIDENCE' in routing_df.columns:
        metrics['avg_confidence'] = routing_df['CONFIDENCE'].mean()
        metrics['min_confidence'] = routing_df['CONFIDENCE'].min()
        metrics['max_confidence'] = routing_df['CONFIDENCE'].max()
        metrics['high_confidence_ratio'] = (routing_df['CONFIDENCE'] >= 0.7).sum() / len(routing_df)
    
    if 'AVG_SIMILARITY' in routing_df.columns:
        metrics['avg_similarity'] = routing_df['AVG_SIMILARITY'].mean()
    
    if 'SIMILAR_ITEMS_USED' in routing_df.columns:
        metrics['avg_similar_items'] = routing_df['SIMILAR_ITEMS_USED'].mean()
    
    time_cols = ['SETUP_TIME', 'RUN_TIME']
    for col in time_cols:
        if col in routing_df.columns:
            metrics[f'total_{col.lower()}'] = routing_df[col].sum()
    
    if model_info:
        quality_score = 0.5
        if model_info.get('is_enhanced'):
            quality_score += 0.1
        if model_info.get('has_pca'):
            quality_score += 0.05
        if model_info.get('has_feature_weights'):
            quality_score += 0.1
        
        avg_conf = metrics.get('avg_confidence', 0.5)
        quality_score = quality_score * (0.5 + avg_conf * 0.5)
        
        metrics['model_quality_score'] = min(1.0, quality_score)
    
    avg_conf = metrics.get('avg_confidence', 0)
    model_score = metrics.get('model_quality_score', 0.5)
    combined_score = (avg_conf + model_score) / 2
    
    if combined_score >= 0.8:
        metrics['quality_grade'] = 'A (ìš°ìˆ˜)'
    elif combined_score >= 0.7:
        metrics['quality_grade'] = 'B (ì–‘í˜¸)'
    elif combined_score >= 0.6:
        metrics['quality_grade'] = 'C (ë³´í†µ)'
    else:
        metrics['quality_grade'] = 'D (ê°œì„ í•„ìš”)'
    
    suggestions = []
    if avg_conf < 0.7:
        suggestions.append("ë” ë§ì€ ìœ ì‚¬ í’ˆëª© ë°ì´í„° ìˆ˜ì§‘ í•„ìš”")
    if not model_info or not model_info.get('is_enhanced'):
        suggestions.append("ê°œì„ ëœ ëª¨ë¸ë¡œ ì¬í•™ìŠµ ê¶Œì¥")
    if metrics.get('avg_similar_items', 0) < 5:
        suggestions.append("ìœ ì‚¬í’ˆ ê²€ìƒ‰ ë²”ìœ„(top_k) í™•ëŒ€ ê³ ë ¤")
    
    metrics['improvement_suggestions'] = suggestions
    
    return metrics

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ ê¸°ì¡´ API í˜¸í™˜ì„± ìœ ì§€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def predict_single_item_with_ml_optimized(
    item_cd: str,
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
    mode: str = "summary"
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    routing_df, cand_df, _ = predict_single_item_with_ml_enhanced(
        item_cd, model_dir, top_k=top_k, miss_thr=miss_thr, 
        config=config, mode=mode
    )
    
    if not cand_df.empty:
        has_routing = []
        routing_counts = []
        
        for candidate in cand_df['CANDIDATE_ITEM_CD']:
            routing = fetch_routing_for_item(candidate)
            if not routing.empty:
                has_routing.append("âœ“ ìˆìŒ")
                routing_counts.append(len(routing))
            else:
                has_routing.append("âœ— ì—†ìŒ")
                routing_counts.append(0)
        
        cand_df['HAS_ROUTING'] = has_routing
        cand_df['PROCESS_COUNT'] = routing_counts
        
        cols = list(cand_df.columns)
        if 'SIMILARITY_SCORE' in cols and 'HAS_ROUTING' in cols:
            cols.remove('HAS_ROUTING')
            cols.remove('PROCESS_COUNT')
            idx = cols.index('SIMILARITY_SCORE') + 1
            cols.insert(idx, 'HAS_ROUTING')
            cols.insert(idx + 1, 'PROCESS_COUNT')
            cand_df = cand_df[cols]
    
    return routing_df, cand_df

def predict_items_with_ml_optimized(
    item_codes: List[str],
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
    mode: str = "summary"
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if config is None:
        config = SCENARIO_CONFIG

    logger.info(f"ğŸš€ Enhanced ML ë°°ì¹˜ ì˜ˆì¸¡ ì‹œì‘: {len(item_codes)}ê°œ í’ˆëª©")

    predicted_routings: Dict[str, pd.DataFrame] = {}
    raw_candidate_frames: List[pd.DataFrame] = []
    routing_cache: Dict[str, pd.DataFrame] = {}

    for item_cd in item_codes:
        routing_df, cand_df, _ = predict_single_item_with_ml_enhanced(
            item_cd, model_dir, top_k=top_k, miss_thr=miss_thr,
            config=config, mode=mode
        )

        predicted_routings[item_cd] = routing_df if routing_df is not None else pd.DataFrame()

        if not cand_df.empty:
            cand_df['ITEM_CD'] = item_cd
            raw_candidate_frames.append(cand_df)
            for cand_item in cand_df['CANDIDATE_ITEM_CD']:
                if cand_item not in routing_cache:
                    routing_cache[cand_item] = fetch_routing_for_item(cand_item)

    composed_frames: List[pd.DataFrame] = []
    enhanced_candidates: List[Dict[str, Any]] = []
    per_item_counts: Dict[str, int] = defaultdict(int)

    # ML ì˜ˆì¸¡ ì¡°í•©(ì¡´ì¬ ì‹œ) ìš°ì„  ì¶”ê°€
    for item_cd, predicted_df in predicted_routings.items():
        if predicted_df is None or predicted_df.empty:
            continue
        signature = build_routing_signature(predicted_df)
        candidate_id = f"{item_cd}_MLP"
        normalized = normalize_routing_frame(
            item_cd,
            candidate_id,
            predicted_df,
            similarity=1.0,
            reference_item="ML_PREDICTED",
            priority="primary",
            signature=signature,
        )
        if normalized.empty:
            continue
        composed_frames.append(normalized)
        per_item_counts[item_cd] += 1
        enhanced_candidates.append({
            'ITEM_CD': item_cd,
            'CANDIDATE_ITEM_CD': 'ML_PREDICTED',
            'SIMILARITY_SCORE': 1.0,
            'ROUTING_SIGNATURE': signature,
            'ROUTING_SUMMARY': f"ML ì˜ˆì¸¡ ê³µì • {len(normalized)}ê°œ",
            'PRIORITY': 'primary',
            'SIMILARITY_TIER': 'HIGH',
            'HAS_ROUTING': 'âœ“ ìˆìŒ',
            'PROCESS_COUNT': len(normalized),
        })


    raw_candidates_df = (
        pd.concat(raw_candidate_frames, ignore_index=True)
        if raw_candidate_frames
        else pd.DataFrame()
    )

    if not raw_candidates_df.empty:
        raw_candidates_df = raw_candidates_df.sort_values(
            ['ITEM_CD', 'SIMILARITY_SCORE'], ascending=[True, False]
        )

        for _, cand_row in raw_candidates_df.iterrows():
            item_cd = cand_row.get('ITEM_CD')
            candidate_item = cand_row.get('CANDIDATE_ITEM_CD')
            similarity = float(cand_row.get('SIMILARITY_SCORE', 0.0))

            if not item_cd or not candidate_item:
                continue

            if per_item_counts[item_cd] >= MAX_ROUTING_VARIANTS:
                continue

            routing = routing_cache.get(candidate_item)
            if routing is None or routing.empty:
                continue

            signature = build_routing_signature(routing)
            priority = 'primary' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'fallback'
            candidate_index = per_item_counts[item_cd] + 1
            candidate_id = f"{item_cd}_C{candidate_index:02d}"

            normalized = normalize_routing_frame(
                item_cd,
                candidate_id,
                routing,
                similarity=similarity,
                reference_item=candidate_item,
                priority=priority,
                signature=signature,
            )

            if normalized.empty:
                continue

            composed_frames.append(normalized)
            per_item_counts[item_cd] += 1

            process_count = len(normalized)
            summary_text = f"ê³µì • {process_count}ê°œ / ìœ ì‚¬ë„ {similarity:.2f}"
            enhanced_candidates.append({
                'ITEM_CD': item_cd,
                'CANDIDATE_ITEM_CD': candidate_item,
                'SIMILARITY_SCORE': similarity,
                'ROUTING_SIGNATURE': signature,
                'ROUTING_SUMMARY': summary_text,
                'PRIORITY': priority,
                'SIMILARITY_TIER': 'HIGH' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'LOW',
                'HAS_ROUTING': 'âœ“ ìˆìŒ',
                'PROCESS_COUNT': process_count,
            })



    raw_candidates_df = (
        pd.concat(raw_candidate_frames, ignore_index=True)
        if raw_candidate_frames
        else pd.DataFrame()
    )


    raw_candidates_df = (
        pd.concat(raw_candidate_frames, ignore_index=True)
        if raw_candidate_frames
        else pd.DataFrame()
    )

    if not raw_candidates_df.empty:
        raw_candidates_df = raw_candidates_df.sort_values(
            ['ITEM_CD', 'SIMILARITY_SCORE'], ascending=[True, False]
        )

        for _, cand_row in raw_candidates_df.iterrows():
            item_cd = cand_row.get('ITEM_CD')
            candidate_item = cand_row.get('CANDIDATE_ITEM_CD')
            similarity = float(cand_row.get('SIMILARITY_SCORE', 0.0))

            if not item_cd or not candidate_item:
                continue

            if per_item_counts[item_cd] >= MAX_ROUTING_VARIANTS:
                continue

            routing = routing_cache.get(candidate_item)
            if routing is None or routing.empty:
                continue

            signature = build_routing_signature(routing)
            priority = 'primary' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'fallback'
            candidate_index = per_item_counts[item_cd] + 1
            candidate_id = f"{item_cd}_C{candidate_index:02d}"

            normalized = normalize_routing_frame(
                item_cd,
                candidate_id,
                routing,
                similarity=similarity,
                reference_item=candidate_item,
                priority=priority,
                signature=signature,
            )

            if normalized.empty:
                continue

            composed_frames.append(normalized)
            per_item_counts[item_cd] += 1

            process_count = len(normalized)
            summary_text = f"ê³µì • {process_count}ê°œ / ìœ ì‚¬ë„ {similarity:.2f}"
            enhanced_candidates.append({
                'ITEM_CD': item_cd,
                'CANDIDATE_ITEM_CD': candidate_item,
                'SIMILARITY_SCORE': similarity,
                'ROUTING_SIGNATURE': signature,
                'ROUTING_SUMMARY': summary_text,
                'PRIORITY': priority,
                'SIMILARITY_TIER': 'HIGH' if similarity >= SIMILARITY_HIGH_THRESHOLD else 'LOW',
                'HAS_ROUTING': 'âœ“ ìˆìŒ',
                'PROCESS_COUNT': process_count,
            })


    final_routing_df = (
        pd.concat(composed_frames, ignore_index=True)
        if composed_frames
        else pd.DataFrame()
    )

    final_cand_df = (
        pd.DataFrame(enhanced_candidates)
        if enhanced_candidates
        else pd.DataFrame()
    )

    logger.info(
        "ë°°ì¹˜ ì˜ˆì¸¡ ì™„ë£Œ: %dê°œ ë¼ìš°íŒ… ì¡°í•©, %dê°œ í›„ë³´",
        len(final_routing_df),
        len(final_cand_df),
    )

    return final_routing_df, final_cand_df

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# âš™ï¸ ëŸ°íƒ€ì„ ì„¤ì • ì ìš© í—¬í¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def apply_runtime_config(runtime: PredictorRuntimeConfig) -> None:
    """ì›Œí¬í”Œë¡œìš° ì €ì¥ì†Œì—ì„œ ì „ë‹¬ëœ ì„¤ì •ì„ ì¦‰ì‹œ ë°˜ì˜í•œë‹¤."""

    global SIMILARITY_HIGH_THRESHOLD, MIN_SIMILARITY_THRESHOLD, MAX_ROUTING_VARIANTS

    SIMILARITY_HIGH_THRESHOLD = runtime.similarity_high_threshold
    MIN_SIMILARITY_THRESHOLD = runtime.similarity_high_threshold
    MAX_ROUTING_VARIANTS = runtime.max_routing_variants

    SCENARIO_CONFIG.TRIM_STD_ENABLED = runtime.trim_std_enabled
    SCENARIO_CONFIG.TRIM_LOWER_PERCENT = runtime.trim_lower_percent
    SCENARIO_CONFIG.TRIM_UPPER_PERCENT = runtime.trim_upper_percent

    _configure_encoding_cache(
        runtime.encoding_cache_maxsize,
        runtime.encoding_cache_ttl_seconds,
    )

    logger.info(
        "ëŸ°íƒ€ì„ ì„¤ì • ê°±ì‹ : threshold=%.2f, variants=%d, trim_std=%s (%.2f~%.2f), encoding_cache=(size=%d, ttl=%ds)",
        SIMILARITY_HIGH_THRESHOLD,
        MAX_ROUTING_VARIANTS,
        SCENARIO_CONFIG.TRIM_STD_ENABLED,
        SCENARIO_CONFIG.TRIM_LOWER_PERCENT,
        SCENARIO_CONFIG.TRIM_UPPER_PERCENT,
        ENCODING_CACHE_MAXSIZE,
        ENCODING_CACHE_TTL_SECONDS,
    )


try:  # ëª¨ë“ˆ ë¡œë“œ ì‹œ ì´ˆê¸° ì„¤ì • ì ìš©
    apply_runtime_config(workflow_config_store.get_predictor_runtime())
except Exception as exc:  # pragma: no cover - ì„¤ì • íŒŒì¼ ë¯¸ì¡´ì¬ ë“±
    logger.debug("ê¸°ë³¸ ëŸ°íƒ€ì„ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: %s", exc)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ ë ˆê±°ì‹œ í˜¸í™˜ì„± í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def predict_single_item_with_statistics_improved(
    item_cd: str,
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    return predict_single_item_with_ml_optimized(
        item_cd, model_dir, top_k=top_k, miss_thr=miss_thr, 
        config=config, mode="summary"
    )

def predict_items_with_ml_improved(
    item_codes: List[str],
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
    config: TimeScenarioConfig = None,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    return predict_items_with_ml_optimized(
        item_codes, model_dir, top_k=top_k, miss_thr=miss_thr,
        config=config, mode="summary"
    )

def predict_single_item_with_ml(
    item_cd: str,
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    return predict_single_item_with_ml_optimized(
        item_cd, model_dir, top_k=top_k, miss_thr=miss_thr, mode="summary"
    )

def predict_items_with_ml(
    item_codes: List[str],
    model_dir: Union[str, Path],
    *,
    top_k: int = DEFAULT_TOP_K,
    miss_thr: float = MISSING_RATIO_THRESHOLD,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    return predict_items_with_ml_optimized(
        item_codes, model_dir, top_k=top_k, miss_thr=miss_thr, mode="summary"
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“¤ ëª¨ë“ˆ ë‚´ë³´ë‚´ê¸° ë° ì„¤ì • í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_scenario_config() -> TimeScenarioConfig:
    return SCENARIO_CONFIG

def set_scenario_config(config: TimeScenarioConfig):
    global SCENARIO_CONFIG
    SCENARIO_CONFIG = config
    logger.info("ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤")

def reset_scenario_config():
    global SCENARIO_CONFIG
    SCENARIO_CONFIG = TimeScenarioConfig()
    logger.info("ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •ì´ ì´ˆê¸°ê°’ìœ¼ë¡œ ì¬ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤")

__all__ = [
    "TimeScenarioConfig",
    "get_scenario_config",
    "set_scenario_config",
    "reset_scenario_config",
    "predict_single_item_with_ml_enhanced",
    "EnhancedModelManager",
    "validate_prediction_quality_enhanced",
    "predict_single_item_with_ml_optimized",
    "predict_items_with_ml_optimized",
    "predict_routing_from_similar_items",
    "predict_single_item_with_statistics_improved",
    "predict_items_with_ml_improved",
    "predict_single_item_with_ml",
    "predict_items_with_ml",
    "calculate_manufacturing_time_stats",
    "filter_by_similarity_threshold",
    "apply_similarity_weights",
    "safe_int_conversion",
    "safe_float_conversion",
    "apply_runtime_config",
]