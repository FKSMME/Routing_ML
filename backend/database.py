from __future__ import annotations

import atexit
import time
from dataclasses import dataclass
from datetime import datetime
from functools import lru_cache
from pathlib import Path
import threading
from contextlib import contextmanager
import warnings
from typing import Any, Dict, List, Optional, Sequence, Tuple

from backend.demo_data import (
    demo_mode_enabled,
    get_demo_item,
    get_demo_routing,
    has_demo_routing,
)

import pandas as pd
import pyodbc

from common.logger import get_logger
from backend.constants import TRAIN_FEATURES

logger = get_logger("database")

_DEMO_MODE = demo_mode_enabled()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 0) ê²½ë¡œ Â· ë·° ìƒìˆ˜ (ë¨¼ì € ì •ì˜)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BASE_DIR   = Path(__file__).resolve().parents[1]     # e.g., machine/
ACCESS_DIR = BASE_DIR / "routing_data"               # Access DB í´ë”

# DB íƒ€ì… ì„ íƒ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´)
import os
DB_TYPE = os.getenv("DB_TYPE", "ACCESS")  # ACCESS or MSSQL

# MSSQL ì—°ê²° ì •ë³´
MSSQL_CONFIG = {
    "server": os.getenv("MSSQL_SERVER", "K3-DB.ksm.co.kr,1433"),
    "database": os.getenv("MSSQL_DATABASE", "KsmErp"),
    "user": os.getenv("MSSQL_USER", "FKSM_BI"),
    "password": os.getenv("MSSQL_PASSWORD", ""),
    "encrypt": os.getenv("MSSQL_ENCRYPT", "False").lower() == "true",
    "trust_certificate": os.getenv("MSSQL_TRUST_CERTIFICATE", "True").lower() == "true",
}

# ë·° ì´ë¦„ (MSSQLì€ dbo. ìŠ¤í‚¤ë§ˆ ì‚¬ìš©, AccessëŠ” dbo_ ì ‘ë‘ì‚¬)
if DB_TYPE == "MSSQL":
    VIEW_ITEM_MASTER = "dbo.BI_ITEM_INFO_VIEW"
    VIEW_ROUTING     = "dbo.BI_ROUTING_HIS_VIEW"
    VIEW_WORK_RESULT = "dbo.BI_WORK_ORDER_RESULTS"
    VIEW_PURCHASE_ORDER = "dbo.BI_PUR_PO_VIEW"
else:
    VIEW_ITEM_MASTER = "dbo_BI_ITEM_INFO_VIEW"
    VIEW_ROUTING     = "dbo_BI_ROUTING_VIEW"
    VIEW_WORK_RESULT = "dbo_BI_WORK_ORDER_RESULTS"
    VIEW_PURCHASE_ORDER = None  # Access DBì—ëŠ” ì—†ìŒ

# ì œí•œëœ ì»¬ëŸ¼ ëª©ë¡ (SELECT * ë°©ì§€)
ITEM_MASTER_EXTRA_COLUMNS: List[str] = [
    "ITEM_GRP2",
    "ITEM_GRP2NM",
    "ITEM_GRP3",
    "ITEM_GRP3NM",
    "INSRT_DT",
    "MODI_DT",
]

# dict.fromkeys ë¡œ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
ITEM_MASTER_VIEW_COLUMNS: Tuple[str, ...] = tuple(
    dict.fromkeys(list(TRAIN_FEATURES) + ITEM_MASTER_EXTRA_COLUMNS)
)

ROUTING_VIEW_COLUMNS: Tuple[str, ...] = (
    "ITEM_CD",
    "ROUT_NO",
    "PROC_SEQ",
    "INSRT_DT",
    "INSIDE_FLAG",
    "JOB_CD",
    "JOB_NM",
    "RES_CD",
    "RES_DIS",
    "TIME_UNIT",
    "MFG_LT",
    "QUEUE_TIME",
    "SETUP_TIME",
    "RUN_TIME",
    "RUN_TIME_UNIT",
    "MACH_WORKED_HOURS",
    "ACT_SETUP_TIME",
    "ACT_RUN_TIME",
    "WAIT_TIME",
    "MOVE_TIME",
    "RUN_TIME_QTY",
    "BATCH_OPER",
    "BP_CD",
    "CUST_NM",
    "CUR_CD",
    "SUBCONTRACT_PRC",
    "TAX_TYPE",
    "MILESTONE_FLG",
    "INSP_FLG",
    "ROUT_ORDER",
    "VALID_FROM_DT",
    "VALID_TO_DT",
    "VIEW_REMARK",
    "ROUT_DOC",
    "DOC_INSIDE",
    "DOC_NO",
    "NC_PROGRAM",
    "NC_PROGRAM_WRITER",
    "NC_WRITER_NM",
    "NC_WRITE_DATE",
    "NC_REVIEWER",
    "NC_REVIEWER_NM",
    "NC_REVIEW_DT",
    "RAW_MATL_SIZE",
    "JAW_SIZE",
    "VALIDITY",
    "PROGRAM_REMARK",
    "OP_DRAW_NO",
    "MTMG_NUMB",
)

WORK_RESULT_VIEW_COLUMNS: Tuple[str, ...] = (
    "ITEM_CD",
    "PROC_SEQ",
    "WORK_CENTER",
    "WORK_CENTER_NM",
    "OPERATION_CD",
    "OPERATION_NM",
    "REPORT_DT",
    "ACT_RUN_TIME",
    "ACT_SETUP_TIME",
    "GOOD_QTY",
    "BAD_QTY",
    "RUN_QTY",
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1) "ê°€ì¥ ìµœê·¼ Access DB" ê²½ë¡œ ê³„ì‚°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _latest_db(path: Path | str) -> Path:
    """<routing_data> ì•ˆì—ì„œ ìµœì‹  *.accdb / *.mdb íŒŒì¼ ë°˜í™˜"""
    p = Path(path).expanduser()
    if not p.exists():
        raise FileNotFoundError(f"í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ â” {p}")
    files = sorted(
        list(p.glob("*.accdb")) + list(p.glob("*.mdb")),
        key=lambda f: f.stat().st_mtime,
        reverse=True,
    )
    if not files:
        raise FileNotFoundError(f"*.accdb / *.mdb íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ â” {p}")
    latest = files[0]
    logger.debug("[DB] ìµœì‹  Access ì„ íƒ: %s", latest.name)
    return latest

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2) ê¸°ë³¸ ì—°ê²° í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _create_mssql_connection() -> pyodbc.Connection:
    """MSSQL ì„œë²„ ì—°ê²° ìƒì„±"""
    conn_str = (
        f"DRIVER={{ODBC Driver 17 for SQL Server}};"
        f"SERVER={MSSQL_CONFIG['server']};"
        f"DATABASE={MSSQL_CONFIG['database']};"
        f"UID={MSSQL_CONFIG['user']};"
        f"PWD={MSSQL_CONFIG['password']};"
        f"Encrypt={'yes' if MSSQL_CONFIG['encrypt'] else 'no'};"
        f"TrustServerCertificate={'yes' if MSSQL_CONFIG['trust_certificate'] else 'no'};"
    )
    try:
        logger.debug(f"MSSQL ì—°ê²° ì‹œë„: {MSSQL_CONFIG['server']}/{MSSQL_CONFIG['database']}")
        return pyodbc.connect(conn_str, timeout=10)
    except pyodbc.Error as exc:
        raise ConnectionError(f"MSSQL DB ì—°ê²° ì‹¤íŒ¨: {exc}") from exc

def _create_access_connection() -> pyodbc.Connection:
    """Access DB ì—°ê²° ìƒì„±"""
    db_path = _latest_db(ACCESS_DIR)
    conn_str = (
        r"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};"
        fr"DBQ={db_path}"
    )
    try:
        return pyodbc.connect(conn_str, timeout=10)
    except pyodbc.Error as exc:
        raise ConnectionError(f"Access DB ì—°ê²° ì‹¤íŒ¨: {exc}") from exc

def _create_connection() -> pyodbc.Connection:
    """ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„± (DB_TYPEì— ë”°ë¼ ìë™ ì„ íƒ)"""
    if DB_TYPE == "MSSQL":
        return _create_mssql_connection()
    else:
        return _create_access_connection()

def _connect() -> pyodbc.Connection:
    """ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²° (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return _create_connection()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3) ì—°ê²° í’€ë§ í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class ConnectionPool:
    """ê°„ë‹¨í•œ ì—°ê²° í’€"""
    def __init__(self, max_connections=5):
        self.max_connections = max_connections
        self.connections = []
        self.lock = threading.Lock()
    
    @contextmanager
    def get_connection(self):
        """ì—°ê²° íšë“"""
        conn = None
        try:
            with self.lock:
                if self.connections:
                    conn = self.connections.pop()
                    # ì—°ê²° ìƒíƒœ í™•ì¸
                    try:
                        conn.execute("SELECT 1")  # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
                    except Exception:
                        # ì—°ê²°ì´ ëŠì–´ì¡Œìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                        try:
                            conn.close()
                        except Exception:
                            pass
                        conn = None
                
                if conn is None:
                    conn = _create_connection()  # ìƒˆ ì—°ê²° ìƒì„±
            yield conn
        except Exception:
            # ì—°ê²°ì— ë¬¸ì œê°€ ìˆìœ¼ë©´ ë‹«ê¸°
            if conn:
                try:
                    conn.close()
                except Exception:
                    pass
            raise
        finally:
            if conn:
                try:
                    with self.lock:
                        if len(self.connections) < self.max_connections:
                            self.connections.append(conn)
                        else:
                            conn.close()
                except Exception as e:
                    logger.warning(f"ì—°ê²° ë°˜í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                    try:
                        conn.close()
                    except Exception:
                        pass

# ì „ì—­ ì—°ê²° í’€
_connection_pool = ConnectionPool()


class CacheStats:
    """ë‹¨ìˆœ íˆíŠ¸/ë¯¸ìŠ¤ ì¹´ìš´í„°."""

    def __init__(self) -> None:
        self.hits = 0
        self.misses = 0
        self._lock = threading.RLock()

    def record_hit(self) -> None:
        with self._lock:
            self.hits += 1

    def record_miss(self) -> None:
        with self._lock:
            self.misses += 1

    def reset(self) -> None:
        with self._lock:
            self.hits = 0
            self.misses = 0

    def snapshot(self) -> Dict[str, Any]:
        with self._lock:
            total = self.hits + self.misses
            hit_rate = (self.hits / total) if total else 0.0
            return {
                "hits": self.hits,
                "misses": self.misses,
                "requests": total,
                "hit_rate": round(hit_rate, 4),
            }


_cache_lock = threading.RLock()
_item_master_cache_lock = threading.RLock()
_routing_cache_lock = threading.RLock()

_cache_versions: Dict[str, str] = {"item_master": "v0", "routing": "v0"}
_cache_counters: Dict[str, int] = {"item_master": 0, "routing": 0}
_cache_stats: Dict[str, CacheStats] = {
    "item_master": CacheStats(),
    "routing": CacheStats(),
}
_last_cache_fetch: Dict[str, Dict[str, Any]] = {"item_master": {}, "routing": {}}


def _snapshot_time() -> str:
    return datetime.utcnow().replace(microsecond=0).isoformat() + "Z"


def _sanitize_columns(
    columns: Optional[Sequence[str]],
    allowed: Sequence[str],
) -> Tuple[str, ...]:
    allowed_lookup = {col.upper(): col for col in allowed}
    if columns:
        sanitized: List[str] = []
        unknown: List[str] = []
        for column in columns:
            key = str(column).strip().upper()
            if not key:
                continue
            actual = allowed_lookup.get(key)
            if actual:
                if actual not in sanitized:
                    sanitized.append(actual)
            else:
                unknown.append(column)
        if unknown:
            logger.warning("[DB] í—ˆìš©ë˜ì§€ ì•Šì€ ì»¬ëŸ¼ ë¬´ì‹œ: %s", ", ".join(map(str, unknown)))
        if sanitized:
            return tuple(sanitized)
    return tuple(allowed)


def _build_select_query(
    view_name: str,
    columns: Sequence[str],
    *,
    where_clause: Optional[str] = None,
    order_clause: Optional[str] = None,
) -> str:
    column_clause = ", ".join(columns)
    query_parts = [f"SELECT {column_clause} FROM {view_name}"]
    if where_clause:
        query_parts.append(where_clause)
    if order_clause:
        query_parts.append(order_clause)
    return " \n".join(query_parts)


def _record_cache_event(
    dataset: str,
    *,
    cached: bool,
    columns: Sequence[str],
    context: Optional[Dict[str, Any]] = None,
) -> None:
    stats = _cache_stats.get(dataset)
    if stats is not None:
        if cached:
            stats.record_hit()
        else:
            stats.record_miss()

    with _cache_lock:
        _last_cache_fetch[dataset] = {
            "cached": cached,
            "columns": list(columns),
            "context": dict(context or {}),
            "timestamp": _snapshot_time(),
            "version": _cache_versions.get(dataset),
        }


def _next_version(dataset: str, tag: Optional[str]) -> str:
    with _cache_lock:
        if tag is not None:
            token = str(tag)
        else:
            _cache_counters[dataset] += 1
            token = f"v{_cache_counters[dataset]}"
        _cache_versions[dataset] = token
        return token

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4) ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def set_debug_mode(enabled: bool = True):
    """ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •"""
    import logging
    level = logging.DEBUG if enabled else logging.INFO
    
    # ëª¨ë“  ê´€ë ¨ ë¡œê±°ì˜ ë ˆë²¨ ë³€ê²½
    loggers = ['database', 'predictor_ml_improved']
    for logger_name in loggers:
        try:
            target_logger = logging.getLogger(logger_name)
            target_logger.setLevel(level)
            for handler in target_logger.handlers:
                handler.setLevel(level)
        except Exception as e:
            logger.warning(f"ë¡œê±° ì„¤ì • ì‹¤íŒ¨ {logger_name}: {e}")
    
    print(f"ë””ë²„ê·¸ ëª¨ë“œ: {'ON' if enabled else 'OFF'}")

def validate_system_requirements():
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
    issues = []
    
    # 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        __import__("pandas")
        __import__("numpy")
    except ImportError as e:
        issues.append(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
    
    # 2. Access ë“œë¼ì´ë²„ í™•ì¸
    try:
        drivers = pyodbc.drivers()
        access_drivers = [d for d in drivers if 'Access' in d]
        if not access_drivers:
            issues.append("Microsoft Access ODBC ë“œë¼ì´ë²„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        else:
            logger.info(f"ë°œê²¬ëœ Access ë“œë¼ì´ë²„: {access_drivers}")
    except Exception as e:
        issues.append(f"ODBC ë“œë¼ì´ë²„ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    # 3. ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ í™•ì¸
    try:
        db_path = _latest_db(ACCESS_DIR)
        if not db_path.exists():
            issues.append(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì—†ìŒ: {db_path}")
        else:
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ í™•ì¸: {db_path}")
    except Exception as e:
        issues.append(f"ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ í™•ì¸ ì‹¤íŒ¨: {e}")
    
    if issues:
        for issue in issues:
            logger.error(f"ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë¬¸ì œ: {issue}")
        return False, issues
    else:
        logger.info("ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì™„ë£Œ")
        return True, []

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5) ì¿¼ë¦¬ ì‹¤í–‰ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _prepare_params(params: Sequence[Any] | None) -> Tuple[Any, ...]:
    if params is None:
        return tuple()
    if isinstance(params, tuple):
        return params
    return tuple(params)


def _run_query_optimized(query: str, params: Sequence[Any] | None = None, retries: int = 3, delay: float = 2.0) -> pd.DataFrame:
    """ìµœì í™”ëœ ì¿¼ë¦¬ ì‹¤í–‰ (ì—°ê²° í’€ ì‚¬ìš©)"""
    for attempt in range(1, retries + 1):
        try:
            prepared_params = _prepare_params(params)
            if "?" in query and not prepared_params:
                raise ValueError("íŒŒë¼ë¯¸í„° í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆëŠ” ì¿¼ë¦¬ì—ëŠ” ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤")
            with _connection_pool.get_connection() as conn:
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore', message='pandas only supports SQLAlchemy connectable')
                    df = pd.read_sql(query, conn, params=list(prepared_params))

            if df.empty:
                logger.debug("[DB] ë¹ˆ ê²°ê³¼ì„¸íŠ¸ ë°˜í™˜ â€“ %s", query[:80])
            return df
            
        except Exception as exc:
            if attempt < retries:
                logger.warning(f"[DB] ì¿¼ë¦¬ ì¬ì‹œë„ {attempt}/{retries}: {exc}")
                time.sleep(delay)
                continue
            raise RuntimeError(f"[DB] ì¿¼ë¦¬ ì‹¤íŒ¨: {exc}") from exc

def _run_query(
    query: str,
    params: Sequence[Any] | None = None,
    retries: int = 3,
    delay: float = 2.0,
) -> pd.DataFrame:
    """ì¿¼ë¦¬ ì‹¤í–‰ â†’ DataFrame (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    for attempt in range(1, retries + 1):
        try:
            prepared_params = _prepare_params(params)
            if "?" in query and not prepared_params:
                raise ValueError("íŒŒë¼ë¯¸í„° í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆëŠ” ì¿¼ë¦¬ì—ëŠ” ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤")
            with _connect() as conn:
                # pandas ê²½ê³  ì–µì œ
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore',
                                          message='pandas only supports SQLAlchemy connectable')
                    df = pd.read_sql(query, conn, params=list(prepared_params))
            if df.empty:
                logger.warning("[DB] ë¹ˆ ê²°ê³¼ì„¸íŠ¸ ë°˜í™˜ â€“ %s", query[:80])
            return df
        except Exception as exc:
            if attempt < retries:
                logger.warning(f"[DB] ì¿¼ë¦¬ ì¬ì‹œë„ {attempt}/{retries}: {exc}")
                time.sleep(delay)
                continue
            raise RuntimeError(f"[DB] ì¿¼ë¦¬ ì‹¤íŒ¨: {exc}") from exc

def _load_item_master(columns_key: Tuple[str, ...]) -> pd.DataFrame:
    query = _build_select_query(VIEW_ITEM_MASTER, columns_key)
    return _run_query(query, ())


@lru_cache(maxsize=32)
def _fetch_item_master_cached(version: str, columns_key: Tuple[str, ...]) -> pd.DataFrame:
    return _load_item_master(columns_key)


def _load_single_item(item_cd_upper: str, columns_key: Tuple[str, ...]) -> pd.DataFrame:
    query = _build_select_query(
        VIEW_ITEM_MASTER,
        columns_key,
        where_clause="WHERE ITEM_CD = ?",
    )
    return _run_query(query, (item_cd_upper,))


@lru_cache(maxsize=512)
def _fetch_single_item_cached(
    version: str,
    item_cd_upper: str,
    columns_key: Tuple[str, ...],
) -> pd.DataFrame:
    return _load_single_item(item_cd_upper, columns_key)


def _load_routing_by_rout_no(
    item_cd_upper: str,
    rout_no: str,
    columns_key: Tuple[str, ...],
) -> pd.DataFrame:
    query = _build_select_query(
        VIEW_ROUTING,
        columns_key,
        where_clause="WHERE ITEM_CD = ? AND ROUT_NO = ?",
        order_clause="ORDER BY PROC_SEQ",
    )
    return _run_query(query, (item_cd_upper, rout_no))


def _load_all_routings(item_cd_upper: str, columns_key: Tuple[str, ...]) -> pd.DataFrame:
    query = _build_select_query(
        VIEW_ROUTING,
        columns_key,
        where_clause="WHERE ITEM_CD = ?",
        order_clause="ORDER BY ROUT_NO, PROC_SEQ",
    )
    return _run_query(query, (item_cd_upper,))


def _load_latest_routing(
    item_cd_upper: str,
    selection_mode: str,
    columns_key: Tuple[str, ...],
) -> pd.DataFrame:
    if selection_mode == "most_used":
        count_query = "\n".join(
            [
                "SELECT ROUT_NO, COUNT(*) as PROC_COUNT",
                f"FROM {VIEW_ROUTING}",
                "WHERE ITEM_CD = ?",
                "GROUP BY ROUT_NO",
                "ORDER BY COUNT(*) DESC, ROUT_NO DESC",
            ]
        )
        count_df = _run_query(count_query, (item_cd_upper,))
        if count_df.empty:
            logger.warning("[DB] ROUT_NOë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ â€“ ITEM_CD: %s", item_cd_upper)
            return pd.DataFrame()
        most_used_rout_no = count_df.iloc[0]["ROUT_NO"]
        logger.info(
            "[DB] ìµœë‹¤ ê³µì • ROUT_NO ì„ íƒ: %s (%sê°œ ê³µì •)",
            most_used_rout_no,
            count_df.iloc[0]["PROC_COUNT"],
        )
        return _load_routing_by_rout_no(item_cd_upper, most_used_rout_no, columns_key)

    all_routing_query = "\n".join(
        [
            "SELECT DISTINCT ROUT_NO, INSRT_DT",
            f"FROM {VIEW_ROUTING}",
            "WHERE ITEM_CD = ?",
            "ORDER BY INSRT_DT DESC, ROUT_NO DESC",
        ]
    )
    rout_df = _run_query(all_routing_query, (item_cd_upper,))
    if rout_df.empty:
        logger.warning("[DB] ROUT_NOë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ â€“ ITEM_CD: %s", item_cd_upper)
        return pd.DataFrame()

    if "INSRT_DT" in rout_df.columns:
        try:
            rout_df["INSRT_DT_SAFE"] = pd.to_datetime(rout_df["INSRT_DT"], errors="coerce")
            rout_df_sorted = rout_df.sort_values(
                ["INSRT_DT_SAFE", "ROUT_NO"],
                ascending=[False, False],
                na_position="last",
            )
            latest_rout_no = rout_df_sorted.iloc[0]["ROUT_NO"]
        except Exception as exc:
            logger.warning("[DB] INSRT_DT ì •ë ¬ ì‹¤íŒ¨, ROUT_NOë¡œë§Œ ì •ë ¬: %s", exc)
            latest_rout_no = rout_df.sort_values("ROUT_NO", ascending=False).iloc[0]["ROUT_NO"]
    else:
        latest_rout_no = rout_df.sort_values("ROUT_NO", ascending=False).iloc[0]["ROUT_NO"]

    logger.info("[DB] ìµœì‹  ROUT_NO ì„ íƒ: %s", latest_rout_no)
    return _load_routing_by_rout_no(item_cd_upper, latest_rout_no, columns_key)


@lru_cache(maxsize=512)
def _fetch_latest_routing_cached(
    version: str,
    item_cd_upper: str,
    selection_mode: str,
    columns_key: Tuple[str, ...],
) -> pd.DataFrame:
    return _load_latest_routing(item_cd_upper, selection_mode, columns_key)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6) ê³µê°œ API - ë‹¨ì¼ ì¡°íšŒ (í•„ìˆ˜ í•¨ìˆ˜ë“¤)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fetch_item_master(
    columns: Optional[List[str]] = None,
    *,
    use_cache: bool = True,
) -> pd.DataFrame:
    """ì „ì²´ í’ˆëª© ë§ˆìŠ¤í„° ì¡°íšŒ"""

    columns_key = _sanitize_columns(columns, ITEM_MASTER_VIEW_COLUMNS)
    logger.info("[DB] ITEM_MASTER ì¡°íšŒâ€¦")

    if use_cache:
        with _item_master_cache_lock:
            before = _fetch_item_master_cached.cache_info()
            df = _fetch_item_master_cached(
                _cache_versions["item_master"],
                columns_key,
            )
            after = _fetch_item_master_cached.cache_info()
            cached = after.hits > before.hits
    else:
        df = _load_item_master(columns_key)
        cached = False

    _record_cache_event(
        "item_master",
        cached=cached,
        columns=columns_key,
        context={"mode": "bulk"},
    )
    return df.copy()


def fetch_single_item(
    item_cd: str,
    columns: Optional[List[str]] = None,
    *,
    use_cache: bool = True,
) -> pd.DataFrame:
    """ë‹¨ì¼ í’ˆëª© ì •ë³´ ì¡°íšŒ - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ"""

    logger.debug("[DB] ITEM ë‹¨ê±´ ì¡°íšŒ: %s", item_cd)
    item_cd_upper = item_cd.upper().strip()
    columns_key = _sanitize_columns(columns, ITEM_MASTER_VIEW_COLUMNS)

    cached = False
    fallback_used = False
    result = pd.DataFrame()

    try:
        if use_cache:
            with _item_master_cache_lock:
                before = _fetch_single_item_cached.cache_info()
                df = _fetch_single_item_cached(
                    _cache_versions["item_master"],
                    item_cd_upper,
                    columns_key,
                )
                after = _fetch_single_item_cached.cache_info()
                cached = after.hits > before.hits
        else:
            df = _load_single_item(item_cd_upper, columns_key)

        if df.empty:
            fallback_used = True
            like_query = _build_select_query(
                VIEW_ITEM_MASTER,
                columns_key,
                where_clause="WHERE ITEM_CD LIKE ?",
            )
            df = _run_query(like_query, (f"%{item_cd_upper}%",))
            if not df.empty and len(df) == 1:
                logger.info("[DB] LIKE ê²€ìƒ‰ìœ¼ë¡œ í’ˆëª© ë°œê²¬: %s", df["ITEM_CD"].iloc[0])
            cached = False

        result = df.copy()
        if result.empty and _DEMO_MODE:
            demo_df = get_demo_item(item_cd_upper)
            if not demo_df.empty:
                logger.info("[DB] ë°ëª¨ ë°ì´í„°ë¡œ í’ˆëª© ì‘ë‹µ: %s", item_cd_upper)
                result = demo_df
                fallback_used = True

    except Exception as exc:
        logger.error("[DB] í’ˆëª© ì¡°íšŒ ì˜¤ë¥˜: %s", exc)
        result = pd.DataFrame()
        cached = False
        fallback_used = True
    finally:
        _record_cache_event(
            "item_master",
            cached=cached and not fallback_used,
            columns=columns_key,
            context={"mode": "single", "item_cd": item_cd_upper},
        )

    return result


def fetch_routing_for_item(
    item_cd: str,
    latest_only: bool = True,
    selection_mode: str = "latest",
    *,
    use_cache: bool = True,
) -> pd.DataFrame:
    """íŠ¹ì • í’ˆëª©ì˜ ë¼ìš°íŒ… ì •ë³´ ì¡°íšŒ - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ"""

    logger.debug(
        "[DB] ROUTING ì¡°íšŒ: %s (latest_only=%s, mode=%s)",
        item_cd,
        latest_only,
        selection_mode,
    )

    item_cd_upper = item_cd.upper().strip()
    columns_key = ROUTING_VIEW_COLUMNS
    cached = False
    result = pd.DataFrame()

    try:
        if latest_only:
            if use_cache:
                with _routing_cache_lock:
                    before = _fetch_latest_routing_cached.cache_info()
                    df = _fetch_latest_routing_cached(
                        _cache_versions["routing"],
                        item_cd_upper,
                        selection_mode,
                        columns_key,
                    )
                    after = _fetch_latest_routing_cached.cache_info()
                    cached = after.hits > before.hits
            else:
                df = _load_latest_routing(item_cd_upper, selection_mode, columns_key)

            if not df.empty:
                proc_seqs = df["PROC_SEQ"].unique()
                logger.info(
                    "[DB] ë¼ìš°íŒ… ì¡°íšŒ ì„±ê³µ: ROUT_NO=%s, %sê°œ ê³µì •",
                    df["ROUT_NO"].iloc[0],
                    len(df),
                )
                logger.debug("[DB] PROC_SEQ ëª©ë¡: %s", sorted(proc_seqs))

        else:
            df = _load_all_routings(item_cd_upper, columns_key)
            cached = False
            if df.empty:
                logger.warning("[DB] ë¹ˆ ê²°ê³¼ì„¸íŠ¸ ë°˜í™˜ â€“ ITEM_CD: %s", item_cd)
            else:
                rout_counts = df["ROUT_NO"].value_counts()
                logger.info(
                    "[DB] ë¼ìš°íŒ… ì¡°íšŒ ì„±ê³µ: ì´ %sê°œ ê³µì •, %sê°œ ROUT_NO",
                    len(df),
                    len(rout_counts),
                )
                for rout_no, count in rout_counts.items():
                    logger.debug("  - ROUT_NO: %s, ê³µì • ìˆ˜: %s", rout_no, count)

        result = df.copy()
        if result.empty and _DEMO_MODE:
            demo_df = get_demo_routing(item_cd_upper)
            if not demo_df.empty:
                logger.info("[DB] ë°ëª¨ ë°ì´í„° ë¼ìš°íŒ… ì‚¬ìš©: %s", item_cd_upper)
                result = demo_df
                cached = False

    except Exception as exc:
        logger.error("[DB] ë¼ìš°íŒ… ì¡°íšŒ ì˜¤ë¥˜: %s", exc)
        result = pd.DataFrame()
        cached = False

    finally:
        _record_cache_event(
            "routing",
            cached=cached,
            columns=columns_key,
            context={
                "item_cd": item_cd_upper,
                "latest_only": latest_only,
                "selection_mode": selection_mode,
            },
        )

    return result


def fetch_work_results_for_item(item_cd: str) -> pd.DataFrame:
    """ë‹¨ì¼ í’ˆëª©ì˜ ì‘ì—… ê²°ê³¼ ì¡°íšŒ"""

    item_cd_upper = item_cd.upper().strip()
    query = _build_select_query(
        VIEW_WORK_RESULT,
        WORK_RESULT_VIEW_COLUMNS,
        where_clause="WHERE ITEM_CD = ?",
    )
    logger.debug("[DB] WORK_RESULT ì¡°íšŒ: %s", item_cd_upper)
    return _run_query(query, (item_cd_upper,)).copy()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7) ğŸš€ ë°°ì¹˜ ì¿¼ë¦¬ API (ê³ ì† ì„±ëŠ¥ ìµœì í™”ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_items_batch(item_codes: List[str]) -> Dict[str, pd.DataFrame]:
    """ğŸš€ ì—¬ëŸ¬ í’ˆëª©ì˜ ë§ˆìŠ¤í„° ì •ë³´ë¥¼ í•œ ë²ˆì— ì¡°íšŒ - pandas ê²½ê³  ì–µì œ"""
    if not item_codes:
        return {}

    item_codes = [code.upper() for code in item_codes]
    columns_key = ITEM_MASTER_VIEW_COLUMNS

    batch_size = 100
    all_results = {}

    for i in range(0, len(item_codes), batch_size):
        batch_codes = item_codes[i:i + batch_size]
        placeholders = ','.join('?' * len(batch_codes))

        query = _build_select_query(
            VIEW_ITEM_MASTER,
            columns_key,
            where_clause=f"WHERE ITEM_CD IN ({placeholders})",
        )

        logger.info("[DB] ITEM_MASTER ë°°ì¹˜ ì¡°íšŒ: %sê°œ í’ˆëª© (ë°°ì¹˜ %s)",
                   len(batch_codes), i // batch_size + 1)

        try:
            df = _run_query_optimized(query, batch_codes)
                
            if not df.empty:
                for item_cd in batch_codes:
                    item_data = df[df['ITEM_CD'] == item_cd].copy()
                    all_results[item_cd] = item_data
            else:
                for item_cd in batch_codes:
                    all_results[item_cd] = pd.DataFrame()
                    
        except Exception as exc:
            logger.error("[DB] í’ˆëª© ë°°ì¹˜ ì¿¼ë¦¬ ì‹¤íŒ¨ (ë°°ì¹˜ %s): %s", i // batch_size + 1, exc)
            for item_cd in batch_codes:
                all_results[item_cd] = pd.DataFrame()
    
    found_count = len([k for k, v in all_results.items() if not v.empty])
    logger.info("[DB] í’ˆëª© ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: %sê°œ ìš”ì²­, %sê°œ ë°œê²¬", 
               len(item_codes), found_count)
    
    return all_results

def fetch_routings_for_items(item_codes: List[str], latest_only: bool = True) -> Dict[str, pd.DataFrame]:
    """ğŸš€ ì—¬ëŸ¬ í’ˆëª©ì˜ ë¼ìš°íŒ… ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ"""
    if not item_codes:
        return {}

    item_codes = [code.upper() for code in item_codes]
    columns_key = ROUTING_VIEW_COLUMNS

    if latest_only:
        # ê°œë³„ ì¡°íšŒ ë°©ì‹ (ìµœì‹  ROUT_NO ë¡œì§ì´ ë³µì¡í•˜ë¯€ë¡œ)
        all_results = {}

        for item_cd in item_codes:
            try:
                routing_df = fetch_routing_for_item(item_cd, latest_only=True)
                all_results[item_cd] = routing_df
            except Exception as e:
                logger.error(f"[DB] {item_cd} ë¼ìš°íŒ… ì¡°íšŒ ì‹¤íŒ¨: {e}")
                all_results[item_cd] = pd.DataFrame()
        
        found_count = len([k for k, v in all_results.items() if not v.empty])
        logger.info(f"[DB] ë¼ìš°íŒ… ê°œë³„ ì¡°íšŒ ì™„ë£Œ (latest_only): {len(item_codes)}ê°œ ìš”ì²­, {found_count}ê°œ ë°œê²¬")
        
        return all_results
    
    # ëª¨ë“  ë¼ìš°íŒ… ì¡°íšŒëŠ” ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 100
    all_results = {}
    
    for i in range(0, len(item_codes), batch_size):
        batch_codes = item_codes[i:i + batch_size]
        placeholders = ','.join('?' * len(batch_codes))

        query = _build_select_query(
            VIEW_ROUTING,
            columns_key,
            where_clause=f"WHERE ITEM_CD IN ({placeholders})",
            order_clause="ORDER BY ITEM_CD, ROUT_NO, PROC_SEQ",
        )

        logger.debug("[DB] ROUTING ë°°ì¹˜ ì¡°íšŒ: %sê°œ í’ˆëª© (ë°°ì¹˜ %s)",
                    len(batch_codes), i // batch_size + 1)

        try:
            df = _run_query_optimized(query, batch_codes)
                
            if not df.empty:
                for item_cd in batch_codes:
                    item_routing = df[df['ITEM_CD'] == item_cd].copy()
                    if not item_routing.empty:
                        rout_counts = item_routing['ROUT_NO'].value_counts()
                        if len(rout_counts) > 1:
                            logger.debug(f"[DB] {item_cd}: {len(rout_counts)}ê°œ ROUT_NO ë°œê²¬")
                    all_results[item_cd] = item_routing
            else:
                for item_cd in batch_codes:
                    all_results[item_cd] = pd.DataFrame()
                    
        except Exception as exc:
            logger.error("[DB] ë¼ìš°íŒ… ë°°ì¹˜ ì¿¼ë¦¬ ì‹¤íŒ¨ (ë°°ì¹˜ %s): %s", i // batch_size + 1, exc)
            for item_cd in batch_codes:
                all_results[item_cd] = pd.DataFrame()
    
    found_count = len([k for k, v in all_results.items() if not v.empty])
    logger.info("[DB] ë¼ìš°íŒ… ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: %sê°œ ìš”ì²­, %sê°œ ë°œê²¬", 
               len(item_codes), found_count)
    
    return all_results

def fetch_work_results_batch(item_codes: List[str]) -> Dict[str, pd.DataFrame]:
    """ğŸš€ ì—¬ëŸ¬ í’ˆëª©ì˜ ì‘ì—… ê²°ê³¼ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ"""
    if not item_codes:
        return {}

    item_codes = [code.upper() for code in item_codes]
    columns_key = WORK_RESULT_VIEW_COLUMNS

    # Access DBì˜ IN ì ˆ ì œí•œì„ ê³ ë ¤í•œ ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 100
    all_results = {}

    for i in range(0, len(item_codes), batch_size):
        batch_codes = item_codes[i:i + batch_size]
        placeholders = ','.join('?' * len(batch_codes))

        query = _build_select_query(
            VIEW_WORK_RESULT,
            columns_key,
            where_clause=f"WHERE ITEM_CD IN ({placeholders})",
        )
        
        logger.debug("[DB] WORK_RESULT ë°°ì¹˜ ì¡°íšŒ: %sê°œ í’ˆëª© (ë°°ì¹˜ %s)", 
                    len(batch_codes), i // batch_size + 1)
        
        try:
            df = _run_query_optimized(query, batch_codes)
                
            if not df.empty:
                # í’ˆëª©ë³„ë¡œ DataFrame ë¶„ë¦¬
                for item_cd in batch_codes:
                    item_results = df[df['ITEM_CD'] == item_cd].copy()
                    all_results[item_cd] = item_results
            else:
                # ë¹ˆ ê²°ê³¼ì¸ ê²½ìš°ì—ë„ í’ˆëª©ë³„ë¡œ ë¹ˆ DataFrame ìƒì„±
                for item_cd in batch_codes:
                    all_results[item_cd] = pd.DataFrame()
                    
        except Exception as exc:
            logger.error("[DB] ì‘ì—…ê²°ê³¼ ë°°ì¹˜ ì¿¼ë¦¬ ì‹¤íŒ¨ (ë°°ì¹˜ %s): %s", i // batch_size + 1, exc)
            # ì‹¤íŒ¨í•œ ë°°ì¹˜ì˜ í’ˆëª©ë“¤ì€ ë¹ˆ DataFrameìœ¼ë¡œ ì²˜ë¦¬
            for item_cd in batch_codes:
                all_results[item_cd] = pd.DataFrame()
    
    found_count = len([k for k, v in all_results.items() if not v.empty])
    logger.info("[DB] ì‘ì—…ê²°ê³¼ ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: %sê°œ ìš”ì²­, %sê°œ ë°œê²¬", 
               len(item_codes), found_count)
    
    return all_results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8) ğŸ’¡ ê³ ê¸‰ ë°°ì¹˜ ì¡°íšŒ (í•„ìš”ì‹œ ì‚¬ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_all_data_batch(item_codes: List[str], latest_routing_only: bool = True) -> Dict[str, Dict[str, pd.DataFrame]]:
    """
    ğŸš€ ëª¨ë“  ê´€ë ¨ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ - ìµœê³  ì„±ëŠ¥ ìµœì í™”
    
    Args:
        item_codes: í’ˆëª© ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        latest_routing_only: Trueì¼ ê²½ìš° ê° í’ˆëª©ì˜ ìµœì‹  ë¼ìš°íŒ…ë§Œ í¬í•¨ (ê¸°ë³¸ê°’ True)
    
    Returns:
        Dict[str, Dict[str, pd.DataFrame]]: í’ˆëª©ë³„ ëª¨ë“  ë°ì´í„°
    """
    if not item_codes:
        return {}
    
    logger.info("[DB] ì „ì²´ ë°ì´í„° ë°°ì¹˜ ì¡°íšŒ ì‹œì‘: %sê°œ í’ˆëª©", len(item_codes))
    
    # ë³‘ë ¬ë¡œ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
    items_data = fetch_items_batch(item_codes)
    routings_data = fetch_routings_for_items(item_codes, latest_only=latest_routing_only)
    work_results_data = fetch_work_results_batch(item_codes)
    
    # í’ˆëª©ë³„ë¡œ ëª¨ë“  ë°ì´í„° í†µí•©
    result = {}
    for item_cd in item_codes:
        item_cd_upper = item_cd.upper()
        result[item_cd_upper] = {
            'item_master': items_data.get(item_cd_upper, pd.DataFrame()),
            'routing': routings_data.get(item_cd_upper, pd.DataFrame()),
            'work_results': work_results_data.get(item_cd_upper, pd.DataFrame())
        }
    
    logger.info("[DB] ì „ì²´ ë°ì´í„° ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: %sê°œ í’ˆëª©", len(item_codes))
    return result

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 9) ğŸ”§ ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_connection() -> bool:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        with _connect() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT TOP 1 ITEM_CD FROM " + VIEW_ITEM_MASTER)
            result = cursor.fetchone()
            logger.info("[DB] ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: %s", result[0] if result else "ë°ì´í„° ì—†ìŒ")
            return True
    except Exception as exc:
        logger.error("[DB] ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: %s", exc)
        return False

def get_database_info() -> Dict[str, Any]:
    """ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        db_path = _latest_db(ACCESS_DIR)
        with _connect() as conn:
            cursor = conn.cursor()
            
            # í…Œì´ë¸” ì •ë³´ ì¡°íšŒ
            tables_info = {}
            for view_name in [VIEW_ITEM_MASTER, VIEW_ROUTING, VIEW_WORK_RESULT]:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {view_name}")
                    count = cursor.fetchone()[0]
                    tables_info[view_name] = count
                except Exception as e:
                    tables_info[view_name] = f"ì¡°íšŒ ì‹¤íŒ¨: {e}"
            
            return {
                'database_path': str(db_path),
                'database_size_mb': round(db_path.stat().st_size / 1024 / 1024, 2),
                'tables_info': tables_info,
                'connection_status': 'ì •ìƒ'
            }
    except Exception as exc:
        logger.error("[DB] ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: %s", exc)
        return {
            'connection_status': f'ì˜¤ë¥˜: {exc}',
            'database_path': 'ì•Œ ìˆ˜ ì—†ìŒ',
            'database_size_mb': 0,
            'tables_info': {}
        }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 10) ì¶”ê°€ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def check_item_has_routing(item_cd: str) -> bool:
    """í’ˆëª©ì˜ ë¼ìš°íŒ… ì¡´ì¬ ì—¬ë¶€ í™•ì¸ - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ"""
    item_cd_upper = item_cd.upper().strip()
    
    query = f"""
        SELECT COUNT(*) as cnt
        FROM {VIEW_ROUTING}
        WHERE ITEM_CD = ?
    """
    
    try:
        result = _run_query(query, [item_cd_upper])

        return result['cnt'].iloc[0] > 0 if not result.empty else False

    except Exception as e:
        logger.error(f"[DB] ë¼ìš°íŒ… ì¡´ì¬ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
        return has_demo_routing(item_cd_upper) if _DEMO_MODE else False

    if _DEMO_MODE and has_demo_routing(item_cd_upper):
        return True

def fetch_item_info_only(item_cd: str) -> pd.DataFrame:
    """
    í’ˆëª© ì •ë³´ë§Œ ì¡°íšŒ (ë¼ìš°íŒ… ì—†ì´)

    Args:
        item_cd: í’ˆëª© ì½”ë“œ

    Returns:
        pd.DataFrame: í’ˆëª© ì •ë³´
    """
    return fetch_single_item(item_cd, use_cache=True)

def fetch_routing_statistics(latest_only: bool = True) -> pd.DataFrame:
    """
    ë¼ìš°íŒ… í†µê³„ ì •ë³´ ì¡°íšŒ (í’ˆëª©ë³„ ê³µì • ìˆ˜, í‰ê·  ì‹œê°„ ë“±)
    
    Args:
        latest_only: Trueì¼ ê²½ìš° ê° í’ˆëª©ì˜ ìµœì‹  ROUT_NOë§Œ ì‚¬ìš© (ê¸°ë³¸ê°’ True)
    
    Returns:
        pd.DataFrame: í†µê³„ ì •ë³´
    """
    if latest_only:
        # ìµœì‹  ROUT_NOë§Œ ì‚¬ìš©í•œ í†µê³„
        query = f"""
        WITH LatestRouting AS (
            SELECT r1.*
            FROM {VIEW_ROUTING} r1
            INNER JOIN (
                SELECT ITEM_CD, MAX(INSRT_DT) as MAX_DT
                FROM {VIEW_ROUTING}
                GROUP BY ITEM_CD
            ) r2 ON r1.ITEM_CD = r2.ITEM_CD AND r1.INSRT_DT = r2.MAX_DT
        )
        SELECT 
            ITEM_CD,
            ROUT_NO,
            COUNT(DISTINCT PROC_SEQ) as PROCESS_COUNT,
            SUM(SETUP_TIME + RUN_TIME) as TOTAL_TIME,
            AVG(SETUP_TIME + RUN_TIME) as AVG_TIME_PER_PROCESS,
            MAX(PROC_SEQ) as MAX_PROC_SEQ
        FROM LatestRouting
        GROUP BY ITEM_CD, ROUT_NO
        """
    else:
        # ëª¨ë“  ROUT_NO í¬í•¨ í†µê³„
        query = f"""
        SELECT 
            ITEM_CD,
            COUNT(DISTINCT ROUT_NO) as ROUT_NO_COUNT,
            COUNT(DISTINCT PROC_SEQ) as PROCESS_COUNT,
            SUM(SETUP_TIME + RUN_TIME) as TOTAL_TIME,
            AVG(SETUP_TIME + RUN_TIME) as AVG_TIME_PER_PROCESS,
            MAX(PROC_SEQ) as MAX_PROC_SEQ
        FROM {VIEW_ROUTING}
        GROUP BY ITEM_CD
        """
    
    return _run_query(query)

def get_distinct_rout_nos(item_cd: str) -> List[str]:
    """
    íŠ¹ì • í’ˆëª©ì˜ ëª¨ë“  ROUT_NO ëª©ë¡ ì¡°íšŒ
    
    Args:
        item_cd: í’ˆëª© ì½”ë“œ
        
    Returns:
        List[str]: ROUT_NO ëª©ë¡ (ìµœì‹ ìˆœ)
    """
    query = f"""
        SELECT DISTINCT ROUT_NO, MAX(INSRT_DT) as LATEST_DT
        FROM {VIEW_ROUTING}
        WHERE ITEM_CD = ?
        GROUP BY ROUT_NO
        ORDER BY MAX(INSRT_DT) DESC
    """
    
    try:
        df = _run_query(query, [item_cd.upper()])
        return df['ROUT_NO'].tolist() if not df.empty else []
    except Exception as e:
        logger.error(f"[DB] ROUT_NO ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return []

def fetch_routing_by_rout_no(item_cd: str, rout_no: str) -> pd.DataFrame:
    """
    íŠ¹ì • ROUT_NOì˜ ë¼ìš°íŒ… ì •ë³´ ì¡°íšŒ

    Args:
        item_cd: í’ˆëª© ì½”ë“œ
        rout_no: ë¼ìš°íŒ… ë²ˆí˜¸

    Returns:
        pd.DataFrame: ë¼ìš°íŒ… ì •ë³´
    """
    logger.debug("[DB] íŠ¹ì • ROUT_NO ë¼ìš°íŒ… ì¡°íšŒ: %s, %s", item_cd, rout_no)
    return _load_routing_by_rout_no(
        item_cd.upper().strip(),
        rout_no,
        ROUTING_VIEW_COLUMNS,
    ).copy()


def fetch_purchase_order_items() -> pd.DataFrame:
    """
    ë°œì£¼ í’ˆëª© ëª©ë¡ ì¡°íšŒ (BI_PUR_PO_VIEW)

    Returns:
        pd.DataFrame: ë°œì£¼ í’ˆëª© ëª©ë¡ (ITEM_CD, PO_NO, PO_DATE, QTY ë“±)
    """
    if VIEW_PURCHASE_ORDER is None:
        logger.warning("[DB] BI_PUR_PO_VIEWëŠ” MSSQL ëª¨ë“œì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤ - ë°ëª¨ ë°ì´í„° ë°˜í™˜")
        # ë°ëª¨ ëª¨ë“œ: ìƒì‚° ì ‘ìˆ˜ëœ í’ˆëª© ìƒ˜í”Œ ë°˜í™˜
        from datetime import datetime, timedelta
        demo_items = []
        base_date = datetime.now() - timedelta(days=7)

        for i, item_cd in enumerate(['ITEM-001', 'ITEM-002', 'ITEM-003', 'ITEM-004', 'ITEM-005']):
            demo_items.append({
                'ITEM_CD': item_cd,
                'PO_NO': f'PO-2024-{1000+i:04d}',
                'PO_DATE': (base_date + timedelta(days=i)).strftime('%Y-%m-%d'),
                'QTY': (i + 1) * 100,
                'UNIT_PRICE': 50000 + (i * 10000),
                'VENDOR_CD': f'V{i+1:03d}',
                'VENDOR_NM': f'ê³µê¸‰ì—…ì²´{i+1}'
            })

        return pd.DataFrame(demo_items)

    query = f"""
        SELECT DISTINCT
            ITEM_CD,
            PO_NO,
            PO_DATE,
            QTY,
            UNIT_PRICE,
            VENDOR_CD,
            VENDOR_NM
        FROM {VIEW_PURCHASE_ORDER}
        WHERE ITEM_CD IS NOT NULL
        ORDER BY PO_DATE DESC
    """

    try:
        logger.info("[DB] ë°œì£¼ í’ˆëª© ëª©ë¡ ì¡°íšŒ ì¤‘...")
        return _run_query(query, ())
    except Exception as e:
        logger.error(f"[DB] ë°œì£¼ í’ˆëª© ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame()


def fetch_item_with_purchase_info(item_cd: str) -> pd.DataFrame:
    """
    í’ˆëª© ì •ë³´ + ë°œì£¼ ì •ë³´ ì¡°íšŒ

    Args:
        item_cd: í’ˆëª© ì½”ë“œ

    Returns:
        pd.DataFrame: í’ˆëª© ì •ë³´ì™€ ë°œì£¼ ì •ë³´ê°€ ê²°í•©ëœ ë°ì´í„°
    """
    item_cd_upper = item_cd.upper().strip()

    # í’ˆëª© ê¸°ë³¸ ì •ë³´
    item_info = fetch_single_item(item_cd_upper)

    if VIEW_PURCHASE_ORDER is None or item_info.empty:
        return item_info

    # ë°œì£¼ ì •ë³´
    query = f"""
        SELECT
            PO_NO,
            PO_DATE,
            QTY,
            UNIT_PRICE,
            VENDOR_CD,
            VENDOR_NM
        FROM {VIEW_PURCHASE_ORDER}
        WHERE ITEM_CD = ?
        ORDER BY PO_DATE DESC
    """

    try:
        po_info = _run_query(query, (item_cd_upper,))
        if not po_info.empty:
            # ê°€ì¥ ìµœê·¼ ë°œì£¼ ì •ë³´ë¥¼ í’ˆëª© ì •ë³´ì— ì¶”ê°€
            latest_po = po_info.iloc[0]
            for col in po_info.columns:
                item_info[f'LATEST_{col}'] = latest_po[col]
            item_info['PO_COUNT'] = len(po_info)
    except Exception as e:
        logger.warning(f"[DB] ë°œì£¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    return item_info


def invalidate_item_master_cache(tag: Optional[str] = None) -> str:
    """í’ˆëª© ë§ˆìŠ¤í„° ìºì‹œ ë¬´íš¨í™” ë° ë²„ì „ ê°±ì‹ ."""

    new_version = _next_version("item_master", tag)
    _fetch_item_master_cached.cache_clear()
    _fetch_single_item_cached.cache_clear()
    _cache_stats["item_master"].reset()
    with _cache_lock:
        _last_cache_fetch["item_master"] = {
            "cached": False,
            "columns": list(ITEM_MASTER_VIEW_COLUMNS),
            "context": {"action": "invalidate"},
            "timestamp": _snapshot_time(),
            "version": new_version,
        }
    return new_version


def invalidate_routing_cache(tag: Optional[str] = None) -> str:
    """ë¼ìš°íŒ… ìºì‹œ ë¬´íš¨í™” ë° ë²„ì „ ê°±ì‹ ."""

    new_version = _next_version("routing", tag)
    _fetch_latest_routing_cached.cache_clear()
    _cache_stats["routing"].reset()
    with _cache_lock:
        _last_cache_fetch["routing"] = {
            "cached": False,
            "columns": list(ROUTING_VIEW_COLUMNS),
            "context": {"action": "invalidate"},
            "timestamp": _snapshot_time(),
            "version": new_version,
        }
    return new_version


def get_cache_versions() -> Dict[str, str]:
    """í˜„ì¬ ìºì‹œ ë²„ì „ íƒœê·¸ ìŠ¤ëƒ…ìƒ·."""

    with _cache_lock:
        return dict(_cache_versions)


def get_cache_snapshot() -> Dict[str, Any]:
    """ìºì‹œ ì ì¤‘ë¥  ë° ìµœê·¼ ì¡°íšŒ ë©”íƒ€ë°ì´í„° ìŠ¤ëƒ…ìƒ·."""

    snapshot: Dict[str, Any] = {}
    with _cache_lock:
        for dataset in ("item_master", "routing"):
            stats = _cache_stats[dataset].snapshot()
            last_fetch = dict(_last_cache_fetch.get(dataset) or {})
            snapshot[dataset] = {
                "version": _cache_versions.get(dataset),
                "stats": stats,
                "last_fetch": last_fetch or None,
            }
    return snapshot

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 11) ì •ë¦¬ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cleanup_connections():
    """ì—°ê²° í’€ ì •ë¦¬"""
    try:
        with _connection_pool.lock:
            for conn in _connection_pool.connections:
                try:
                    conn.close()
                except Exception:
                    pass
            _connection_pool.connections.clear()
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ì—°ê²° í’€ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ëª¨ë“ˆ ì¢…ë£Œ ì‹œ ìë™ ì •ë¦¬
atexit.register(cleanup_connections)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 12) FastAPI ì˜ì¡´ì„± í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_db_connection():
    """
    FastAPI ì˜ì¡´ì„±ìœ¼ë¡œ ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì œê³µ.
    
    Yields:
        pyodbc.Connection: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        
    Example:
        @app.get("/items")
        def get_items(conn = Depends(get_db_connection)):
            df = pd.read_sql("SELECT * FROM ITEM_MASTER", conn)
            return df.to_dict('records')
    """
    with _connection_pool.get_connection() as conn:
        yield conn
