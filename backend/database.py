from __future__ import annotations

import atexit
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import threading
from contextlib import contextmanager
import warnings
from typing import Any, Dict, List, Optional, Sequence, Tuple

from backend.demo_data import (
    demo_mode_enabled,
    get_demo_item,
    get_demo_routing,
    has_demo_routing,
)

import pandas as pd

# pyodbcë¥¼ optionalë¡œ import (ODBC ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì„ ë•Œ ëŒ€ë¹„)
try:
    import pyodbc
    PYODBC_AVAILABLE = True
except ImportError:
    pyodbc = None
    PYODBC_AVAILABLE = False

from common.datetime_utils import utc_isoformat_z
from common.logger import get_logger
from backend.constants import TRAIN_FEATURES

logger = get_logger("database")

_DEMO_MODE = demo_mode_enabled()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 0) ê²½ë¡œ Â· ë·° ìƒìˆ˜ (ë¨¼ì € ì •ì˜)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BASE_DIR   = Path(__file__).resolve().parents[1]     # e.g., machine/

# DB íƒ€ì… ì„ íƒ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´)
import os
DB_TYPE = os.getenv("DB_TYPE", "MSSQL").upper()  # MSSQL only

# MSSQL ì—°ê²° ì •ë³´
MSSQL_CONFIG = {
    "server": os.getenv("MSSQL_SERVER", "K3-DB.ksm.co.kr,1433"),
    "database": os.getenv("MSSQL_DATABASE", "KsmErp"),
    "user": os.getenv("MSSQL_USER", "FKSM_BI"),
    "password": os.getenv("MSSQL_PASSWORD", ""),
    "encrypt": os.getenv("MSSQL_ENCRYPT", "False").lower() == "true",
    "trust_certificate": os.getenv("MSSQL_TRUST_CERTIFICATE", "True").lower() == "true",
}

# ë·° ì´ë¦„ (MSSQLì€ dbo. ìŠ¤í‚¤ë§ˆ ì‚¬ìš©)
VIEW_ITEM_MASTER = "dbo.BI_ITEM_INFO_VIEW"
VIEW_ROUTING     = "dbo.BI_ROUTING_HIS_VIEW"
VIEW_WORK_RESULT = "dbo.BI_WORK_ORDER_RESULTS"
VIEW_PURCHASE_ORDER = "dbo.BI_PUR_PO_VIEW"

# ì œí•œëœ ì»¬ëŸ¼ ëª©ë¡ (SELECT * ë°©ì§€)
ITEM_MASTER_EXTRA_COLUMNS: List[str] = [
    "ITEM_GRP2",
    "ITEM_GRP2NM",
    "ITEM_GRP3",
    "ITEM_GRP3NM",
    "INSRT_DT",
    "MODI_DT",
]

# dict.fromkeys ë¡œ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
ITEM_MASTER_VIEW_COLUMNS: Tuple[str, ...] = tuple(
    dict.fromkeys(list(TRAIN_FEATURES) + ITEM_MASTER_EXTRA_COLUMNS)
)

ROUTING_VIEW_COLUMNS: Tuple[str, ...] = (
    "ITEM_CD",
    "ROUT_NO",
    "PROC_SEQ",
    "INSRT_DT",
    "INSIDE_FLAG",
    "JOB_CD",
    "JOB_NM",
    "RES_CD",
    "RES_DIS",
    "TIME_UNIT",
    "MFG_LT",
    "QUEUE_TIME",
    "SETUP_TIME",
    "RUN_TIME",
    "RUN_TIME_UNIT",
    "MACH_WORKED_HOURS",
    "ACT_SETUP_TIME",
    "ACT_RUN_TIME",
    "WAIT_TIME",
    "MOVE_TIME",
    "RUN_TIME_QTY",
    "BATCH_OPER",
    "BP_CD",
    "CUST_NM",
    "CUR_CD",
    "SUBCONTRACT_PRC",
    "TAX_TYPE",
    "MILESTONE_FLG",
    "INSP_FLG",
    "ROUT_ORDER",
    "VALID_FROM_DT",
    "VALID_TO_DT",
    "VIEW_REMARK",
    "ROUT_DOC",
    "DOC_INSIDE",
    "DOC_NO",
    "NC_PROGRAM",
    "NC_PROGRAM_WRITER",
    "NC_WRITER_NM",
    "NC_WRITE_DATE",
    "NC_REVIEWER",
    "NC_REVIEWER_NM",
    "NC_REVIEW_DT",
    "RAW_MATL_SIZE",
    "JAW_SIZE",
    "VALIDITY",
    "PROGRAM_REMARK",
    "OP_DRAW_NO",
    "MTMG_NUMB",
)

WORK_RESULT_VIEW_COLUMNS: Tuple[str, ...] = (
    "ITEM_CD",
    "PROC_SEQ",
    "WORK_CENTER",
    "WORK_CENTER_NM",
    "OPERATION_CD",
    "OPERATION_NM",
    "REPORT_DT",
    "ACT_RUN_TIME",
    "ACT_SETUP_TIME",
    "GOOD_QTY",
    "BAD_QTY",
    "RUN_QTY",
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1) ê¸°ë³¸ ì—°ê²° í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _create_mssql_connection_with_config(config: Optional[Dict[str, Any]] = None):
    """MSSQL ì„œë²„ ì—°ê²° ìƒì„± (ì»¤ìŠ¤í…€ ì„¤ì • ì§€ì›)

    Args:
        config: ì—°ê²° ì„¤ì • ë”•ì…”ë„ˆë¦¬. Noneì´ë©´ MSSQL_CONFIG ì‚¬ìš©.
                í•„ìˆ˜ í‚¤: server, database, user, password
                ì„ íƒ í‚¤: encrypt, trust_certificate

    Returns:
        pyodbc.Connection: MSSQL ì—°ê²° ê°ì²´
    """
    if not PYODBC_AVAILABLE:
        raise ImportError("pyodbc is not available. Please install ODBC drivers.")

    # ì„¤ì • ê²°ì •
    if config is None:
        config = MSSQL_CONFIG

    # FreeTDS ë“œë¼ì´ë²„ ìš°ì„  ì‹œë„, ì—†ìœ¼ë©´ ODBC Driver 17 ì‚¬ìš©
    available_drivers = pyodbc.drivers()

    if "FreeTDS" in available_drivers:
        driver_name = "FreeTDS"
    elif "ODBC Driver 17 for SQL Server" in available_drivers:
        driver_name = "ODBC Driver 17 for SQL Server"
    elif "ODBC Driver 18 for SQL Server" in available_drivers:
        driver_name = "ODBC Driver 18 for SQL Server"
    else:
        raise ConnectionError(f"No suitable ODBC driver found. Available: {available_drivers}")

    conn_str = (
        f"DRIVER={{{driver_name}}};"
        f"SERVER={config['server']};"
        f"DATABASE={config['database']};"
        f"UID={config['user']};"
        f"PWD={config['password']};"
    )

    # FreeTDSëŠ” Encrypt/TrustServerCertificate ì˜µì…˜ ë¶ˆí•„ìš”
    if driver_name != "FreeTDS":
        encrypt = config.get('encrypt', False)
        trust_cert = config.get('trust_certificate', True)
        conn_str += (
            f"Encrypt={'yes' if encrypt else 'no'};"
            f"TrustServerCertificate={'yes' if trust_cert else 'no'};"
        )

    logger.info(
        "MSSQL ì—°ê²° ì‹œë„ (Driver: %s): %s/%s",
        driver_name,
        config["server"],
        config["database"],
    )
    try:
        return pyodbc.connect(conn_str, timeout=10)
    except pyodbc.Error as exc:
        raise ConnectionError(f"MSSQL DB ì—°ê²° ì‹¤íŒ¨ (Driver: {driver_name}): {exc}") from exc


def _create_mssql_connection():
    """MSSQL ì„œë²„ ì—°ê²° ìƒì„± (ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)"""
    return _create_mssql_connection_with_config(None)

def _create_connection() -> pyodbc.Connection:
    """ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„± (í˜„ì¬ëŠ” MSSQLë§Œ ì§€ì›)"""
    if DB_TYPE != "MSSQL":
        raise RuntimeError(f"Unsupported DB_TYPE '{DB_TYPE}'. Only MSSQL is supported.")
    return _create_mssql_connection()

def _connect() -> pyodbc.Connection:
    """ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²° (ê¸°ì¡´ í˜¸í™˜ì„±)"""
    return _create_connection()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3) ì—°ê²° í’€ë§ í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class ConnectionPool:
    """ê°„ë‹¨í•œ ì—°ê²° í’€"""
    def __init__(self, max_connections=5):
        self.max_connections = max_connections
        self.connections = []
        self.lock = threading.Lock()
    
    @contextmanager
    def get_connection(self):
        """ì—°ê²° íšë“"""
        conn = None
        try:
            with self.lock:
                if self.connections:
                    conn = self.connections.pop()
                    # ì—°ê²° ìƒíƒœ í™•ì¸
                    try:
                        conn.execute("SELECT 1")  # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
                    except Exception:
                        # ì—°ê²°ì´ ëŠì–´ì¡Œìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                        try:
                            conn.close()
                        except Exception:
                            pass
                        conn = None
                
                if conn is None:
                    conn = _create_connection()  # ìƒˆ ì—°ê²° ìƒì„±
            yield conn
        except Exception:
            # ì—°ê²°ì— ë¬¸ì œê°€ ìˆìœ¼ë©´ ë‹«ê¸°
            if conn:
                try:
                    conn.close()
                except Exception:
                    pass
            raise
        finally:
            if conn:
                try:
                    with self.lock:
                        if len(self.connections) < self.max_connections:
                            self.connections.append(conn)
                        else:
                            conn.close()
                except Exception as e:
                    logger.warning(f"ì—°ê²° ë°˜í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                    try:
                        conn.close()
                    except Exception:
                        pass

# ì „ì—­ ì—°ê²° í’€
_connection_pool = ConnectionPool()


class CacheStats:
    """ë‹¨ìˆœ íˆíŠ¸/ë¯¸ìŠ¤ ì¹´ìš´í„°."""

    def __init__(self) -> None:
        self.hits = 0
        self.misses = 0
        self._lock = threading.RLock()

    def record_hit(self) -> None:
        with self._lock:
            self.hits += 1

    def record_miss(self) -> None:
        with self._lock:
            self.misses += 1

    def reset(self) -> None:
        with self._lock:
            self.hits = 0
            self.misses = 0

    def snapshot(self) -> Dict[str, Any]:
        with self._lock:
            total = self.hits + self.misses
            hit_rate = (self.hits / total) if total else 0.0
            return {
                "hits": self.hits,
                "misses": self.misses,
                "requests": total,
                "hit_rate": round(hit_rate, 4),
            }


_cache_lock = threading.RLock()
_item_master_cache_lock = threading.RLock()
_routing_cache_lock = threading.RLock()

_cache_versions: Dict[str, str] = {"item_master": "v0", "routing": "v0"}
_cache_counters: Dict[str, int] = {"item_master": 0, "routing": 0}
_cache_stats: Dict[str, CacheStats] = {
    "item_master": CacheStats(),
    "routing": CacheStats(),
}
_last_cache_fetch: Dict[str, Dict[str, Any]] = {"item_master": {}, "routing": {}}


def _snapshot_time() -> str:
    return utc_isoformat_z()


def _sanitize_columns(
    columns: Optional[Sequence[str]],
    allowed: Sequence[str],
) -> Tuple[str, ...]:
    allowed_lookup = {col.upper(): col for col in allowed}
    if columns:
        sanitized: List[str] = []
        unknown: List[str] = []
        for column in columns:
            key = str(column).strip().upper()
            if not key:
                continue
            actual = allowed_lookup.get(key)
            if actual:
                if actual not in sanitized:
                    sanitized.append(actual)
            else:
                unknown.append(column)
        if unknown:
            logger.warning("[DB] í—ˆìš©ë˜ì§€ ì•Šì€ ì»¬ëŸ¼ ë¬´ì‹œ: %s", ", ".join(map(str, unknown)))
        if sanitized:
            return tuple(sanitized)
    return tuple(allowed)


def _build_select_query(
    view_name: str,
    columns: Sequence[str],
    *,
    where_clause: Optional[str] = None,
    order_clause: Optional[str] = None,
) -> str:
    column_clause = ", ".join(columns)
    query_parts = [f"SELECT {column_clause} FROM {view_name}"]
    if where_clause:
        query_parts.append(where_clause)
    if order_clause:
        query_parts.append(order_clause)
    return " \n".join(query_parts)


def _record_cache_event(
    dataset: str,
    *,
    cached: bool,
    columns: Sequence[str],
    context: Optional[Dict[str, Any]] = None,
) -> None:
    stats = _cache_stats.get(dataset)
    if stats is not None:
        if cached:
            stats.record_hit()
        else:
            stats.record_miss()

    with _cache_lock:
        _last_cache_fetch[dataset] = {
            "cached": cached,
            "columns": list(columns),
            "context": dict(context or {}),
            "timestamp": _snapshot_time(),
            "version": _cache_versions.get(dataset),
        }


def _next_version(dataset: str, tag: Optional[str]) -> str:
    with _cache_lock:
        if tag is not None:
            token = str(tag)
        else:
            _cache_counters[dataset] += 1
            token = f"v{_cache_counters[dataset]}"
        _cache_versions[dataset] = token
        return token

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4) ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def set_debug_mode(enabled: bool = True):
    """ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •"""
    import logging
    level = logging.DEBUG if enabled else logging.INFO
    
    # ëª¨ë“  ê´€ë ¨ ë¡œê±°ì˜ ë ˆë²¨ ë³€ê²½
    loggers = ['database', 'predictor_ml_improved']
    for logger_name in loggers:
        try:
            target_logger = logging.getLogger(logger_name)
            target_logger.setLevel(level)
            for handler in target_logger.handlers:
                handler.setLevel(level)
        except Exception as e:
            logger.warning(f"ë¡œê±° ì„¤ì • ì‹¤íŒ¨ {logger_name}: {e}")
    
    print(f"ë””ë²„ê·¸ ëª¨ë“œ: {'ON' if enabled else 'OFF'}")

def validate_system_requirements():
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê²€ì¦"""
    issues = []
    
    # 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        __import__("pandas")
        __import__("numpy")
    except ImportError as e:
        issues.append(f"í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
    
    # 2. SQL Server ODBC ë“œë¼ì´ë²„ í™•ì¸
    try:
        drivers = pyodbc.drivers()
        sqlserver_drivers = [
            d for d in drivers if "SQL Server" in d or d == "FreeTDS"
        ]
        if not sqlserver_drivers:
            issues.append("MSSQL ODBC ë“œë¼ì´ë²„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì˜ˆ: ODBC Driver 17/18 for SQL Server)")
        else:
            logger.info("ë°œê²¬ëœ SQL Server ë“œë¼ì´ë²„: %s", sqlserver_drivers)
    except Exception as e:
        issues.append(f"ODBC ë“œë¼ì´ë²„ í™•ì¸ ì‹¤íŒ¨: {e}")

    # 3. MSSQL ì—°ê²° í™•ì¸
    try:
        with _create_mssql_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            cursor.fetchone()
    except Exception as e:
        issues.append(f"MSSQL ì—°ê²° ì‹¤íŒ¨: {e}")
    
    if issues:
        for issue in issues:
            logger.error(f"ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ë¬¸ì œ: {issue}")
        return False, issues
    else:
        logger.info("ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê²€ì¦ ì™„ë£Œ")
        return True, []

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5) ì¿¼ë¦¬ ì‹¤í–‰ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _prepare_params(params: Sequence[Any] | None) -> Tuple[Any, ...]:
    if params is None:
        return tuple()
    if isinstance(params, tuple):
        return params
    return tuple(params)


def _run_query_optimized(query: str, params: Sequence[Any] | None = None, retries: int = 3, delay: float = 2.0) -> pd.DataFrame:
    """ìµœì í™”ëœ ì¿¼ë¦¬ ì‹¤í–‰ (ì—°ê²° í’€ ì‚¬ìš©)"""
    for attempt in range(1, retries + 1):
        try:
            prepared_params = _prepare_params(params)
            if "?" in query and not prepared_params:
                raise ValueError("íŒŒë¼ë¯¸í„° í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆëŠ” ì¿¼ë¦¬ì—ëŠ” ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤")
            with _connection_pool.get_connection() as conn:
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore', message='pandas only supports SQLAlchemy connectable')
                    df = pd.read_sql(query, conn, params=list(prepared_params))

            if df.empty:
                logger.debug("[DB] ë¹ˆ ê²°ê³¼ì„¸íŠ¸ ë°˜í™˜ â€“ %s", query[:80])
            return df
            
        except Exception as exc:
            if attempt < retries:
                logger.warning(f"[DB] ì¿¼ë¦¬ ì¬ì‹œë„ {attempt}/{retries}: {exc}")
                time.sleep(delay)
                continue
            raise RuntimeError(f"[DB] ì¿¼ë¦¬ ì‹¤íŒ¨: {exc}") from exc

def _run_query(
    query: str,
    params: Sequence[Any] | None = None,
    retries: int = 3,
    delay: float = 2.0,
) -> pd.DataFrame:
    """ì¿¼ë¦¬ ì‹¤í–‰ â†’ DataFrame (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    for attempt in range(1, retries + 1):
        try:
            prepared_params = _prepare_params(params)
            if "?" in query and not prepared_params:
                raise ValueError("íŒŒë¼ë¯¸í„° í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆëŠ” ì¿¼ë¦¬ì—ëŠ” ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤")
            with _connect() as conn:
                # pandas ê²½ê³  ì–µì œ
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore',
                                          message='pandas only supports SQLAlchemy connectable')
                    df = pd.read_sql(query, conn, params=list(prepared_params))
            if df.empty:
                logger.warning("[DB] ë¹ˆ ê²°ê³¼ì„¸íŠ¸ ë°˜í™˜ â€“ %s", query[:80])
            return df
        except Exception as exc:
            if attempt < retries:
                logger.warning(f"[DB] ì¿¼ë¦¬ ì¬ì‹œë„ {attempt}/{retries}: {exc}")
                time.sleep(delay)
                continue
            raise RuntimeError(f"[DB] ì¿¼ë¦¬ ì‹¤íŒ¨: {exc}") from exc

def _load_item_master(columns_key: Tuple[str, ...]) -> pd.DataFrame:
    query = _build_select_query(VIEW_ITEM_MASTER, columns_key)
    return _run_query(query, ())


@lru_cache(maxsize=32)
def _fetch_item_master_cached(version: str, columns_key: Tuple[str, ...]) -> pd.DataFrame:
    return _load_item_master(columns_key)


def _load_single_item(item_cd_upper: str, columns_key: Tuple[str, ...]) -> pd.DataFrame:
    query = _build_select_query(
        VIEW_ITEM_MASTER,
        columns_key,
        where_clause="WHERE ITEM_CD = ?",
    )
    return _run_query(query, (item_cd_upper,))


@lru_cache(maxsize=512)
def _fetch_single_item_cached(
    version: str,
    item_cd_upper: str,
    columns_key: Tuple[str, ...],
) -> pd.DataFrame:
    return _load_single_item(item_cd_upper, columns_key)


def _load_routing_by_rout_no(
    item_cd_upper: str,
    rout_no: str,
    columns_key: Tuple[str, ...],
) -> pd.DataFrame:
    query = _build_select_query(
        VIEW_ROUTING,
        columns_key,
        where_clause="WHERE ITEM_CD = ? AND ROUT_NO = ?",
        order_clause="ORDER BY PROC_SEQ",
    )
    return _run_query(query, (item_cd_upper, rout_no))


def _load_all_routings(item_cd_upper: str, columns_key: Tuple[str, ...]) -> pd.DataFrame:
    query = _build_select_query(
        VIEW_ROUTING,
        columns_key,
        where_clause="WHERE ITEM_CD = ?",
        order_clause="ORDER BY ROUT_NO, PROC_SEQ",
    )
    return _run_query(query, (item_cd_upper,))


def _load_latest_routing(
    item_cd_upper: str,
    selection_mode: str,
    columns_key: Tuple[str, ...],
) -> pd.DataFrame:
    if selection_mode == "most_used":
        count_query = "\n".join(
            [
                "SELECT ROUT_NO, COUNT(*) as PROC_COUNT",
                f"FROM {VIEW_ROUTING}",
                "WHERE ITEM_CD = ?",
                "GROUP BY ROUT_NO",
                "ORDER BY COUNT(*) DESC, ROUT_NO DESC",
            ]
        )
        count_df = _run_query(count_query, (item_cd_upper,))
        if count_df.empty:
            logger.warning("[DB] ROUT_NOë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ â€“ ITEM_CD: %s", item_cd_upper)
            return pd.DataFrame()
        most_used_rout_no = count_df.iloc[0]["ROUT_NO"]
        logger.info(
            "[DB] ìµœë‹¤ ê³µì • ROUT_NO ì„ íƒ: %s (%sê°œ ê³µì •)",
            most_used_rout_no,
            count_df.iloc[0]["PROC_COUNT"],
        )
        return _load_routing_by_rout_no(item_cd_upper, most_used_rout_no, columns_key)

    all_routing_query = "\n".join(
        [
            "SELECT DISTINCT ROUT_NO, INSRT_DT",
            f"FROM {VIEW_ROUTING}",
            "WHERE ITEM_CD = ?",
            "ORDER BY INSRT_DT DESC, ROUT_NO DESC",
        ]
    )
    rout_df = _run_query(all_routing_query, (item_cd_upper,))
    if rout_df.empty:
        logger.warning("[DB] ROUT_NOë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ â€“ ITEM_CD: %s", item_cd_upper)
        return pd.DataFrame()

    if "INSRT_DT" in rout_df.columns:
        try:
            rout_df["INSRT_DT_SAFE"] = pd.to_datetime(rout_df["INSRT_DT"], errors="coerce")
            rout_df_sorted = rout_df.sort_values(
                ["INSRT_DT_SAFE", "ROUT_NO"],
                ascending=[False, False],
                na_position="last",
            )
            latest_rout_no = rout_df_sorted.iloc[0]["ROUT_NO"]
        except Exception as exc:
            logger.warning("[DB] INSRT_DT ì •ë ¬ ì‹¤íŒ¨, ROUT_NOë¡œë§Œ ì •ë ¬: %s", exc)
            latest_rout_no = rout_df.sort_values("ROUT_NO", ascending=False).iloc[0]["ROUT_NO"]
    else:
        latest_rout_no = rout_df.sort_values("ROUT_NO", ascending=False).iloc[0]["ROUT_NO"]

    logger.info("[DB] ìµœì‹  ROUT_NO ì„ íƒ: %s", latest_rout_no)
    return _load_routing_by_rout_no(item_cd_upper, latest_rout_no, columns_key)


@lru_cache(maxsize=512)
def _fetch_latest_routing_cached(
    version: str,
    item_cd_upper: str,
    selection_mode: str,
    columns_key: Tuple[str, ...],
) -> pd.DataFrame:
    return _load_latest_routing(item_cd_upper, selection_mode, columns_key)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6) ê³µê°œ API - ë‹¨ì¼ ì¡°íšŒ (í•„ìˆ˜ í•¨ìˆ˜ë“¤)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fetch_item_master(
    columns: Optional[List[str]] = None,
    *,
    use_cache: bool = True,
) -> pd.DataFrame:
    """ì „ì²´ í’ˆëª© ë§ˆìŠ¤í„° ì¡°íšŒ"""

    columns_key = _sanitize_columns(columns, ITEM_MASTER_VIEW_COLUMNS)
    logger.info("[DB] ITEM_MASTER ì¡°íšŒâ€¦")

    if use_cache:
        with _item_master_cache_lock:
            before = _fetch_item_master_cached.cache_info()
            df = _fetch_item_master_cached(
                _cache_versions["item_master"],
                columns_key,
            )
            after = _fetch_item_master_cached.cache_info()
            cached = after.hits > before.hits
    else:
        df = _load_item_master(columns_key)
        cached = False

    _record_cache_event(
        "item_master",
        cached=cached,
        columns=columns_key,
        context={"mode": "bulk"},
    )
    return df.copy()


def fetch_single_item(
    item_cd: str,
    columns: Optional[List[str]] = None,
    *,
    use_cache: bool = True,
) -> pd.DataFrame:
    """ë‹¨ì¼ í’ˆëª© ì •ë³´ ì¡°íšŒ - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ"""

    logger.debug("[DB] ITEM ë‹¨ê±´ ì¡°íšŒ: %s", item_cd)
    item_cd_upper = item_cd.upper().strip()
    columns_key = _sanitize_columns(columns, ITEM_MASTER_VIEW_COLUMNS)

    cached = False
    fallback_used = False
    result = pd.DataFrame()

    try:
        if use_cache:
            with _item_master_cache_lock:
                before = _fetch_single_item_cached.cache_info()
                df = _fetch_single_item_cached(
                    _cache_versions["item_master"],
                    item_cd_upper,
                    columns_key,
                )
                after = _fetch_single_item_cached.cache_info()
                cached = after.hits > before.hits
        else:
            df = _load_single_item(item_cd_upper, columns_key)

        if df.empty:
            fallback_used = True
            like_query = _build_select_query(
                VIEW_ITEM_MASTER,
                columns_key,
                where_clause="WHERE ITEM_CD LIKE ?",
            )
            df = _run_query(like_query, (f"%{item_cd_upper}%",))
            if not df.empty and len(df) == 1:
                logger.info("[DB] LIKE ê²€ìƒ‰ìœ¼ë¡œ í’ˆëª© ë°œê²¬: %s", df["ITEM_CD"].iloc[0])
            cached = False

        result = df.copy()
        if result.empty and _DEMO_MODE:
            demo_df = get_demo_item(item_cd_upper)
            if not demo_df.empty:
                logger.info("[DB] ë°ëª¨ ë°ì´í„°ë¡œ í’ˆëª© ì‘ë‹µ: %s", item_cd_upper)
                result = demo_df
                fallback_used = True

    except Exception as exc:
        logger.error("[DB] í’ˆëª© ì¡°íšŒ ì˜¤ë¥˜: %s", exc)
        result = pd.DataFrame()
        cached = False
        fallback_used = True
    finally:
        _record_cache_event(
            "item_master",
            cached=cached and not fallback_used,
            columns=columns_key,
            context={"mode": "single", "item_cd": item_cd_upper},
        )

    return result


def fetch_routing_for_item(
    item_cd: str,
    latest_only: bool = True,
    selection_mode: str = "latest",
    *,
    use_cache: bool = True,
) -> pd.DataFrame:
    """íŠ¹ì • í’ˆëª©ì˜ ë¼ìš°íŒ… ì •ë³´ ì¡°íšŒ - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ"""

    logger.debug(
        "[DB] ROUTING ì¡°íšŒ: %s (latest_only=%s, mode=%s)",
        item_cd,
        latest_only,
        selection_mode,
    )

    item_cd_upper = item_cd.upper().strip()
    columns_key = ROUTING_VIEW_COLUMNS
    cached = False
    result = pd.DataFrame()

    try:
        if latest_only:
            if use_cache:
                with _routing_cache_lock:
                    before = _fetch_latest_routing_cached.cache_info()
                    df = _fetch_latest_routing_cached(
                        _cache_versions["routing"],
                        item_cd_upper,
                        selection_mode,
                        columns_key,
                    )
                    after = _fetch_latest_routing_cached.cache_info()
                    cached = after.hits > before.hits
            else:
                df = _load_latest_routing(item_cd_upper, selection_mode, columns_key)

            if not df.empty:
                proc_seqs = df["PROC_SEQ"].unique()
                logger.info(
                    "[DB] ë¼ìš°íŒ… ì¡°íšŒ ì„±ê³µ: ROUT_NO=%s, %sê°œ ê³µì •",
                    df["ROUT_NO"].iloc[0],
                    len(df),
                )
                logger.debug("[DB] PROC_SEQ ëª©ë¡: %s", sorted(proc_seqs))

        else:
            df = _load_all_routings(item_cd_upper, columns_key)
            cached = False
            if df.empty:
                logger.warning("[DB] ë¹ˆ ê²°ê³¼ì„¸íŠ¸ ë°˜í™˜ â€“ ITEM_CD: %s", item_cd)
            else:
                rout_counts = df["ROUT_NO"].value_counts()
                logger.info(
                    "[DB] ë¼ìš°íŒ… ì¡°íšŒ ì„±ê³µ: ì´ %sê°œ ê³µì •, %sê°œ ROUT_NO",
                    len(df),
                    len(rout_counts),
                )
                for rout_no, count in rout_counts.items():
                    logger.debug("  - ROUT_NO: %s, ê³µì • ìˆ˜: %s", rout_no, count)

        result = df.copy()
        if result.empty and _DEMO_MODE:
            demo_df = get_demo_routing(item_cd_upper)
            if not demo_df.empty:
                logger.info("[DB] ë°ëª¨ ë°ì´í„° ë¼ìš°íŒ… ì‚¬ìš©: %s", item_cd_upper)
                result = demo_df
                cached = False

    except Exception as exc:
        logger.error("[DB] ë¼ìš°íŒ… ì¡°íšŒ ì˜¤ë¥˜: %s", exc)
        result = pd.DataFrame()
        cached = False

    finally:
        _record_cache_event(
            "routing",
            cached=cached,
            columns=columns_key,
            context={
                "item_cd": item_cd_upper,
                "latest_only": latest_only,
                "selection_mode": selection_mode,
            },
        )

    return result


def fetch_work_results_for_item(item_cd: str) -> pd.DataFrame:
    """ë‹¨ì¼ í’ˆëª©ì˜ ì‘ì—… ê²°ê³¼ ì¡°íšŒ"""

    item_cd_upper = item_cd.upper().strip()
    query = _build_select_query(
        VIEW_WORK_RESULT,
        WORK_RESULT_VIEW_COLUMNS,
        where_clause="WHERE ITEM_CD = ?",
    )
    logger.debug("[DB] WORK_RESULT ì¡°íšŒ: %s", item_cd_upper)
    return _run_query(query, (item_cd_upper,)).copy()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7) ğŸš€ ë°°ì¹˜ ì¿¼ë¦¬ API (ê³ ì† ì„±ëŠ¥ ìµœì í™”ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_items_batch(item_codes: List[str]) -> Dict[str, pd.DataFrame]:
    """ğŸš€ ì—¬ëŸ¬ í’ˆëª©ì˜ ë§ˆìŠ¤í„° ì •ë³´ë¥¼ í•œ ë²ˆì— ì¡°íšŒ - pandas ê²½ê³  ì–µì œ"""
    if not item_codes:
        return {}

    item_codes = [code.upper() for code in item_codes]
    columns_key = ITEM_MASTER_VIEW_COLUMNS

    batch_size = 100
    all_results = {}

    for i in range(0, len(item_codes), batch_size):
        batch_codes = item_codes[i:i + batch_size]
        placeholders = ','.join('?' * len(batch_codes))

        query = _build_select_query(
            VIEW_ITEM_MASTER,
            columns_key,
            where_clause=f"WHERE ITEM_CD IN ({placeholders})",
        )

        logger.info("[DB] ITEM_MASTER ë°°ì¹˜ ì¡°íšŒ: %sê°œ í’ˆëª© (ë°°ì¹˜ %s)",
                   len(batch_codes), i // batch_size + 1)

        try:
            df = _run_query_optimized(query, batch_codes)
                
            if not df.empty:
                for item_cd in batch_codes:
                    item_data = df[df['ITEM_CD'] == item_cd].copy()
                    all_results[item_cd] = item_data
            else:
                for item_cd in batch_codes:
                    all_results[item_cd] = pd.DataFrame()
                    
        except Exception as exc:
            logger.error("[DB] í’ˆëª© ë°°ì¹˜ ì¿¼ë¦¬ ì‹¤íŒ¨ (ë°°ì¹˜ %s): %s", i // batch_size + 1, exc)
            for item_cd in batch_codes:
                all_results[item_cd] = pd.DataFrame()
    
    found_count = len([k for k, v in all_results.items() if not v.empty])
    logger.info("[DB] í’ˆëª© ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: %sê°œ ìš”ì²­, %sê°œ ë°œê²¬", 
               len(item_codes), found_count)
    
    return all_results

def fetch_routings_for_items(item_codes: List[str], latest_only: bool = True) -> Dict[str, pd.DataFrame]:
    """ğŸš€ ì—¬ëŸ¬ í’ˆëª©ì˜ ë¼ìš°íŒ… ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ"""
    if not item_codes:
        return {}

    item_codes = [code.upper() for code in item_codes]
    columns_key = ROUTING_VIEW_COLUMNS

    if latest_only:
        # ê°œë³„ ì¡°íšŒ ë°©ì‹ (ìµœì‹  ROUT_NO ë¡œì§ì´ ë³µì¡í•˜ë¯€ë¡œ)
        all_results = {}

        for item_cd in item_codes:
            try:
                routing_df = fetch_routing_for_item(item_cd, latest_only=True)
                all_results[item_cd] = routing_df
            except Exception as e:
                logger.error(f"[DB] {item_cd} ë¼ìš°íŒ… ì¡°íšŒ ì‹¤íŒ¨: {e}")
                all_results[item_cd] = pd.DataFrame()
        
        found_count = len([k for k, v in all_results.items() if not v.empty])
        logger.info(f"[DB] ë¼ìš°íŒ… ê°œë³„ ì¡°íšŒ ì™„ë£Œ (latest_only): {len(item_codes)}ê°œ ìš”ì²­, {found_count}ê°œ ë°œê²¬")
        
        return all_results
    
    # ëª¨ë“  ë¼ìš°íŒ… ì¡°íšŒëŠ” ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 100
    all_results = {}
    
    for i in range(0, len(item_codes), batch_size):
        batch_codes = item_codes[i:i + batch_size]
        placeholders = ','.join('?' * len(batch_codes))

        query = _build_select_query(
            VIEW_ROUTING,
            columns_key,
            where_clause=f"WHERE ITEM_CD IN ({placeholders})",
            order_clause="ORDER BY ITEM_CD, ROUT_NO, PROC_SEQ",
        )

        logger.debug("[DB] ROUTING ë°°ì¹˜ ì¡°íšŒ: %sê°œ í’ˆëª© (ë°°ì¹˜ %s)",
                    len(batch_codes), i // batch_size + 1)

        try:
            df = _run_query_optimized(query, batch_codes)
                
            if not df.empty:
                for item_cd in batch_codes:
                    item_routing = df[df['ITEM_CD'] == item_cd].copy()
                    if not item_routing.empty:
                        rout_counts = item_routing['ROUT_NO'].value_counts()
                        if len(rout_counts) > 1:
                            logger.debug(f"[DB] {item_cd}: {len(rout_counts)}ê°œ ROUT_NO ë°œê²¬")
                    all_results[item_cd] = item_routing
            else:
                for item_cd in batch_codes:
                    all_results[item_cd] = pd.DataFrame()
                    
        except Exception as exc:
            logger.error("[DB] ë¼ìš°íŒ… ë°°ì¹˜ ì¿¼ë¦¬ ì‹¤íŒ¨ (ë°°ì¹˜ %s): %s", i // batch_size + 1, exc)
            for item_cd in batch_codes:
                all_results[item_cd] = pd.DataFrame()
    
    found_count = len([k for k, v in all_results.items() if not v.empty])
    logger.info("[DB] ë¼ìš°íŒ… ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: %sê°œ ìš”ì²­, %sê°œ ë°œê²¬", 
               len(item_codes), found_count)
    
    return all_results

def fetch_work_results_batch(item_codes: List[str]) -> Dict[str, pd.DataFrame]:
    """ğŸš€ ì—¬ëŸ¬ í’ˆëª©ì˜ ì‘ì—… ê²°ê³¼ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ"""
    if not item_codes:
        return {}

    item_codes = [code.upper() for code in item_codes]
    columns_key = WORK_RESULT_VIEW_COLUMNS

    # Access DBì˜ IN ì ˆ ì œí•œì„ ê³ ë ¤í•œ ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 100
    all_results = {}

    for i in range(0, len(item_codes), batch_size):
        batch_codes = item_codes[i:i + batch_size]
        placeholders = ','.join('?' * len(batch_codes))

        query = _build_select_query(
            VIEW_WORK_RESULT,
            columns_key,
            where_clause=f"WHERE ITEM_CD IN ({placeholders})",
        )
        
        logger.debug("[DB] WORK_RESULT ë°°ì¹˜ ì¡°íšŒ: %sê°œ í’ˆëª© (ë°°ì¹˜ %s)", 
                    len(batch_codes), i // batch_size + 1)
        
        try:
            df = _run_query_optimized(query, batch_codes)
                
            if not df.empty:
                # í’ˆëª©ë³„ë¡œ DataFrame ë¶„ë¦¬
                for item_cd in batch_codes:
                    item_results = df[df['ITEM_CD'] == item_cd].copy()
                    all_results[item_cd] = item_results
            else:
                # ë¹ˆ ê²°ê³¼ì¸ ê²½ìš°ì—ë„ í’ˆëª©ë³„ë¡œ ë¹ˆ DataFrame ìƒì„±
                for item_cd in batch_codes:
                    all_results[item_cd] = pd.DataFrame()
                    
        except Exception as exc:
            logger.error("[DB] ì‘ì—…ê²°ê³¼ ë°°ì¹˜ ì¿¼ë¦¬ ì‹¤íŒ¨ (ë°°ì¹˜ %s): %s", i // batch_size + 1, exc)
            # ì‹¤íŒ¨í•œ ë°°ì¹˜ì˜ í’ˆëª©ë“¤ì€ ë¹ˆ DataFrameìœ¼ë¡œ ì²˜ë¦¬
            for item_cd in batch_codes:
                all_results[item_cd] = pd.DataFrame()
    
    found_count = len([k for k, v in all_results.items() if not v.empty])
    logger.info("[DB] ì‘ì—…ê²°ê³¼ ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: %sê°œ ìš”ì²­, %sê°œ ë°œê²¬", 
               len(item_codes), found_count)
    
    return all_results

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 8) ğŸ’¡ ê³ ê¸‰ ë°°ì¹˜ ì¡°íšŒ (í•„ìš”ì‹œ ì‚¬ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fetch_all_data_batch(item_codes: List[str], latest_routing_only: bool = True) -> Dict[str, Dict[str, pd.DataFrame]]:
    """
    ğŸš€ ëª¨ë“  ê´€ë ¨ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì¡°íšŒ - ìµœê³  ì„±ëŠ¥ ìµœì í™”
    
    Args:
        item_codes: í’ˆëª© ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        latest_routing_only: Trueì¼ ê²½ìš° ê° í’ˆëª©ì˜ ìµœì‹  ë¼ìš°íŒ…ë§Œ í¬í•¨ (ê¸°ë³¸ê°’ True)
    
    Returns:
        Dict[str, Dict[str, pd.DataFrame]]: í’ˆëª©ë³„ ëª¨ë“  ë°ì´í„°
    """
    if not item_codes:
        return {}
    
    logger.info("[DB] ì „ì²´ ë°ì´í„° ë°°ì¹˜ ì¡°íšŒ ì‹œì‘: %sê°œ í’ˆëª©", len(item_codes))
    
    # ë³‘ë ¬ë¡œ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
    items_data = fetch_items_batch(item_codes)
    routings_data = fetch_routings_for_items(item_codes, latest_only=latest_routing_only)
    work_results_data = fetch_work_results_batch(item_codes)
    
    # í’ˆëª©ë³„ë¡œ ëª¨ë“  ë°ì´í„° í†µí•©
    result = {}
    for item_cd in item_codes:
        item_cd_upper = item_cd.upper()
        result[item_cd_upper] = {
            'item_master': items_data.get(item_cd_upper, pd.DataFrame()),
            'routing': routings_data.get(item_cd_upper, pd.DataFrame()),
            'work_results': work_results_data.get(item_cd_upper, pd.DataFrame())
        }
    
    logger.info("[DB] ì „ì²´ ë°ì´í„° ë°°ì¹˜ ì¡°íšŒ ì™„ë£Œ: %sê°œ í’ˆëª©", len(item_codes))
    return result

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 9) ğŸ”§ ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ìœ í‹¸ë¦¬í‹°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_connection() -> bool:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        with _connect() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT TOP 1 ITEM_CD FROM " + VIEW_ITEM_MASTER)
            result = cursor.fetchone()
            logger.info("[DB] ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: %s", result[0] if result else "ë°ì´í„° ì—†ìŒ")
            return True
    except Exception as exc:
        logger.error("[DB] ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: %s", exc)
        return False

def get_database_info() -> Dict[str, Any]:
    """ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ"""
    try:
        with _connect() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT @@SERVERNAME")
            server_name = cursor.fetchone()[0] if cursor.description else MSSQL_CONFIG["server"]

            cursor.execute("SELECT DB_NAME()")
            database_name = cursor.fetchone()[0] if cursor.description else MSSQL_CONFIG["database"]

            cursor.execute(
                """
                SELECT
                    CAST(SUM(size) * 8.0 / 1024 AS DECIMAL(18,2)) AS size_mb
                FROM sys.database_files
                """
            )
            size_row = cursor.fetchone()
            database_size_mb = float(size_row[0]) if size_row and size_row[0] is not None else 0.0

            tables_info: Dict[str, Any] = {}
            for view_name in [VIEW_ITEM_MASTER, VIEW_ROUTING, VIEW_WORK_RESULT]:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {view_name}")
                    count_row = cursor.fetchone()
                    tables_info[view_name] = int(count_row[0]) if count_row else 0
                except Exception as e:
                    tables_info[view_name] = f"ì¡°íšŒ ì‹¤íŒ¨: {e}"

            return {
                "connection_status": "ì •ìƒ",
                "server": server_name,
                "database": database_name,
                "database_size_mb": float(database_size_mb),
                "tables_info": tables_info,
            }
    except Exception as exc:
        logger.error("[DB] ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: %s", exc)
        return {
            "connection_status": f"ì˜¤ë¥˜: {exc}",
            "server": MSSQL_CONFIG["server"],
            "database": MSSQL_CONFIG["database"],
            "database_size_mb": 0.0,
            "tables_info": {},
        }


def _split_table_identifier(identifier: str) -> Tuple[str, str]:
    """`schema.table` í˜•ì‹ì˜ ì‹ë³„ìë¥¼ ë¶„ë¦¬í•œë‹¤."""
    if not identifier:
        return "dbo", ""

    token = identifier.strip().strip("[]")
    if "." in token:
        schema, table = token.split(".", 1)
    else:
        schema, table = "dbo", token

    return schema.strip("[]"), table.strip("[]")


def describe_table(table_identifier: str) -> List[Dict[str, Any]]:
    """INFORMATION_SCHEMA ê¸°ë°˜ìœ¼ë¡œ ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„°ë¥¼ ì¡°íšŒí•œë‹¤."""

    schema, table = _split_table_identifier(table_identifier)
    query = """
        SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?
        ORDER BY ORDINAL_POSITION
    """

    with _connect() as conn:
        cursor = conn.cursor()
        cursor.execute(query, schema, table)
        rows = cursor.fetchall()

    if not rows:
        raise ValueError(f"No metadata returned for table '{schema}.{table}'")

    columns: List[Dict[str, Any]] = []
    for row in rows:
        column_name = getattr(row, "COLUMN_NAME", None) or row[0]
        data_type = getattr(row, "DATA_TYPE", None) or row[1]
        nullable_raw = getattr(row, "IS_NULLABLE", None) or row[2]
        if isinstance(nullable_raw, str):
            nullable = nullable_raw.strip().upper() in {"YES", "Y", "TRUE", "1"}
        else:
            nullable = bool(nullable_raw)
        columns.append(
            {
                "name": str(column_name),
                "type": str(data_type),
                "nullable": nullable,
            }
        )
    return columns


def list_tables(*, schema: Optional[str] = None) -> List[str]:
    """MSSQL í…Œì´ë¸”/ë·° ëª©ë¡ì„ ë°˜í™˜í•œë‹¤."""

    query = """
        SELECT TABLE_SCHEMA, TABLE_NAME
        FROM INFORMATION_SCHEMA.TABLES
        WHERE TABLE_TYPE IN ('BASE TABLE', 'VIEW')
    """
    params: List[str] = []
    if schema:
        query += " AND TABLE_SCHEMA = ?"
        params.append(schema)
    query += " ORDER BY TABLE_SCHEMA, TABLE_NAME"

    with _connect() as conn:
        cursor = conn.cursor()
        cursor.execute(query, params)
        rows = cursor.fetchall()

    tables = [f"{getattr(row, 'TABLE_SCHEMA', row[0])}.{getattr(row, 'TABLE_NAME', row[1])}" for row in rows]
    return tables


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 10) ì¶”ê°€ í•¨ìˆ˜ë“¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def check_item_has_routing(item_cd: str) -> bool:
    """í’ˆëª©ì˜ ë¼ìš°íŒ… ì¡´ì¬ ì—¬ë¶€ í™•ì¸ - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ"""
    item_cd_upper = item_cd.upper().strip()
    
    query = f"""
        SELECT COUNT(*) as cnt
        FROM {VIEW_ROUTING}
        WHERE ITEM_CD = ?
    """
    
    try:
        result = _run_query(query, [item_cd_upper])

        return result['cnt'].iloc[0] > 0 if not result.empty else False

    except Exception as e:
        logger.error(f"[DB] ë¼ìš°íŒ… ì¡´ì¬ í™•ì¸ ì˜¤ë¥˜: {str(e)}")
        return has_demo_routing(item_cd_upper) if _DEMO_MODE else False

    if _DEMO_MODE and has_demo_routing(item_cd_upper):
        return True

def fetch_item_info_only(item_cd: str) -> pd.DataFrame:
    """
    í’ˆëª© ì •ë³´ë§Œ ì¡°íšŒ (ë¼ìš°íŒ… ì—†ì´)

    Args:
        item_cd: í’ˆëª© ì½”ë“œ

    Returns:
        pd.DataFrame: í’ˆëª© ì •ë³´
    """
    return fetch_single_item(item_cd, use_cache=True)

def fetch_routing_statistics(latest_only: bool = True) -> pd.DataFrame:
    """
    ë¼ìš°íŒ… í†µê³„ ì •ë³´ ì¡°íšŒ (í’ˆëª©ë³„ ê³µì • ìˆ˜, í‰ê·  ì‹œê°„ ë“±)
    
    Args:
        latest_only: Trueì¼ ê²½ìš° ê° í’ˆëª©ì˜ ìµœì‹  ROUT_NOë§Œ ì‚¬ìš© (ê¸°ë³¸ê°’ True)
    
    Returns:
        pd.DataFrame: í†µê³„ ì •ë³´
    """
    if latest_only:
        # ìµœì‹  ROUT_NOë§Œ ì‚¬ìš©í•œ í†µê³„
        query = f"""
        WITH LatestRouting AS (
            SELECT r1.*
            FROM {VIEW_ROUTING} r1
            INNER JOIN (
                SELECT ITEM_CD, MAX(INSRT_DT) as MAX_DT
                FROM {VIEW_ROUTING}
                GROUP BY ITEM_CD
            ) r2 ON r1.ITEM_CD = r2.ITEM_CD AND r1.INSRT_DT = r2.MAX_DT
        )
        SELECT 
            ITEM_CD,
            ROUT_NO,
            COUNT(DISTINCT PROC_SEQ) as PROCESS_COUNT,
            SUM(SETUP_TIME + RUN_TIME) as TOTAL_TIME,
            AVG(SETUP_TIME + RUN_TIME) as AVG_TIME_PER_PROCESS,
            MAX(PROC_SEQ) as MAX_PROC_SEQ
        FROM LatestRouting
        GROUP BY ITEM_CD, ROUT_NO
        """
    else:
        # ëª¨ë“  ROUT_NO í¬í•¨ í†µê³„
        query = f"""
        SELECT 
            ITEM_CD,
            COUNT(DISTINCT ROUT_NO) as ROUT_NO_COUNT,
            COUNT(DISTINCT PROC_SEQ) as PROCESS_COUNT,
            SUM(SETUP_TIME + RUN_TIME) as TOTAL_TIME,
            AVG(SETUP_TIME + RUN_TIME) as AVG_TIME_PER_PROCESS,
            MAX(PROC_SEQ) as MAX_PROC_SEQ
        FROM {VIEW_ROUTING}
        GROUP BY ITEM_CD
        """
    
    return _run_query(query)

def get_distinct_rout_nos(item_cd: str) -> List[str]:
    """
    íŠ¹ì • í’ˆëª©ì˜ ëª¨ë“  ROUT_NO ëª©ë¡ ì¡°íšŒ
    
    Args:
        item_cd: í’ˆëª© ì½”ë“œ
        
    Returns:
        List[str]: ROUT_NO ëª©ë¡ (ìµœì‹ ìˆœ)
    """
    query = f"""
        SELECT DISTINCT ROUT_NO, MAX(INSRT_DT) as LATEST_DT
        FROM {VIEW_ROUTING}
        WHERE ITEM_CD = ?
        GROUP BY ROUT_NO
        ORDER BY MAX(INSRT_DT) DESC
    """
    
    try:
        df = _run_query(query, [item_cd.upper()])
        return df['ROUT_NO'].tolist() if not df.empty else []
    except Exception as e:
        logger.error(f"[DB] ROUT_NO ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return []

def fetch_routing_by_rout_no(item_cd: str, rout_no: str) -> pd.DataFrame:
    """
    íŠ¹ì • ROUT_NOì˜ ë¼ìš°íŒ… ì •ë³´ ì¡°íšŒ

    Args:
        item_cd: í’ˆëª© ì½”ë“œ
        rout_no: ë¼ìš°íŒ… ë²ˆí˜¸

    Returns:
        pd.DataFrame: ë¼ìš°íŒ… ì •ë³´
    """
    logger.debug("[DB] íŠ¹ì • ROUT_NO ë¼ìš°íŒ… ì¡°íšŒ: %s, %s", item_cd, rout_no)
    return _load_routing_by_rout_no(
        item_cd.upper().strip(),
        rout_no,
        ROUTING_VIEW_COLUMNS,
    ).copy()


def fetch_purchase_order_items() -> pd.DataFrame:
    """
    ë°œì£¼ í’ˆëª© ëª©ë¡ ì¡°íšŒ (BI_PUR_PO_VIEW)

    Returns:
        pd.DataFrame: ë°œì£¼ í’ˆëª© ëª©ë¡ (ITEM_CD, PO_NO, PO_DATE, QTY ë“±)
    """
    if VIEW_PURCHASE_ORDER is None:
        logger.warning("[DB] BI_PUR_PO_VIEWëŠ” MSSQL ëª¨ë“œì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤ - ë°ëª¨ ë°ì´í„° ë°˜í™˜")
        # ë°ëª¨ ëª¨ë“œ: ìƒì‚° ì ‘ìˆ˜ëœ í’ˆëª© ìƒ˜í”Œ ë°˜í™˜
        from datetime import datetime, timedelta
        demo_items = []
        base_date = datetime.now() - timedelta(days=7)

        for i, item_cd in enumerate(['ITEM-001', 'ITEM-002', 'ITEM-003', 'ITEM-004', 'ITEM-005']):
            demo_items.append({
                'ITEM_CD': item_cd,
                'PO_NO': f'PO-2024-{1000+i:04d}',
                'PO_DATE': (base_date + timedelta(days=i)).strftime('%Y-%m-%d'),
                'QTY': (i + 1) * 100,
                'UNIT_PRICE': 50000 + (i * 10000),
                'VENDOR_CD': f'V{i+1:03d}',
                'VENDOR_NM': f'ê³µê¸‰ì—…ì²´{i+1}'
            })

        return pd.DataFrame(demo_items)

    query = f"""
        SELECT DISTINCT
            ITEM_CD,
            PO_NO,
            PO_DATE,
            QTY,
            UNIT_PRICE,
            VENDOR_CD,
            VENDOR_NM
        FROM {VIEW_PURCHASE_ORDER}
        WHERE ITEM_CD IS NOT NULL
        ORDER BY PO_DATE DESC
    """

    try:
        logger.info("[DB] ë°œì£¼ í’ˆëª© ëª©ë¡ ì¡°íšŒ ì¤‘...")
        return _run_query(query, ())
    except Exception as e:
        logger.error(f"[DB] ë°œì£¼ í’ˆëª© ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame()


def fetch_item_with_purchase_info(item_cd: str) -> pd.DataFrame:
    """
    í’ˆëª© ì •ë³´ + ë°œì£¼ ì •ë³´ ì¡°íšŒ

    Args:
        item_cd: í’ˆëª© ì½”ë“œ

    Returns:
        pd.DataFrame: í’ˆëª© ì •ë³´ì™€ ë°œì£¼ ì •ë³´ê°€ ê²°í•©ëœ ë°ì´í„°
    """
    item_cd_upper = item_cd.upper().strip()

    # í’ˆëª© ê¸°ë³¸ ì •ë³´
    item_info = fetch_single_item(item_cd_upper)

    if VIEW_PURCHASE_ORDER is None or item_info.empty:
        return item_info

    # ë°œì£¼ ì •ë³´
    query = f"""
        SELECT
            PO_NO,
            PO_DATE,
            QTY,
            UNIT_PRICE,
            VENDOR_CD,
            VENDOR_NM
        FROM {VIEW_PURCHASE_ORDER}
        WHERE ITEM_CD = ?
        ORDER BY PO_DATE DESC
    """

    try:
        po_info = _run_query(query, (item_cd_upper,))
        if not po_info.empty:
            # ê°€ì¥ ìµœê·¼ ë°œì£¼ ì •ë³´ë¥¼ í’ˆëª© ì •ë³´ì— ì¶”ê°€
            latest_po = po_info.iloc[0]
            for col in po_info.columns:
                item_info[f'LATEST_{col}'] = latest_po[col]
            item_info['PO_COUNT'] = len(po_info)
    except Exception as e:
        logger.warning(f"[DB] ë°œì£¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    return item_info


def invalidate_item_master_cache(tag: Optional[str] = None) -> str:
    """í’ˆëª© ë§ˆìŠ¤í„° ìºì‹œ ë¬´íš¨í™” ë° ë²„ì „ ê°±ì‹ ."""

    new_version = _next_version("item_master", tag)
    _fetch_item_master_cached.cache_clear()
    _fetch_single_item_cached.cache_clear()
    _cache_stats["item_master"].reset()
    with _cache_lock:
        _last_cache_fetch["item_master"] = {
            "cached": False,
            "columns": list(ITEM_MASTER_VIEW_COLUMNS),
            "context": {"action": "invalidate"},
            "timestamp": _snapshot_time(),
            "version": new_version,
        }
    return new_version


def invalidate_routing_cache(tag: Optional[str] = None) -> str:
    """ë¼ìš°íŒ… ìºì‹œ ë¬´íš¨í™” ë° ë²„ì „ ê°±ì‹ ."""

    new_version = _next_version("routing", tag)
    _fetch_latest_routing_cached.cache_clear()
    _cache_stats["routing"].reset()
    with _cache_lock:
        _last_cache_fetch["routing"] = {
            "cached": False,
            "columns": list(ROUTING_VIEW_COLUMNS),
            "context": {"action": "invalidate"},
            "timestamp": _snapshot_time(),
            "version": new_version,
        }
    return new_version


def get_cache_versions() -> Dict[str, str]:
    """í˜„ì¬ ìºì‹œ ë²„ì „ íƒœê·¸ ìŠ¤ëƒ…ìƒ·."""

    with _cache_lock:
        return dict(_cache_versions)


def get_cache_snapshot() -> Dict[str, Any]:
    """ìºì‹œ ì ì¤‘ë¥  ë° ìµœê·¼ ì¡°íšŒ ë©”íƒ€ë°ì´í„° ìŠ¤ëƒ…ìƒ·."""

    snapshot: Dict[str, Any] = {}
    with _cache_lock:
        for dataset in ("item_master", "routing"):
            stats = _cache_stats[dataset].snapshot()
            last_fetch = dict(_last_cache_fetch.get(dataset) or {})
            snapshot[dataset] = {
                "version": _cache_versions.get(dataset),
                "stats": stats,
                "last_fetch": last_fetch or None,
            }
    return snapshot

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 11) ì •ë¦¬ í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cleanup_connections():
    """ì—°ê²° í’€ ì •ë¦¬"""
    try:
        with _connection_pool.lock:
            for conn in _connection_pool.connections:
                try:
                    conn.close()
                except Exception:
                    pass
            _connection_pool.connections.clear()
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì •ë¦¬ ì™„ë£Œ")
    except Exception as e:
        logger.warning(f"ì—°ê²° í’€ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

# ëª¨ë“ˆ ì¢…ë£Œ ì‹œ ìë™ ì •ë¦¬
atexit.register(cleanup_connections)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 12) FastAPI ì˜ì¡´ì„± í•¨ìˆ˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_db_connection():
    """
    FastAPI ì˜ì¡´ì„±ìœ¼ë¡œ ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì œê³µ (pyodbc).

    Yields:
        pyodbc.Connection: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°

    Example:
        @app.get("/items")
        def get_items(conn = Depends(get_db_connection)):
            df = pd.read_sql("SELECT * FROM ITEM_MASTER", conn)
            return df.to_dict('records')
    """
    with _connection_pool.get_connection() as conn:
        yield conn


def get_session():
    """
    FastAPI ì˜ì¡´ì„±ìœ¼ë¡œ ì‚¬ìš©í•  SQLAlchemy Session ì œê³µ.

    RSL ë°ì´í„°ë² ì´ìŠ¤ì˜ session factoryë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

    Yields:
        sqlalchemy.orm.Session: SQLAlchemy ì„¸ì…˜

    Example:
        @app.get("/data-quality/metrics")
        async def get_metrics(session: Session = Depends(get_session)):
            return await data_quality_service.get_metrics(session)
    """
    from backend.database_rsl import get_session_factory

    session_factory = get_session_factory()
    session = session_factory()
    try:
        yield session
    finally:
        session.close()
