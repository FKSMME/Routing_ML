# User Guide: Routing ML Iterative Training System

**Document ID**: USER_GUIDE_2025-10-22_routing-ml-iterative
**Version**: 1.0
**Created**: 2025-10-22
**Audience**: End Users, Data Analysts, ML Engineers

**Related Documents**:
- PRD: [docs/planning/PRD_2025-10-22_routing-ml-iterative-training.md](../docs/planning/PRD_2025-10-22_routing-ml-iterative-training.md)
- Operator Manual: [deliverables/OPERATOR_MANUAL_2025-10-22_routing-ml-iterative.md](OPERATOR_MANUAL_2025-10-22_routing-ml-iterative.md)
- QA Report: [deliverables/QA_REPORT_2025-10-22_routing-ml-iterative.md](QA_REPORT_2025-10-22_routing-ml-iterative.md)

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [Getting Started](#2-getting-started)
3. [Quality Monitoring Dashboard](#3-quality-monitoring-dashboard)
4. [Training Monitor](#4-training-monitor)
5. [Settings Configuration](#5-settings-configuration)
6. [Log Viewer](#6-log-viewer)
7. [Interpreting Metrics](#7-interpreting-metrics)
8. [Common Workflows](#8-common-workflows)
9. [Troubleshooting](#9-troubleshooting)
10. [FAQ](#10-faq)

---

## 1. Introduction

### 1.1 What is the Iterative Training System?

The Routing ML Iterative Training System is an automated quality monitoring and model retraining platform that continuously evaluates prediction accuracy and automatically improves the machine learning model based on real-world production data.

**Key Benefits**:
- **Automated Quality Monitoring**: Evaluates prediction accuracy every cycle
- **Proactive Model Improvement**: Automatically retrains when quality degrades
- **Transparent Metrics**: Real-time dashboards show prediction performance
- **Zero Downtime**: Model updates happen seamlessly without service interruption

### 1.2 Who Should Use This Guide?

- **Data Analysts**: Monitor quality metrics and identify trends
- **ML Engineers**: Trigger manual retraining and adjust thresholds
- **Operations Team**: Troubleshoot issues and manage deployments
- **Business Users**: Understand prediction confidence and quality

### 1.3 System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Quality Monitor â”‚ â† Samples items, predicts, evaluates
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Quality degrades?
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Queue  â”‚ â† Queues retraining jobs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Job starts
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Training Worker â”‚ â† Trains new model
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Model ready?
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Deployer  â”‚ â† Deploys new model (with backup)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Getting Started

### 2.1 Accessing the System

**URL**: `http(s)://[server-address]/`

**Login**:
1. Navigate to the system URL
2. Enter your credentials (username/password)
3. Click "ë¡œê·¸ì¸" (Login)

**First-Time Setup**:
- Default settings are pre-configured
- No configuration required for basic usage
- Advanced users can adjust settings (see Section 5)

### 2.2 Navigation

**Main Menu** (left sidebar or top navigation):

| Menu Item | Icon | Description |
|-----------|------|-------------|
| **í’ˆì§ˆ ëª¨ë‹ˆí„°** (Quality Monitor) | ğŸ“Š | View quality metrics and trends |
| **í•™ìŠµ ëª¨ë‹ˆí„°** (Training Monitor) | ğŸ¯ | Monitor training jobs and history |
| **í•™ìŠµ ì„¤ì •** (Training Settings) | âš™ï¸ | Configure thresholds and parameters |
| **ë¡œê·¸ ë·°ì–´** (Log Viewer) | ğŸ“„ | View real-time system logs |

### 2.3 Dashboard Overview

**Home Screen** shows:
- Current quality metrics (MAE, Process Match, etc.)
- Recent training jobs
- System status (Active/Idle)
- Quick actions (Start Training, View Logs)

---

## 3. Quality Monitoring Dashboard

### 3.1 Accessing the Dashboard

**Navigation**: ë©”ì¸ ë©”ë‰´ â†’ **í’ˆì§ˆ ëª¨ë‹ˆí„°** (Quality Monitor)

**URL**: `/#/quality-monitor`

### 3.2 Current Metrics Cards

The dashboard displays 6 key metric cards:

#### 3.2.1 MAE (Mean Absolute Error)

**Definition**: Average prediction error in minutes

**Card Display**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MAE                 â”‚
â”‚ 4.2 ë¶„ (minutes)    â”‚
â”‚ â–¼ -0.5 (improving)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Interpretation**:
- **<5 minutes**: âœ… Excellent (green)
- **5-10 minutes**: âš ï¸ Acceptable (yellow)
- **>10 minutes**: âŒ Poor (red)

**Trend Indicator**:
- â–¼ (down arrow): Quality improving
- â–² (up arrow): Quality degrading

#### 3.2.2 Trim-MAE (Trimmed Mean Absolute Error)

**Definition**: MAE after removing top/bottom 10% outliers

**Why It Matters**: More robust to extreme outliers than regular MAE

**Interpretation**:
- Trim-MAE < MAE: Outliers inflating error (normal)
- Trim-MAE â‰ˆ MAE: Error distributed evenly
- Trim-MAE > MAE: Unusual (investigate)

#### 3.2.3 RMSE (Root Mean Squared Error)

**Definition**: Square root of mean squared errors (penalizes large errors)

**Interpretation**:
- RMSE â‰ˆ MAE: Errors evenly distributed
- RMSE >> MAE: Many large outliers (investigate)

#### 3.2.4 Process Match (%)

**Definition**: Percentage of predictions where process matches WORK_ORDER result

**Card Display**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Process Match       â”‚
â”‚ 87.3%               â”‚
â”‚ â–² +2.1% (improving) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Interpretation**:
- **>90%**: âœ… Excellent
- **80-90%**: âš ï¸ Acceptable
- **<80%**: âŒ Poor (investigate)

**Common Causes of Low Match**:
- Model trained on outdated data
- Process changes not reflected in training data
- Data quality issues (missing/incorrect work orders)

#### 3.2.5 Outsourcing Success (%)

**Definition**: Percentage of outsourcing decisions that were successful

**Interpretation**:
- **>95%**: âœ… Excellent
- **85-95%**: âš ï¸ Acceptable
- **<85%**: âŒ Poor

#### 3.2.6 CV (Coefficient of Variation)

**Definition**: Standard deviation / Mean (variability measure)

**Interpretation**:
- **<0.3**: âœ… Consistent predictions
- **0.3-0.5**: âš ï¸ Moderate variability
- **>0.5**: âŒ High variability (unreliable)

**Example**:
- Mean prediction: 10 minutes
- StdDev: 2 minutes
- CV = 2 / 10 = 0.2 (âœ… Consistent)

### 3.3 MAE Trend Chart

**Chart Type**: Line chart with 3 series

**X-Axis**: Evaluation cycle date/time
**Y-Axis**: Error in minutes

**Series**:
1. **MAE** (blue line): Mean Absolute Error
2. **Trim-MAE** (green line): Trimmed MAE
3. **RMSE** (red line): Root Mean Squared Error

**Reading the Chart**:
- Downward trend: Quality improving âœ…
- Flat trend: Quality stable
- Upward trend: Quality degrading âš ï¸
- Sudden spike: Investigate outliers

**Example Interpretation**:
```
MAE (minutes)
12 â”‚        â•±â•²
10 â”‚       â•±  â•²
 8 â”‚      â•±    â•²___
 6 â”‚_____â•±         â•²___
 4 â”‚                   â•²___
 2 â”‚                        â•²___
 0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
   Day 1  Day 3  Day 5  Day 7  Time

Analysis:
- Days 1-3: Stable quality (~6 min MAE)
- Days 3-4: Quality spike (model drift?)
- Days 5-7: Quality improving after retraining
```

### 3.4 Recent Alerts Table

**Table Columns**:

| Column | Description | Example |
|--------|-------------|---------|
| **Severity** | Alert level | ğŸ”´ Critical, ğŸŸ¡ Warning, ğŸ”µ Info |
| **Message** | Alert description | "MAE exceeds threshold (5.0 minutes)" |
| **Value** | Metric value | "8.2 minutes" |
| **Timestamp** | When alert triggered | "2025-10-22 14:30:15" |

**Alert Severity**:
- **ğŸ”´ Critical**: Immediate action required (MAE >10 min)
- **ğŸŸ¡ Warning**: Monitor closely (MAE 5-10 min, CV >0.5)
- **ğŸ”µ Info**: FYI only (model retrained, queue full)

**Example Alerts**:
```
ğŸ”´ Critical â”‚ MAE exceeds critical threshold      â”‚ 12.5 min â”‚ 2025-10-22 14:30:15
ğŸŸ¡ Warning  â”‚ CV exceeds threshold (0.3)          â”‚ 0.42     â”‚ 2025-10-22 14:15:10
ğŸ”µ Info     â”‚ Model retrained successfully        â”‚ v1.2.3   â”‚ 2025-10-22 13:00:00
```

### 3.5 Date Range Filters

**Filter Options**:

1. **Cycle Limit** (cycles ìˆ˜ ì œí•œ)
   - Display last N evaluation cycles
   - Default: 30 cycles
   - Range: 1-100 cycles

2. **Start Date** (ì‹œì‘ ë‚ ì§œ)
   - Filter cycles after this date
   - Example: 2025-10-15

3. **End Date** (ì¢…ë£Œ ë‚ ì§œ)
   - Filter cycles before this date
   - Example: 2025-10-22

**Usage**:
```
1. Set filters:
   - Cycle Limit: 50
   - Start Date: 2025-10-01
   - End Date: 2025-10-22

2. Click "í•„í„° ì ìš©" (Apply Filters)

3. Chart and table update automatically
```

### 3.6 Export Functionality

**Export Options**:

1. **Export to JSON**
   - Full quality history data
   - All metrics, alerts, metadata
   - Filename: `quality_history_{timestamp}.json`

2. **Export to CSV**
   - Cycle summary table
   - Metrics only (no nested data)
   - Filename: `quality_summary_{timestamp}.csv`

**Usage**:
1. Configure date filters (optional)
2. Click **"JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"** or **"CSVë¡œ ë‚´ë³´ë‚´ê¸°"**
3. Download starts automatically
4. Open file in Excel, Python, or analysis tool

**CSV Example**:
```csv
cycle_id,timestamp,mae,trim_mae,rmse,process_match,cv
cycle_001,2025-10-22T10:00:00,4.2,3.8,5.1,87.3,0.28
cycle_002,2025-10-22T14:00:00,5.5,4.9,6.8,85.1,0.35
```

---

## 4. Training Monitor

### 4.1 Accessing Training Monitor

**Navigation**: ë©”ì¸ ë©”ë‰´ â†’ **í•™ìŠµ ëª¨ë‹ˆí„°** (Training Monitor)

**URL**: `/#/training-monitor`

### 4.2 Starting a Training Job

**Button**: **"í•™ìŠµ ì‹œì‘"** (Start Training)

**Steps**:
1. Click "í•™ìŠµ ì‹œì‘" button
2. Configure parameters (modal dialog):
   - **Sample Size** (ìƒ˜í”Œ í¬ê¸°): 100-10,000 items (default: 500)
   - **Sampling Strategy** (ìƒ˜í”Œë§ ì „ëµ):
     - `random`: Random sampling (default)
     - `stratified`: Stratified by PART_TYPE/ITEM_TYPE
     - `recent_bias`: Bias towards recent items
3. Click "í™•ì¸" (Confirm)
4. Training job starts immediately

**Response**:
- Success: "í•™ìŠµ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. Job ID: job_20251022_143015"
- Failure: Error message (e.g., "íê°€ ê°€ë“ ì°¼ìŠµë‹ˆë‹¤")

### 4.3 Monitoring Training Progress

**Progress Bar**:
```
Progress: 45% [â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘]

Current Step: Training MLP model...
Elapsed Time: 2 min 15 sec
Estimated Remaining: 2 min 45 sec
```

**Status Indicators**:
- ğŸŸ¢ **RUNNING**: Training in progress
- ğŸ”µ **SUCCEEDED**: Training completed successfully
- ğŸ”´ **FAILED**: Training failed (see error message)
- â¸ï¸ **CANCELLED**: User cancelled training
- â­ï¸ **SKIPPED**: Job skipped (cooldown period)

### 4.4 Real-Time Logs

**Log Viewer** (scrollable box):
```
[2025-10-22 14:30:15 INFO] Starting training job: job_20251022_143015
[2025-10-22 14:30:17 INFO] Sampling 500 items with strategy: random
[2025-10-22 14:30:25 INFO] Sample complete: 500 items retrieved
[2025-10-22 14:30:30 INFO] Training baseline model (HNSW)...
[2025-10-22 14:31:45 INFO] Baseline MAE: 6.2 minutes
[2025-10-22 14:31:50 INFO] Training MLP model...
[2025-10-22 14:33:10 INFO] MLP MAE: 4.8 minutes (âœ… 22% improvement)
[2025-10-22 14:33:15 INFO] Deploying model version v1.2.3...
[2025-10-22 14:33:20 INFO] âœ… Training complete! New model deployed.
```

**Auto-Scroll**:
- Enabled by default (scrolls to latest log)
- Toggle with checkbox: â˜‘ "ìë™ ìŠ¤í¬ë¡¤" (Auto-scroll)

### 4.5 Cancelling a Training Job

**Steps**:
1. Locate running job in progress bar
2. Click **"ì·¨ì†Œ"** (Cancel) button
3. Confirm cancellation (popup)
4. Job status changes to **CANCELLED**

**Note**: Cancellation may take up to 30 seconds (worker cleanup)

### 4.6 Training History Table

**Table Columns**:

| Column | Description | Example |
|--------|-------------|---------|
| **Job ID** | Unique job identifier | job_20251022_143015 |
| **Status** | Current status | ğŸ”µ SUCCEEDED |
| **Progress** | Completion % | 100% |
| **Current Step** | Last step executed | Model deployed |
| **Started At** | Job start time | 2025-10-22 14:30:15 |
| **Completed At** | Job end time | 2025-10-22 14:33:20 |

**Sorting**:
- Default: Most recent jobs first
- Click column header to sort

**Filtering**:
- Filter by status: ALL / SUCCEEDED / FAILED / RUNNING
- Date range filter (optional)

---

## 5. Settings Configuration

### 5.1 Accessing Settings

**Navigation**: ë©”ì¸ ë©”ë‰´ â†’ **í•™ìŠµ ì„¤ì •** (Training Settings)

**URL**: `/#/training-settings`

### 5.2 Configuration Parameters

#### 5.2.1 Sample Size (ìƒ˜í”Œ í¬ê¸°)

**Purpose**: Number of items to sample for quality evaluation

**Range**: 10 - 10,000 items
**Default**: 500 items

**Guidance**:
- **Small datasets (<10,000 items)**: Use 100-500 samples
- **Medium datasets (10,000-100,000 items)**: Use 500-1,000 samples
- **Large datasets (>100,000 items)**: Use 1,000-5,000 samples

**Trade-offs**:
- **Larger sample**: More accurate metrics, slower evaluation
- **Smaller sample**: Faster evaluation, less accurate metrics

#### 5.2.2 MAE Threshold (MAE ì„ê³„ê°’)

**Purpose**: Retraining trigger threshold for MAE

**Range**: 0.1 - 100.0 minutes
**Default**: 5.0 minutes

**Guidance**:
- **Tight SLA (Â±3 min)**: Set threshold to 3.0-5.0 minutes
- **Moderate SLA (Â±5-10 min)**: Set threshold to 5.0-10.0 minutes
- **Loose SLA (>Â±10 min)**: Set threshold to 10.0-20.0 minutes

**Example**:
```
Current MAE: 8.2 minutes
Threshold: 5.0 minutes
Result: âš ï¸ Retraining triggered (MAE exceeds threshold)
```

#### 5.2.3 CV Threshold (CV ì„ê³„ê°’)

**Purpose**: Retraining trigger threshold for Coefficient of Variation

**Range**: 0.01 - 1.0
**Default**: 0.3

**Guidance**:
- **Consistent predictions required**: Set to 0.2-0.3
- **Moderate variability acceptable**: Set to 0.3-0.5
- **High variability tolerated**: Set to 0.5-1.0

**Example**:
```
Current CV: 0.45
Threshold: 0.3
Result: âš ï¸ Retraining triggered (high variability)
```

#### 5.2.4 Queue Max Size (í ìµœëŒ€ í¬ê¸°)

**Purpose**: Maximum number of concurrent training jobs in queue

**Range**: 1 - 20 jobs
**Default**: 3 jobs

**Guidance**:
- **Single-user environment**: 1-3 jobs
- **Multi-user environment**: 3-5 jobs
- **High-traffic environment**: 5-10 jobs

**Trade-offs**:
- **Larger queue**: More concurrent jobs, higher resource usage
- **Smaller queue**: Fewer concurrent jobs, lower resource usage

#### 5.2.5 Polling Interval (í´ë§ ê°„ê²©)

**Purpose**: Auto-refresh interval for training status (seconds)

**Range**: 1 - 60 seconds
**Default**: 5 seconds

**Guidance**:
- **Real-time monitoring**: 1-5 seconds
- **Casual monitoring**: 5-10 seconds
- **Low-priority monitoring**: 10-30 seconds

### 5.3 Saving Settings

**Steps**:
1. Modify parameters as needed
2. Click **"ì €ì¥"** (Save) button
3. Success message: "ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤" (Settings saved)
4. Settings persist in browser localStorage

**Validation**:
- Form validates inputs before saving
- Invalid values show error messages:
  - "Sample size must be between 10 and 10,000"
  - "MAE threshold must be between 0.1 and 100"

### 5.4 Resetting to Defaults

**Steps**:
1. Click **"ê¸°ë³¸ê°’ìœ¼ë¡œ ì¬ì„¤ì •"** (Reset to Defaults) button
2. Confirm reset (popup)
3. All parameters reset to default values
4. Click "ì €ì¥" to persist changes

---

## 6. Log Viewer

### 6.1 Accessing Log Viewer

**Navigation**: ë©”ì¸ ë©”ë‰´ â†’ **ë¡œê·¸ ë·°ì–´** (Log Viewer)

**URL**: `/#/log-viewer`

### 6.2 Log Display

**Display Format**:
```
[2025-10-22 14:30:15] INFO: Quality evaluation started (cycle_123)
[2025-10-22 14:30:17] DEBUG: Sampled 500 items from BI_ITEM_INFO_VIEW
[2025-10-22 14:30:20] WARNING: Prediction timeout for 2 items (retrying...)
[2025-10-22 14:30:25] INFO: Evaluation complete: MAE = 4.2 minutes
[2025-10-22 14:30:30] ERROR: Failed to save metrics to database (retrying...)
```

**Log Levels** (color-coded):
- **ERROR** (ğŸ”´ red): Critical errors requiring attention
- **WARNING** (ğŸŸ¡ yellow): Warnings or potential issues
- **INFO** (ğŸ”µ blue): Informational messages
- **DEBUG** (âšª gray): Detailed debugging information

### 6.3 Auto-Refresh

**Default**: Enabled (5-second interval)

**Controls**:
- **Pause** (ì¼ì‹œì •ì§€): Stop auto-refresh
- **Resume** (ì¬ê°œ): Resume auto-refresh

**Usage**:
1. Auto-refresh runs by default
2. New logs appear automatically every 5 seconds
3. Click "Pause" to freeze logs (for reading)
4. Click "Resume" to resume auto-refresh

### 6.4 Auto-Scroll

**Default**: Enabled (scrolls to bottom)

**Toggle**: â˜‘ "ìë™ ìŠ¤í¬ë¡¤" (Auto-scroll)

**Usage**:
- **Enabled**: Latest logs always visible (auto-scrolls)
- **Disabled**: Scroll position fixed (manual scrolling)

### 6.5 Download Logs

**Steps**:
1. Click **"ì „ì²´ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ"** (Download Full Log) button
2. File downloads: `quality_log_{timestamp}.txt`
3. Open in text editor or log viewer

**File Format**: Plain text (.txt)

**Example Content**:
```
=== Quality Evaluation Logs ===
Downloaded: 2025-10-22 14:45:00

[2025-10-22 10:00:00] INFO: System started
[2025-10-22 10:05:15] INFO: Quality evaluation started
...
[2025-10-22 14:40:00] INFO: Training job completed
```

---

## 7. Interpreting Metrics

### 7.1 MAE (Mean Absolute Error)

**Formula**: `MAE = Î£|predicted - actual| / N`

**Example Calculation**:
```
Item 1: Predicted = 10 min, Actual = 12 min â†’ Error = 2 min
Item 2: Predicted = 15 min, Actual = 10 min â†’ Error = 5 min
Item 3: Predicted = 8 min,  Actual = 9 min  â†’ Error = 1 min

MAE = (2 + 5 + 1) / 3 = 2.67 minutes
```

**Business Impact**:
- MAE = 2.67 min â†’ Predictions off by ~3 minutes on average
- Lower MAE = more accurate predictions = better planning

### 7.2 Process Match

**Formula**: `Process Match = (Correct Processes / Total Items) Ã— 100%`

**Example Calculation**:
```
Total Items: 100
Predicted: LASER â†’ Actual: LASER (âœ… match)
Predicted: BEND  â†’ Actual: LASER (âŒ mismatch)
...
Correct: 87 items
Incorrect: 13 items

Process Match = 87 / 100 Ã— 100% = 87%
```

**Business Impact**:
- 87% match â†’ 13% of items routed to wrong process
- Higher match = fewer routing errors = less rework

### 7.3 CV (Coefficient of Variation)

**Formula**: `CV = StdDev / Mean`

**Example Calculation**:
```
Predictions: [8, 10, 12, 14, 16] minutes
Mean: 12 minutes
StdDev: 2.83 minutes

CV = 2.83 / 12 = 0.236 (23.6%)
```

**Interpretation**:
- CV = 0.236 â†’ Predictions vary by Â±24% from mean (âœ… acceptable)
- CV > 0.5 â†’ High variability (âš ï¸ unreliable predictions)

---

## 8. Common Workflows

### 8.1 Daily Quality Check

**Frequency**: Daily (morning)

**Steps**:
1. Log in to system
2. Navigate to **Quality Monitor**
3. Check current metrics:
   - âœ… MAE < 5 minutes?
   - âœ… Process Match > 85%?
   - âœ… CV < 0.3?
4. Review recent alerts (any ğŸ”´ Critical?)
5. If quality degraded:
   - Check MAE trend chart (sudden spike?)
   - Review training history (recent retraining?)
   - Consider manual retraining if needed

**Time Required**: 2-3 minutes

### 8.2 Triggering Manual Retraining

**When to Use**:
- Quality degraded below threshold
- New production data available (monthly)
- Business process changes (new equipment, processes)

**Steps**:
1. Navigate to **Training Monitor**
2. Click "í•™ìŠµ ì‹œì‘" (Start Training)
3. Configure:
   - Sample Size: 1,000 items (larger sample for better accuracy)
   - Strategy: `stratified` (balanced across part types)
4. Click "í™•ì¸" (Confirm)
5. Monitor progress (check every 5 minutes)
6. Wait for completion (~10-30 minutes)
7. Verify deployment:
   - Check Training History (status = SUCCEEDED?)
   - Navigate to Quality Monitor
   - Verify MAE improved in next evaluation cycle

**Time Required**: 30-60 minutes (mostly waiting)

### 8.3 Investigating Quality Degradation

**Symptoms**:
- MAE suddenly increased (e.g., 4 min â†’ 10 min)
- Process Match dropped (e.g., 90% â†’ 75%)
- Multiple ğŸ”´ Critical alerts

**Investigation Steps**:

1. **Check MAE Trend Chart**
   - Gradual increase? (model drift, normal)
   - Sudden spike? (data quality issue, process change)

2. **Review Recent Alerts**
   - Any "Prediction failure" alerts? (API issue)
   - Any "CV exceeds threshold" alerts? (variability spike)

3. **Check Training History**
   - When was last successful retraining?
   - Any failed training jobs?

4. **Download Logs**
   - Navigate to Log Viewer
   - Click "Download Full Log"
   - Search for ERROR or WARNING messages

5. **Check Database**
   - Query BI_ITEM_INFO_VIEW (recent data quality?)
   - Query BI_WORK_ORDER_RESULTS (work orders available?)

6. **Contact Support** (if unresolved)
   - Attach: Quality metrics, logs, screenshots
   - Describe: Timeline, symptoms, investigation steps

### 8.4 Adjusting Thresholds

**When to Adjust**:
- Too many retraining jobs (threshold too tight)
- Too few retraining jobs (threshold too loose)
- Business SLA changed

**Steps**:
1. Review historical MAE:
   - Navigate to Quality Monitor
   - Set date range: Last 30 days
   - Export to CSV
   - Calculate: Average MAE, 95th percentile MAE

2. Set new threshold:
   - **Conservative**: 95th percentile MAE Ã— 1.2
   - **Moderate**: 95th percentile MAE Ã— 1.5
   - **Aggressive**: 95th percentile MAE Ã— 2.0

3. Update settings:
   - Navigate to Training Settings
   - Set new MAE Threshold
   - Click "ì €ì¥" (Save)

4. Monitor impact (next 7 days):
   - Track retraining frequency
   - Verify quality stays within SLA

**Example**:
```
Historical Data (last 30 days):
- Average MAE: 4.2 minutes
- 95th percentile MAE: 6.5 minutes

New Threshold Calculation:
- Conservative: 6.5 Ã— 1.2 = 7.8 minutes
- Moderate: 6.5 Ã— 1.5 = 9.75 minutes
- Aggressive: 6.5 Ã— 2.0 = 13.0 minutes

Decision: Set threshold to 8.0 minutes (conservative)
```

---

## 9. Troubleshooting

### 9.1 Dashboard Not Loading

**Symptoms**:
- Blank screen or spinner
- Error message: "Failed to load quality metrics"

**Solutions**:

1. **Check Network**:
   - Open browser console (F12)
   - Check for 500/503 errors
   - Verify server reachable (ping server)

2. **Check Backend**:
   - Verify FastAPI server running
   - Check logs: `logs/performance/performance.quality.log`
   - Restart server if needed

3. **Clear Cache**:
   - Clear browser cache (Ctrl+Shift+Del)
   - Hard refresh (Ctrl+F5)
   - Try incognito mode

4. **Check Database**:
   - Verify database connection (MSSQL K3-DB)
   - Test query: `SELECT TOP 10 * FROM BI_ITEM_INFO_VIEW`

### 9.2 Training Job Stuck

**Symptoms**:
- Progress bar frozen at X%
- Status = RUNNING for >2 hours
- No new logs

**Solutions**:

1. **Check Worker Process**:
   - Open Task Manager (Windows) or `top` (Linux)
   - Verify Python process running (high CPU = still training)

2. **Check Logs**:
   - Navigate to Log Viewer
   - Download full log
   - Search for last log entry (timestamp?)
   - Look for ERROR or TIMEOUT messages

3. **Cancel Job** (if hung):
   - Click "ì·¨ì†Œ" (Cancel) button
   - Wait 30 seconds for cancellation
   - Retry training with smaller sample size

4. **Restart Worker** (admin only):
   - Stop FastAPI server
   - Kill hung Python process (if any)
   - Restart FastAPI server
   - Re-enqueue job

### 9.3 Settings Not Saving

**Symptoms**:
- Click "ì €ì¥" (Save) â†’ no success message
- Settings reset after page refresh

**Solutions**:

1. **Check localStorage**:
   - Open browser console (F12)
   - Run: `localStorage.getItem('iterTrainingConfig')`
   - If null: localStorage disabled (privacy mode?)

2. **Disable Privacy Mode**:
   - Exit private/incognito mode
   - Enable cookies for site
   - Retry saving

3. **Clear localStorage** (if corrupted):
   - Browser console (F12)
   - Run: `localStorage.clear()`
   - Reload page
   - Reconfigure settings

### 9.4 Queue Full Error

**Symptoms**:
- Error: "íê°€ ê°€ë“ ì°¼ìŠµë‹ˆë‹¤" (Queue is full)
- Cannot start new training job

**Solutions**:

1. **Check Queue Status**:
   - Navigate to Training Monitor
   - View Training History table
   - Count RUNNING jobs (should be <3)

2. **Wait for Completion**:
   - Wait for 1 job to finish (10-30 min)
   - Retry starting new job

3. **Cancel Low-Priority Job** (if urgent):
   - Identify oldest/lowest-priority RUNNING job
   - Click "ì·¨ì†Œ" (Cancel) button
   - Start new high-priority job

4. **Increase Queue Size** (admin only):
   - Navigate to Training Settings
   - Increase "Queue Max Size" (e.g., 3 â†’ 5)
   - Click "ì €ì¥" (Save)
   - Note: Higher resource usage!

---

## 10. FAQ

### 10.1 How often should I check quality metrics?

**Recommendation**: Daily (morning check)

**Automated Alerts**: System sends alerts for critical issues (if configured)

**High-Traffic Periods**: Check more frequently (2-3 times per day)

### 10.2 When does automatic retraining happen?

**Triggers**:
1. MAE exceeds threshold (default: 5.0 minutes)
2. CV exceeds threshold (default: 0.3)
3. Process Match drops below threshold (default: 80%)

**Cooldown**: 24 hours (prevents excessive retraining)

**Manual Override**: You can trigger retraining anytime via Training Monitor

### 10.3 How long does retraining take?

**Typical Duration**: 10-30 minutes

**Factors**:
- Sample size (larger = slower)
- Server resources (CPU, RAM)
- Model complexity (MLP vs Stacking)

**Breakdown**:
- Sampling: 1-2 minutes
- Prediction: 3-5 minutes
- Training: 5-15 minutes
- Deployment: 1-2 minutes

### 10.4 Will retraining affect production predictions?

**No**: Retraining runs in background without affecting production

**Deployment**: New model deployed seamlessly (zero downtime)

**Rollback**: If new model performs worse, automatic rollback to previous version

### 10.5 What if the new model is worse than the old one?

**Automatic Protection**:
1. New model must improve Trim-MAE by â‰¥5% (configurable)
2. If not, deployment skipped (keep old model)
3. Alert generated: "ì‹ ê·œ ëª¨ë¸ ì„±ëŠ¥ ë¶€ì¡±" (New model underperformed)

**Manual Rollback** (if deployed):
- Contact operator/admin
- Rollback to previous version (1-click)

### 10.6 Can I see past quality reports?

**Yes**: Navigate to Quality Monitor

**Export Historical Data**:
1. Set date range filters (Start Date, End Date)
2. Click "Export to CSV"
3. Open in Excel for analysis

**Deliverables Folder**: `deliverables/quality_reports/`
- `cycle_{timestamp}.json`: Individual cycle reports
- `quality_summary.csv`: Summary table

### 10.7 What is the difference between MAE and Trim-MAE?

**MAE (Mean Absolute Error)**:
- Average of ALL errors
- Sensitive to outliers (extreme values)

**Trim-MAE (Trimmed MAE)**:
- Average after removing top/bottom 10% outliers
- More robust to extreme values

**Example**:
```
Errors: [1, 2, 3, 4, 100] minutes

MAE = (1+2+3+4+100) / 5 = 22 minutes (inflated by outlier!)
Trim-MAE = (2+3+4) / 3 = 3 minutes (outlier removed)
```

**Which to Use?**:
- **Trim-MAE**: Better for trigger thresholds (ignores outliers)
- **MAE**: Better for overall system performance (includes all errors)

### 10.8 Can I customize sampling strategy?

**Yes**: When starting manual training

**Strategies**:
1. **random**: Uniform random sampling (default)
2. **stratified**: Balanced across PART_TYPE/ITEM_TYPE
3. **recent_bias**: Weighted towards recent items (last 30 days)

**Recommendation**:
- **Production environment**: Use `stratified` (balanced)
- **Rapid prototyping**: Use `random` (faster)
- **Recent changes**: Use `recent_bias` (captures new patterns)

### 10.9 What happens if the database is unavailable?

**Graceful Degradation**:
1. Quality evaluation fails
2. Error logged: "Database connection timeout"
3. Alert generated: ğŸ”´ "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"
4. Previous metrics remain visible (not replaced)

**Auto-Retry**:
- System retries 3 times (exponential backoff)
- If all retries fail, evaluation skipped
- Next cycle attempts again (no impact on future cycles)

### 10.10 Can I disable automatic retraining?

**Not directly**, but you can:

1. **Set very high thresholds**:
   - MAE Threshold: 100.0 minutes (effectively disabled)
   - CV Threshold: 1.0 (effectively disabled)

2. **Set queue size to 0** (requires code change):
   - Navigate to Training Settings
   - Note: Queue size minimum is 1 (cannot be 0)

3. **Contact admin** for custom configuration

**Recommendation**: Keep auto-retraining enabled for best quality

---

## Appendix A: Metric Reference Table

| Metric | Unit | Good | Acceptable | Poor | Trigger Threshold (Default) |
|--------|------|------|------------|------|-----------------------------|
| MAE | minutes | <5 | 5-10 | >10 | 5.0 |
| Trim-MAE | minutes | <4 | 4-8 | >8 | N/A |
| RMSE | minutes | <6 | 6-12 | >12 | N/A |
| Process Match | % | >90 | 80-90 | <80 | 80.0 |
| Outsourcing Success | % | >95 | 85-95 | <85 | N/A |
| CV | ratio | <0.3 | 0.3-0.5 | >0.5 | 0.3 |
| Sample Count | count | >500 | 100-500 | <100 | N/A |

---

## Appendix B: Glossary

- **MAE**: Mean Absolute Error - average prediction error in minutes
- **Trim-MAE**: Trimmed MAE - MAE after removing outliers (top/bottom 10%)
- **RMSE**: Root Mean Squared Error - error metric that penalizes large errors
- **Process Match**: Percentage of predictions where process matches work order
- **Outsourcing Success**: Percentage of successful outsourcing decisions
- **CV**: Coefficient of Variation - standard deviation / mean (variability measure)
- **FIFO**: First-In-First-Out - queue processing order
- **Cooldown**: Minimum time between retraining jobs (prevents excessive retraining)
- **Rollback**: Restoring previous model version after failed deployment
- **Stratified Sampling**: Sampling balanced across categories (PART_TYPE/ITEM_TYPE)

---

## Appendix C: Keyboard Shortcuts

| Action | Shortcut | Context |
|--------|----------|---------|
| Refresh Dashboard | F5 | Quality Monitor |
| Export to CSV | Ctrl+E | Quality Monitor |
| Start Training | Ctrl+T | Training Monitor |
| Cancel Training | Esc | Training Monitor (job running) |
| Download Logs | Ctrl+D | Log Viewer |
| Pause Auto-Refresh | Space | Log Viewer |
| Toggle Auto-Scroll | Ctrl+S | Log Viewer |

---

**Document Version**: 1.0
**Last Updated**: 2025-10-22
**Feedback**: Report issues to ML team or create ticket in issue tracker
